{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Text Preprocessing<br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Using NLTK</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277212e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Nltk Text Preprocessing (Revisit)\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b61ee4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "#!pip install pyspellchecker  # For spelling correction\n",
    "#!pip install nltk            # For text processing and normalization\n",
    "\n",
    "# Import necessary modules\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data (if needed)\n",
    "#nltk.download('punkt')  # For tokenization\n",
    "#nltk.download('stopwords')  # For stopword removal\n",
    "#nltk.download('punkt_tab') # If 'punkt' doesn't work, try 'punkt_tab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a54a02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"OMG!! 😱 I cant believed u getting the job!!! thats gr8! 🎉 I'm so happi for u! 😊 btw, I still remembr when we last met, u were so nervous abt the interview..lol 😂. Hope u come soon so we can hang out! 😎 Don't forget to call me 2moro!! 📞\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8036b",
   "metadata": {},
   "source": [
    "1. **Tokenization** is the process of splitting a piece of text into individual words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ffd9ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', '!', '!', '😱', 'I', 'cant', 'believed', 'u', 'getting', 'the', 'job', '!', '!', '!', 'thats', 'gr8', '!', '🎉', 'I', \"'m\", 'so', 'happi', 'for', 'u', '!', '😊', 'btw', ',', 'I', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'nervous', 'abt', 'the', 'interview', '..', 'lol', '😂', '.', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'out', '!', '😎', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', '📞']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Output the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2782110a",
   "metadata": {},
   "source": [
    "2. **Stop Words removal** helps focus on the important words in reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "00e52649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')  # Download stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e980316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', '!', '!', '😱', 'cant', 'believed', 'u', 'getting', 'job', '!', '!', '!', 'thats', 'gr8', '!', '🎉', \"'m\", 'happi', 'u', '!', '😊', 'btw', ',', 'still', 'remembr', 'last', 'met', ',', 'u', 'nervous', 'abt', 'interview', '..', 'lol', '😂', '.', 'Hope', 'u', 'come', 'soon', 'hang', '!', '😎', \"n't\", 'forget', 'call', '2moro', '!', '!', '📞']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "55c67c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', '!', '!', '😱', 'I', 'cant', 'believed', 'u', 'getting', 'the', 'job', '!', '!', '!', 'thats', 'gr8', '!', '🎉', 'I', \"'m\", 'so', 'happi', 'for', 'u', '!', '😊', 'btw', ',', 'I', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'nervous', 'abt', 'the', 'interview', '..', 'lol', '😂', '.', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'out', '!', '😎', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', '📞']\n"
     ]
    }
   ],
   "source": [
    "# remove a specific word\n",
    "\n",
    "word_to_remove = 'OMG'  # Word to remove\n",
    "#words_to_remove = ['OMG!!', 'gr8', 'lol']\n",
    "filtered_tokens2 = [word for word in tokens if word.lower() != word_to_remove]\n",
    "\n",
    "print(filtered_tokens2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8fd49",
   "metadata": {},
   "source": [
    "3. **Stemming** is the process of reducing words to their base or root form (e.g., \"running\" to \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "242949eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: ['OMG', '!', '!', '😱', 'I', 'cant', 'believed', 'u', 'getting', 'the', 'job', '!', '!', '!', 'thats', 'gr8', '!', '🎉', 'I', \"'m\", 'so', 'happi', 'for', 'u', '!', '😊', 'btw', ',', 'I', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'nervous', 'abt', 'the', 'interview', '..', 'lol', '😂', '.', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'out', '!', '😎', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', '📞']\n",
      "Stemmed Text: ['omg', '!', '!', '😱', 'i', 'cant', 'believ', 'u', 'get', 'the', 'job', '!', '!', '!', 'that', 'gr8', '!', '🎉', 'i', \"'m\", 'so', 'happi', 'for', 'u', '!', '😊', 'btw', ',', 'i', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'nervou', 'abt', 'the', 'interview', '..', 'lol', '😂', '.', 'hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'out', '!', '😎', 'do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', '📞']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem each word\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "print(\"Original Text:\", tokens)\n",
    "print(\"Stemmed Text:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db656e92",
   "metadata": {},
   "source": [
    "**Can we create a function to do this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "17ffe146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'children', 'were', 'run', 'quickli', ',', 'they', 'will', 'be', 'play', 'until', 'even', '.']\n"
     ]
    }
   ],
   "source": [
    "def stemmedtext(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]  \n",
    "\n",
    "text2 = \"The children were running quickly, they will be playing until evening.\"\n",
    "stemmedwords = stemmedtext(text2)\n",
    "print(stemmedwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d08a6a",
   "metadata": {},
   "source": [
    "4. **Lemmatization** is similar to stemming, but it reduces words to their dictionary form. For example, \"better\" becomes \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b62350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG!! 😱 I cant believed u getting the job!!! thats gr8! 🎉 I'm so happi for u! 😊 btw, I still remembr when we last met, u were so nervous abt the interview..lol 😂. Hope u come soon so we can hang out! 😎 Don't forget to call me 2moro!! 📞\n",
      "['OMG', '!', '!', '😱', 'I', 'cant', 'believe', 'u', 'get', 'the', 'job', '!', '!', '!', 'thats', 'gr8', '!', '🎉', 'I', \"'m\", 'so', 'happi', 'for', 'u', '!', '😊', 'btw', ',', 'I', 'still', 'remembr', 'when', 'we', 'last', 'meet', ',', 'u', 'be', 'so', 'nervous', 'abt', 'the', 'interview', '..', 'lol', '😂', '.', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'out', '!', '😎', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', '📞']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lemmatizedtext(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "\n",
    "text2 = \"The children were running quickly, they will be playing until evening.\"\n",
    "\n",
    "lemmatizedwords = lemmatizedtext(text)\n",
    "print(text)\n",
    "print(lemmatizedwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc472e3f",
   "metadata": {},
   "source": [
    "5. **Punctuation** removal marks such as commas, periods, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90984d",
   "metadata": {},
   "source": [
    "We use a list comprehension to filter out any token found in string.punctuation, which contains common punctuation characters like !, ?, ,, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92f2a793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', '😱', 'I', 'cant', 'believed', 'u', 'getting', 'the', 'job', 'thats', 'gr8', '🎉', 'I', \"'m\", 'so', 'happi', 'for', 'u', '😊', 'btw', 'I', 'still', 'remembr', 'when', 'we', 'last', 'met', 'u', 'were', 'so', 'nervous', 'abt', 'the', 'interview', '..', 'lol', '😂', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'out', '😎', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '📞']\n"
     ]
    }
   ],
   "source": [
    "import string  # Import string module\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove punctuation\n",
    "cleaned_tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ac928",
   "metadata": {},
   "source": [
    "6. **Normalizing text** means converting text into a consistent format, such as converting all letters to lowercase.\n",
    "- Lowercasing: Converts all characters to lowercase.\n",
    "- Removing Punctuation: As mentioned earlier, punctuation is removed.\n",
    "- Handling Numbers and Special Characters: Numbers can be removed or converted to words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bebbffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg!! 😱 i cant believed u getting the job!!! thats gr8! 🎉 i'm so happi for u! 😊 btw, i still remembr when we last met, u were so nervous abt the interview..lol 😂. hope u come soon so we can hang out! 😎 don't forget to call me 2moro!! 📞\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to lowercase\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b210411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg!! 😱 i cant believed u getting the job!!! thats gr! 🎉 i'm so happi for u! 😊 btw, i still remembr when we last met, u were so nervous abt the interview..lol 😂. hope u come soon so we can hang out! 😎 don't forget to call me moro!! 📞\n"
     ]
    }
   ],
   "source": [
    "# Removing numbers\n",
    "import re\n",
    "\n",
    "# Remove numbers\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ca28e",
   "metadata": {},
   "source": [
    "- `re.sub(r'\\d+', '', text)` is using Python's `re` (regular expression) module to remove all numerical digits from the given string text.\n",
    "- `re.sub()`: This function is used to search for a pattern (defined by a regular expression) in a string and replace it with another string. It has the following syntax:\n",
    "`re.sub(pattern, replacement, string)\n",
    "- `r'\\d+'`: This is the regular expression pattern used to find numbers in the text.\n",
    "\n",
    "`\\d` represents any digit (equivalent to [0-9]).\n",
    "The `+` means one or more of the preceding element (in this case, digits). So, \\d+ matches one or more consecutive digits (i.e., any number).\n",
    "\n",
    "- [Regex](https://www.w3schools.com/python/python_regex.asp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "844e00e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have  apples and  bananas.\n"
     ]
    }
   ],
   "source": [
    "textx = \"I have 2 apples and 3 bananas.\"\n",
    "result = re.sub(r'\\d+', '', textx)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52231713",
   "metadata": {},
   "source": [
    "7. **Spelling correction** aims to fix misspelled words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3b6996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5ebc024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "textx = \"I havv goood speling\"\n",
    "corrected_text = TextBlob(textx).correct()\n",
    "print(corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50e4874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg!! 😱 i cant believed u getting the job!!! thats gr! 🎉 i'm so happi for u! 😊 btw, i still remembr when we last met, u were so nervous abt the interview..lol 😂. hope u come soon so we can hang out! 😎 don't forget to call me moro!! 📞\n",
      "org!! 😱 i can believed u getting the job!!! that gr! 🎉 i'm so happy for u! 😊 bow, i still remember when we last met, u were so nervous at the interview..ll 😂. hope u come soon so we can hang out! 😎 don't forget to call me more!! 📞\n"
     ]
    }
   ],
   "source": [
    "corrected_text = TextBlob(text).correct()\n",
    "print(text)\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857bdf0",
   "metadata": {},
   "source": [
    "**Manual corrections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfcee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh my god!! I cant blv you got the job!!! thats great! I'm so happi for you!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a dictionary of common text-based contractions or slang to formalize\n",
    "corrections = {\n",
    "    \"gr8\": \"great\",\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"omg\": \"oh my god\"\n",
    "}\n",
    "\n",
    "# Function to apply the corrections to a given text\n",
    "def correct_text(text):\n",
    "    # Iterate through each key-value pair in the corrections dictionary\n",
    "    for incorrect, correct in corrections.items():\n",
    "        # Replace the incorrect term with the correct one using re.sub\n",
    "        text = re.sub(rf'\\b{incorrect}\\b', correct, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Example text\n",
    "textx = \"OMG!! I cant blv u got the job!!! thats gr8! I'm so happi for u!\"\n",
    "\n",
    "# Correct the text\n",
    "corrected_text = correct_text(textx)\n",
    "print(corrected_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b04c3",
   "metadata": {},
   "source": [
    "8. **Emojis** are often not useful for text analysis and are removed using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sample text with emojis\n",
    "text = \"OMG!! I cant blv u got the job!!! 😃👍 thats gr8! I'm so happi for u! 🎉\"\n",
    "\n",
    "# Simple regex to remove emojis\n",
    "text_without_emojis = re.sub(r'[^\\w\\s,]', '', text)\n",
    "\n",
    "# Output the text without emojis\n",
    "print(text_without_emojis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2647f3",
   "metadata": {},
   "source": [
    "The pattern `[^\\w\\s,]` removes anything that is not a word character (`\\w`), whitespace (`\\s`), or a comma (`,`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49310ad",
   "metadata": {},
   "source": [
    "9. **Removing URLs and Mentions** makes the analysis cleaner and more focused on the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2c7ad175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my blog at http://example.com! Follow me on Twitter @username  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text with URLs, mentions, and hashtags\n",
    "text = \"Check out my blog at http://example.com! Follow me on Twitter @username #excited #python\"\n",
    "\n",
    "# Remove URLs (http, https, and ftp)\n",
    "text_no_urls = re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "# Remove mentions (e.g., @username)\n",
    "text_no_mentions = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "# Remove hashtags (e.g., #excited)\n",
    "text_no_hashtags = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "# Print the cleaned text\n",
    "print(text_no_hashtags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82214690",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f12f1",
   "metadata": {},
   "source": [
    "**Everything together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f55e1f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5d48c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh god cant believed getting job thats gareeat happi foare follow youseare excited\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sample text\n",
    "text = \"OMG!! 😱 I cant believed u getting the job!!! thats gr8! 🎉 I'm so happi for u! Follow @user #excited http://example.com 😊👍\"\n",
    "\n",
    "# Define a dictionary for slang corrections\n",
    "corrections = {\n",
    "    \"gr8\": \"great\",\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"omg\": \"oh my god\"\n",
    "}\n",
    "\n",
    "# 1. Lowercasing\n",
    "text = text.lower()\n",
    "\n",
    "# 2. Replace slang terms (correct spelling)\n",
    "for incorrect, correct in corrections.items():\n",
    "    text = text.replace(incorrect, correct)\n",
    "\n",
    "# 3. Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# 4. Remove punctuation\n",
    "tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# 5. Remove emojis\n",
    "tokens = [word for word in tokens if re.sub(r'[^\\w\\s,]', '', word) == word]\n",
    "\n",
    "\n",
    "# 6. Remove URLs\n",
    "tokens = [word for word in tokens if not word.startswith(\"http\")]\n",
    "\n",
    "# 7. Remove mentions (e.g., @username)\n",
    "tokens = [word for word in tokens if not word.startswith(\"@\")]\n",
    "\n",
    "# 8. Remove hashtags (e.g., #hashtag)\n",
    "tokens = [word for word in tokens if not word.startswith(\"#\")]\n",
    "\n",
    "# 9. Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# 10. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Join tokens back to form the cleaned text\n",
    "cleaned_text = \" \".join(tokens)\n",
    "\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Advanced NLP Techniques<br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Word embeddings and transformers</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.  Introduction to Advanced NLP<p><p> \n",
    "2.  Word Embeddings <p>\n",
    "3. Transformers <p> \n",
    "4. Building the Sentiment Analyzer (OPTIONAL/ADVANCED)<p>\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    Can acts of kindness sometimes be self-serving, even when we don't realize it?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa86dc9",
   "metadata": {},
   "source": [
    "How do we differentiate between genuine kindness and actions that benefit ourselves?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Introduction to Advanced NLP\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aef031-1416-4707-b2a0-fc60ca4ee7ca",
   "metadata": {},
   "source": [
    "Advanced Natural Language Processing (NLP) refers to the study and application of more complex and powerful techniques in processing human language, going beyond basic tasks like tokenization, part-of-speech tagging, and named entity recognition. \n",
    "\n",
    "While traditional NLP methods focused on rule-based or statistical models, advanced NLP incorporates deep learning techniques, which allow for better handling of ambiguous, large-scale, and context-dependent language data.\n",
    "\n",
    "**What We Study in Advanced NLP:**\n",
    "\n",
    "1. **Word Embeddings**: \n",
    "\n",
    "We study methods like **Word2Vec** and **GloVe** to represent words as vectors in continuous space, capturing semantic relationships. These representations are crucial for understanding word meanings in context.\n",
    "\n",
    "2. **Transformers**: \n",
    "\n",
    "The introduction of transformer models, with self-attention mechanisms, revolutionized NLP by enabling models to capture long-range dependencies and understand context better than previous models. This is the foundation for many modern NLP models, including **BERT**, **GPT**, and **T5**.\n",
    "\n",
    "3. **Generative Models & Language Models**: \n",
    "\n",
    "We dive into **Generative AI** models like GPT and other transformer-based models. These models are designed to generate human-like text, enabling tasks such as text generation, summarization, translation, and creative writing.\n",
    "\n",
    "4. **Transfer Learning**: \n",
    "\n",
    "We often use pre-trained models and fine-tune them on specific tasks. This technique has made it easier to apply powerful models to a variety of language tasks with relatively small amounts of labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b10a45",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Word Embeddings\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ecbc8",
   "metadata": {},
   "source": [
    "Learn more from:\n",
    "- [geeks](https://www.bing.com/ck/a?!&&p=f32648e0377c96d7797cc0531cbef670926742093017f8674c2fd952fdc8b5a3JmltdHM9MTczMjY2NTYwMA&ptn=3&ver=2&hsh=4&fclid=1f46f25e-a3d6-62d5-1977-e75fa22e6379&psq=word+embeddings&u=a1aHR0cHM6Ly93d3cuZ2Vla3Nmb3JnZWVrcy5vcmcvd29yZC1lbWJlZGRpbmdzLWluLW5scC8&ntb=1)\n",
    "- [ibm](https://www.ibm.com/topics/word-embeddings)\n",
    "- [Medium Article](https://medium.com/analytics-vidhya/word-embeddings-in-nlp-word2vec-glove-fasttext-24d4d4286a73) \n",
    "- [Tensorflow](https://www.tensorflow.org/text/guide/word_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d2700c",
   "metadata": {},
   "source": [
    "Word embeddings are a way to represent words in numerical form so that computers can understand and process them more efficiently. \n",
    "\n",
    "Instead of treating words as individual tokens, word embeddings map each word to a vector (a list of numbers), where the numbers represent the word’s meaning in relation to other words.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*sAJdxEsDjsPMioHyzlN3_A.png)\n",
    "\n",
    "In simple terms, word embeddings allow *words with similar meanings* to have similar vector representations. This helps the computer understand relationships between words in a much deeper way than just looking at the words themselves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095752f6",
   "metadata": {},
   "source": [
    "### **Why Use Word Embeddings?**\n",
    "Traditional methods like one-hot encoding represent each word as a unique vector where only one element is \"1\" and the rest are \"0\". \n",
    "![](https://www.tensorflow.org/static/text/guide/images/one-hot.png)\n",
    "\n",
    "This method doesn’t capture any relationships between words. For example, \"cat\" and \"dog\" would be equally distant from each other, even though they are more similar in meaning than \"cat\" and \"car\".\n",
    "\n",
    "With word embeddings, words with similar meanings will be closer in the vector space.\n",
    "\n",
    "![](https://www.tensorflow.org/static/text/guide/images/embedding2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10952c",
   "metadata": {},
   "source": [
    "### **Popular Word Embedding Models**\n",
    "\n",
    "1. **Word2Vec**: [Learn more...](https://www.geeksforgeeks.org/python-word-embedding-using-word2vec)\n",
    "   - **Word2Vec** is a model that learns to represent words as vectors in a way that captures word meanings based on their context in large text data.\n",
    "   - It has two main approaches:\n",
    "   - **Continuous Bag of Words (CBOW)**: Predicts a target word from the surrounding context words.\n",
    "     - **Concept**: The CBOW model predicts a target word (the center word) based on its surrounding context words.\n",
    "\n",
    "   - **Example**:\n",
    "     - Sentence: The cat sits on the mat.\n",
    "     - Target word (center): sits\n",
    "     - Context words: The, cat, on, the, mat\n",
    "     \n",
    "     In CBOW, we use the context words (\"The\", \"cat\", \"on\", \"the\", \"mat\") to predict the target word \"sits.\"\n",
    "\n",
    "\n",
    "\n",
    "   - **Skip-Gram**: Predicts surrounding context words from a target word.\n",
    "     - **Concept**: The Skip-gram model does the opposite of CBOW—it predicts the context words from the target word.\n",
    "\n",
    "     - **Example**:\n",
    "      - Sentence: The cat sits on the mat.\n",
    "      - Target word: sits\n",
    "      - Context words: The, cat, on, the, mat\n",
    "     \n",
    "     In Skip-gram, we use the target word \"sits\" to predict the context words (\"The\", \"cat\", \"on\", \"the\", \"mat\").\n",
    "\n",
    "  #### **Conclusion**:\n",
    "   - **CBOW**: Predict the center word using the context.\n",
    "   - **Skip-gram**: Predict the context words using the center word.\n",
    "\n",
    "![](https://swimm.io/wp-content/webp-express/webp-images/uploads/2023/11/word2vec--1024x559.png.webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309f7e0",
   "metadata": {},
   "source": [
    "2. **GloVe (Global Vectors for Word Representation)**:\n",
    "   - **GloVe** is another popular word embedding technique. \n",
    "   \n",
    "   It works by analyzing the global co-occurrence of words in a corpus (the whole collection of text) and using this information to create word vectors.\n",
    "\n",
    "   - Unlike Word2Vec, which focuses on the local context (neighboring words), GloVe focuses on the global statistical information across the entire corpus.\n",
    "   \n",
    "   - Example: GloVe might place \"cat\" and \"dog\" closer to each other in the vector space because they appear in similar contexts (like \"pet\", \"animal\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff475b0",
   "metadata": {},
   "source": [
    "  #### **Example to Understand Word Embeddings:**\n",
    "Imagine you have three words: **\"king\"**, **\"queen\"**, and **\"man\"**.\n",
    "\n",
    "- Word embeddings help in capturing relationships like:\n",
    "  - **king** is to **queen** as **man** is to **woman**.\n",
    "\n",
    "![](https://th.bing.com/th/id/OIP._Aap0YgGccqfpGmADIL0lgHaHT?rs=1&pid=ImgDetMain)\n",
    "\n",
    "In the vector space, the difference between \"king\" and \"queen\" might be similar to the difference between \"man\" and \"woman\". This allows for mathematical operations like:\n",
    "  \n",
    "  **king - man + woman = queen**\n",
    "\n",
    "This shows that embeddings can capture relationships between words and perform operations on them (like analogies) that were not possible with older techniques like one-hot encoding.\n",
    "\n",
    "#### **Conclusion:**\n",
    "- Word embeddings are a way to convert words into numerical vectors.\n",
    "- They capture the meaning of words in a way that allows similar words to have similar vectors.\n",
    "- Popular methods like Word2Vec and GloVe learn these embeddings from text data, allowing for better word relationships, analogies, and understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345749d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Transformers\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f0466",
   "metadata": {},
   "source": [
    "- [Geeks](https://www.geeksforgeeks.org/getting-started-with-transformers/)\n",
    "- [Hugging Face](https://huggingface.co/learn/nlp-course/chapter1/4)\n",
    "\n",
    "Transformers have fundamentally changed the way machines understand and generate human language, making them the go-to architecture for state-of-the-art NLP applications today.\n",
    "\n",
    "**Transformers** are a type of deep learning models that have revolutionized Natural Language Processing (NLP). \n",
    "\n",
    "They are designed to handle sequential data (like text) but in a much more efficient and powerful way compared to previous models, such as RNNs and LSTMs.\n",
    "\n",
    "#### **What Makes Transformers Special?**\n",
    "1. **Attention Mechanism**:\n",
    "   - The key innovation behind transformers is the **attention mechanism**. In simple terms, attention **allows the model to \"focus\"** on different parts of the input sequence when making predictions, rather than just processing the data in order (like RNNs).\n",
    "   - This means that when processing a word in a sentence, the **model can consider the entire sentence** and give more importance to relevant words, regardless of their position. This is especially helpful for tasks like translation or text generation.\n",
    "\n",
    "   Example: In the sentence \"The cat sat on the mat,\" when predicting the word \"sat,\" the model might pay more attention to \"cat\" than to \"the\" or \"on.\"\n",
    "\n",
    "2. **Parallel Processing**:\n",
    "   - Unlike RNNs, which process data sequentially (one word at a time), transformers can process all words in the sentence simultaneously. This leads to **faster training** and better results on long sequences of text.\n",
    "\n",
    "3. **Positional Encoding**:\n",
    "   - Since transformers process all words at once, they need a way to understand the order of words in the sentence. This is where **positional encoding** comes in. It adds information about the position of each word in the sequence, allowing the model to maintain an understanding of word order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8412622",
   "metadata": {},
   "source": [
    "#### **Transformers in Action:**\n",
    "Transformers have led to the development of powerful models like:\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: \n",
    "\n",
    "A transformer-based model trained to understand the context of words in both directions (left-to-right and right-to-left). For understanding the context of a word in relation to its surrounding words, making it powerful for tasks like question answering, sentiment analysis, and language understanding.\n",
    "\n",
    "- **GPT (Generative Pre-trained Transformer)**: \n",
    "\n",
    "A model trained to generate text in a conversational style, often used for text completion, summarization, and even creative tasks like writing poetry.\n",
    "\n",
    "#### **Real-World Example:**\n",
    "Imagine you’re using a machine translation system (like Google Translate). \n",
    "\n",
    "A transformer model can take the sentence: \n",
    "\n",
    "and translate it to French as\n",
    "\n",
    "**\"I love reading books\"**  \n",
    "\n",
    "**\"J'aime lire des livres.\"**\n",
    "\n",
    "The attention mechanism helps the model focus on key words like \"love\" and \"reading\" when making the translation, ensuring the output makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89cf099",
   "metadata": {},
   "source": [
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: A model that understands the context of a word by looking at the words before and after it, used for tasks like text classification and question answering.\n",
    "\n",
    "- **GPT (Generative Pre-trained Transformer)**: A language generation model that predicts the next word in a sequence, excelling at tasks like text generation and conversational agents.\n",
    "\n",
    "- **T5 (Text-to-Text Transfer Transformer)**: Treats all NLP tasks as text-to-text problems, such as translation, summarization, and question answering.\n",
    "\n",
    "- **RoBERTa (Robustly Optimized BERT Approach)**: An optimized version of BERT trained with more data and without next-sentence prediction, improving its performance across NLP tasks.\n",
    "\n",
    "- **XLNet**: A model that combines autoregressive and bidirectional training to capture richer context dependencies, outperforming BERT in many tasks.\n",
    "\n",
    "- **ALBERT (A Lite BERT)**: A memory-efficient version of BERT with shared parameters across layers, reducing the model size without sacrificing performance.\n",
    "\n",
    "- **BART (Bidirectional and Auto-Regressive Transformers)**: A hybrid model combining BERT and GPT, used for sequence-to-sequence tasks like text generation and summarization.\n",
    "\n",
    "- **DeBERTa (Decoding-enhanced BERT with Disentangled Attention)**: An advanced version of BERT with improved attention mechanisms, enhancing performance on text classification and understanding tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e15d3",
   "metadata": {},
   "source": [
    "### **How Do Transformers Work?**\n",
    "Transformers are made up of two main components:\n",
    "1. **Encoder**: The encoder takes in the input sequence (like a sentence) and processes it into a sequence of vectors. Each word in the sentence is converted into a vector using an embedding.\n",
    "2. **Decoder**: The decoder takes the output from the encoder and generates the final prediction (such as the next word in a sentence or a translation).\n",
    "\n",
    "The encoder and decoder are made up of multiple layers of attention and other operations that help the model learn better representations of the text.\n",
    "\n",
    "### **The Transformer Architecture:**\n",
    "- **Self-Attention**: This allows the model to determine which words in the input sequence are important for predicting the next word.\n",
    "- **Multi-Head Attention**: Instead of focusing on just one aspect of the sentence, the model uses multiple attention heads to learn different relationships at once.\n",
    "- **Feedforward Neural Networks**: After attention layers, transformers use simple neural networks to process the data further.\n",
    "- **Layer Normalization**: This technique helps stabilize and speed up training.\n",
    "\n",
    "\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "- **Transformers** use an attention mechanism to understand the relationships between words in a sentence.\n",
    "- They process all words in a sequence at once, making them faster and more powerful than older models like RNNs.\n",
    "- They are the foundation for modern models like **BERT**, **GPT**, and **T5**, which perform well on a wide range of NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5865a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Hugging Face library\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a38fd7",
   "metadata": {},
   "source": [
    "The **Hugging Face** library is one of the most popular and powerful tools for working with state-of-the-art models in Natural Language Processing (NLP). \n",
    "\n",
    "\n",
    "\n",
    "It provides easy access to a wide variety of **pre-trained models** and **datasets**, which can be used for tasks such as text classification, translation, summarization, and question answering.\n",
    "\n",
    "### **What is Hugging Face?**\n",
    "\n",
    "Hugging Face is an open-source company that focuses on advancing NLP by providing tools that simplify working with complex models like transformers. The **Transformers** library by Hugging Face allows developers to download pre-trained models, fine-tune them, and use them for specific NLP tasks without needing to train the model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c58854",
   "metadata": {},
   "source": [
    "### **Why Use Hugging Face?**\n",
    "1. **Pre-trained Models**: Hugging Face provides a large number of pre-trained models for various NLP tasks. These models have been trained on massive datasets and can be used immediately or fine-tuned for specific tasks.\n",
    "   \n",
    "2. **Ease of Use**: With just a few lines of code, you can load, use, and fine-tune models on your own data. It makes working with advanced NLP models accessible to beginners and experts alike.\n",
    "\n",
    "3. **Integration with Other Libraries**: Hugging Face integrates seamlessly with libraries like PyTorch and TensorFlow, which allows users to leverage the full power of these frameworks for training and deploying models.\n",
    "\n",
    "4. **Community and Resources**: Hugging Face has a large community that shares models, datasets, and tutorials. It is a great place to learn and collaborate on NLP projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f425c7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Generative Models and Language Models\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e40a34",
   "metadata": {},
   "source": [
    "### Generative Models\n",
    "\n",
    "**Generative models** are machine learning models that generate new data based on the patterns they learn from existing data. In NLP, these models create new text or predict possible sequences of text.\n",
    "\n",
    "Key Concept:\n",
    "A **generative model** learns the underlying distribution of data and generates new content that resembles the data it was trained on. It doesn't just classify or predict based on input; it creates new, realistic outputs.\n",
    "\n",
    "Examples of Generative Models in NLP:\n",
    "1. **GPT (Generative Pretrained Transformer)**:\n",
    "   - GPT is a transformer-based generative model. It can generate coherent and contextually relevant text, answer questions, or write essays, all based on a prompt.\n",
    "   - Trained on massive text corpora from the internet, GPT models like GPT-3 are capable of creating highly human-like text.\n",
    "\n",
    "2. **BERT (Bidirectional Encoder Representations from Transformers)**:\n",
    "   - BERT is a transformer model designed to understand context by looking at both left and right of a word. It's often used in pretraining models to understand the relationship between words in a sentence.\n",
    "   - Although BERT is primarily used for tasks like classification, it is a key part of the foundation for generative models, especially in language understanding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d507b",
   "metadata": {},
   "source": [
    "###Language Models\n",
    "\n",
    "Language models are a type of generative model, specifically designed to predict the likelihood of a sequence of words or generate coherent sentences.\n",
    "\n",
    "Key Concept:\n",
    "A **language model** is a model trained to understand and predict the probability of sequences of words in a language. It can be used for tasks like text generation, text completion, and more.\n",
    "\n",
    "Examples of Language Models:\n",
    "1. **RNN (Recurrent Neural Networks)**:\n",
    "   - RNNs are one of the earliest types of models used for sequential data, including text. They are capable of maintaining a memory of previous inputs, making them suitable for language modeling tasks.\n",
    "\n",
    "2. **LSTM (Long Short-Term Memory)**:\n",
    "   - LSTM networks are an advanced version of RNNs, specifically designed to address the issue of vanishing gradients and maintain long-term dependencies in sequences, making them better suited for language modeling than standard RNNs.\n",
    "\n",
    "3. **Transformer Models**:\n",
    "   - Transformers, like GPT and BERT, have revolutionized language modeling by using self-attention mechanisms. They can consider the entire context of a sentence or document in parallel, which greatly improves performance in a variety of NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad47e3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Transfer Learning\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8716c",
   "metadata": {},
   "source": [
    "**Transfer learning** is a technique where a model trained on one task is reused on a new but related task. Instead of training a model from scratch, you take advantage of the knowledge learned from a different task, and adapt it for your own needs. \n",
    "\n",
    "This concept has been widely used in natural language processing (NLP), especially with large pre-trained language models.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "1. **Pre-trained Models**:\n",
    "   - A model is first trained on a large, general corpus of text (such as Wikipedia or a news dataset) to learn the basic structure of language, like grammar, sentence structure, and common word relationships. \n",
    "   - These models are then fine-tuned for specific NLP tasks, such as sentiment analysis, named entity recognition, or text classification.\n",
    "\n",
    "2. **Fine-tuning**:\n",
    "   - Once a model has been pre-trained, it can be adapted to a specific task with additional training on a smaller, task-specific dataset. This is called **fine-tuning**.\n",
    "   - Fine-tuning adjusts the model’s parameters to make it more effective for the new task, of|ten requiring less data and time compared to training a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02212f",
   "metadata": {},
   "source": [
    "**Examples of Transfer Learning in NLP:**\n",
    "\n",
    "1. **BERT** (Bidirectional Encoder Representations from Transformers):\n",
    "   - BERT is pre-trained on large corpora like Wikipedia and BookCorpus. Once pre-trained, it can be fine-tuned for tasks like text classification, question answering, and language inference. \n",
    "   - This allows BERT to leverage its general understanding of language and apply it to specific problems.\n",
    "\n",
    "2. **GPT** (Generative Pre-trained Transformer):\n",
    "   - Similar to BERT, GPT models are pre-trained on vast amounts of text. They can then be fine-tuned to perform tasks like text generation, summarization, or machine translation.\n",
    "   - For example, GPT can be fine-tuned to generate text based on a given prompt or to answer specific questions.\n",
    "\n",
    "---\n",
    "\n",
    "Why Transfer Learning is Important in NLP:\n",
    "\n",
    "- **Reduced Training Time**: Instead of starting from scratch, you build upon the knowledge gained from pre-training, making the learning process faster.\n",
    "- **Improved Performance with Less Data**: Fine-tuning a pre-trained model requires fewer data points than training from scratch, especially for tasks that are closely related.\n",
    "- **Generalization**: Pre-trained models learn patterns from diverse data, allowing them to generalize better to new, unseen tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "Transfer learning enables us to use models that have learned general language patterns and apply them to specific NLP tasks. By fine-tuning pre-trained models, you can achieve high performance with limited task-specific data and significantly reduce computational costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624662c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Building the Sentiment Analyzer (OPTIONAL/ADVANCED)\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44015ad",
   "metadata": {},
   "source": [
    "**Fine-tuning a Pre-trained Sentiment Analysis Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c81dc3",
   "metadata": {},
   "source": [
    "Now, let's walk through how to fine-tune a pre-trained sentiment analysis model using Hugging Face's **Transformers** library. We'll use a model like **BERT** (Bidirectional Encoder Representations from Transformers) for sentiment analysis, which is pre-trained on a large corpus of text and can be fine-tuned on a smaller dataset for specific tasks.\n",
    "\n",
    "### **Steps to Fine-tune a Pre-trained Model:**\n",
    "\n",
    "1. **Install the Required Libraries:**\n",
    "\n",
    "   If you haven’t already, you need to install Hugging Face's `transformers` and `datasets` libraries. You can do this by running:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c979381",
   "metadata": {},
   "source": [
    "Might need rust for this:\n",
    "https://rustup.rs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d732ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10fa792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 labels for binary sentiment (positive, negative)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca2500",
   "metadata": {},
   "source": [
    "2. **Load and Preprocess the Dataset:**\n",
    "\n",
    "For sentiment analysis, we can use datasets such as IMDb or SST-2. Hugging Face's datasets library makes it easy to load these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True)\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24cb53",
   "metadata": {},
   "source": [
    "3. **Set Up Training Arguments:**\n",
    "\n",
    "Fine-tuning a model requires specifying the training configuration, such as batch size, number of epochs, and learning rate. Hugging Face’s Trainer API makes this process much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de8a1f",
   "metadata": {},
   "source": [
    "4. **Train the Model:**\n",
    "\n",
    "Once everything is set up, you can start fine-tuning the model using the Trainer API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model to be trained\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=tokenized_datasets['train'],   # training dataset\n",
    "    eval_dataset=tokenized_datasets['test'],    # evaluation dataset\n",
    ")\n",
    "\n",
    "# Start fine-tuning the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a60f4f",
   "metadata": {},
   "source": [
    "5. **Evaluate the Model:**\n",
    "\n",
    "After fine-tuning, you can evaluate the model’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abcbe22",
   "metadata": {},
   "source": [
    "6. **Make Predictions:**\n",
    "\n",
    "Once the model is fine-tuned, you can use it to make predictions on new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "text = \"I love this movie!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "print(\"Prediction:\", \"Positive\" if predictions == 1 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2185430-3791-45df-824f-bdec6d7145e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Live Exercise</font> </b>\n",
    "</div>\n",
    "\n",
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d61af-f3e8-4afc-a66b-3814e160aaf3",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task 1: description of task\n",
    "\n",
    "    - instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9ddd9-5558-4b1d-a3e7-04c3dca33b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "1. topic:\n",
    "    - question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b86c1-64b0-4abd-8ba9-54746bdc9007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

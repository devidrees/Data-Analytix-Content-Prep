{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    NLP with Python <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Using NLTK and SpaCy for text classification</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.  Introduction to Text Classification<p><p> \n",
    "2.  Overview of Nltk and SpaCy <p>\n",
    "3.  Text classification in Nltk <p>\n",
    "4.  Text classification in spaCy <p>  \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    “You were born with wings, why prefer to crawl through life?”<br>\n",
    "    - Rumi\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb68d6",
   "metadata": {},
   "source": [
    "\n",
    "What does this mean to you? How might AI interpret ‘wings’ literally and miss the deeper meaning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Introduction to Text Classification\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beebe0c1",
   "metadata": {},
   "source": [
    "Text classification is a Natural Language Processing (NLP) task where text is analyzed, understood, and categorized into predefined classes or categories.\n",
    "\n",
    "It involves assigning a label or category to a given piece of text based on its content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a9759",
   "metadata": {},
   "source": [
    "- Categorizing text into predefined categories based on its content.\n",
    "- **Examples:**\n",
    "  - Email filtering (spam vs. not spam).\n",
    "  - Sentiment analysis (positive, negative, neutral).\n",
    "  - News categorization (politics, sports, technology).\n",
    "- **Importance:** Automates the understanding of large volumes of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddcaae",
   "metadata": {},
   "source": [
    "**Applications of Text Classification**\n",
    "- **Sentiment Analysis:** Classifying text as positive, negative, or neutral (e.g., analyzing customer reviews).\n",
    "Spam Detection: Identifying spam or non-spam emails.\n",
    "- **Topic Categorization:** Grouping articles or documents by topic (e.g., politics, sports, technology).\n",
    "- **Language Detection:** Identifying the language of a given text.\n",
    "- **Intent Detection:** Understanding the intent behind a query in chatbots or virtual assistants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fef9c",
   "metadata": {},
   "source": [
    "**Methods Used in Text Classification**\n",
    "\n",
    "- **Rule-Based Systems:** Predefined rules are used to classify text based on patterns or keywords. For example, an email containing the phrase \"Congratulations, you've won!\" could be classified as spam.\n",
    "\n",
    "- **Machine Learning Models:** Algorithms such as *Naive Bayes*, *Support Vector Machines (SVM)*, or *Logistic Regression* are trained on numerical features extracted from text data. These models learn patterns from labeled datasets to make predictions.\n",
    "\n",
    "- **Deep Learning Models:** Advanced neural network architectures like Convolutional Neural Networks (CNNs), or Transformers (e.g., BERT, GPT) are used for understanding and classifying complex text data, especially when large datasets are available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b10a45",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Overview of NLTK and SpaCy\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127c025",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> NLTK\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71edec",
   "metadata": {},
   "source": [
    "\n",
    "- **NLTK:**\n",
    "  - Python library for working with text.\n",
    "  - Designed primarily as an educational tool for teaching and learning NLP.\n",
    "  - For basic NLP tasks like tokenization, stopword removal, stemming, lemmatization, POS tagging, and syntactic parsing (where a sentence is analyzed to determine its grammatical structure).\n",
    "  - Generally slower because it processes text one step at a time and is designed more for research and teaching. \n",
    "- NLTK can be slower when dealing with large datasets or real-time processing due to its older design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf68b2",
   "metadata": {},
   "source": [
    "- [NLTK Website](https://www.nltk.org/#natural-language-toolkit)\n",
    "- [NLTK Book](https://www.nltk.org/book_1ed)\n",
    "- [NLTK Sentiment Analysis](https://www.geeksforgeeks.org/nltk-sentiment-analysis-tutorial-for-beginners/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa57a62",
   "metadata": {},
   "source": [
    "Building a Text Classification Pipeline with NLTK\n",
    "\n",
    "1. **Import libraries.**\n",
    "2. **Load and preprocess text data.**\n",
    "3. **Feature extraction using Bag of Words or TF-IDF.**\n",
    "4. **Train a simple Naive Bayes classifier.**\n",
    "5. **Test and evaluate the classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5865a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> spaCy\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e5c11",
   "metadata": {},
   "source": [
    "- **SpaCy:**\n",
    "  - Built for production-level tasks and focuses on efficiency and performance. It is optimized for industrial use cases, handling large-scale data and real-time NLP applications.\n",
    "  - Known for its speed and efficiency.\n",
    "  - For advanced preprocessing like lemmatization, POS tagging, dependency parsing, named entity recognition (NER), and word embeddings.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017923d2",
   "metadata": {},
   "source": [
    "- [spaCy 101](https://spacy.io/usage/spacy-101) A brief introduction\n",
    "- [spaCy Course](https://course.spacy.io/en/) Advanced NLP with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea591cf7",
   "metadata": {},
   "source": [
    "Text Classification with SpaCy\n",
    "\n",
    "1. **Load SpaCy and process text.**\n",
    "2. **Extract linguistic features (POS, entities, etc.).**\n",
    "3. **Train a classifier using features.**\n",
    "4. **Evaluate the classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906dd8dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "183d90d6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Text Classification with NLTK\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3dd28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Download NLTK resources\n",
    "# Download necessary datasets\n",
    "nltk.download('movie_reviews')  # Movie review dataset\n",
    "nltk.download('stopwords')      # Stopwords for preprocessing\n",
    "nltk.download('punkt')          # Tokenizer for text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d8f7a",
   "metadata": {},
   "source": [
    "We create a small dataset with sentences labeled as \"pos\" (positive) or \"neg\" (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61ffe241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample dataset of sentences labeled as positive or negative\n",
    "training_data = [\n",
    "    (\"I love this movie\", \"pos\"),\n",
    "    (\"This film is amazing\", \"pos\"),\n",
    "    (\"I hated this movie\", \"neg\"),\n",
    "    (\"This film is terrible\", \"neg\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc48ab1",
   "metadata": {},
   "source": [
    "**Preprocessing:**\n",
    "- Tokenization: Break the sentence into individual words.\n",
    "- Stopword Removal: Remove common words like \"the\", \"and\", etc., to focus on meaningful words.\n",
    "- Feature Extraction: We create a dictionary where the keys are words, and the values are True (indicating the presence of the word in the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b76a7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocessing and Feature Extraction\n",
    "def extract_features(sentence):\n",
    "    words = word_tokenize(sentence.lower())  # Tokenize and convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))  # Get stop words\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]  # Remove punctuation and stop words\n",
    "    return {word: True for word in words}  # Create feature dictionary with word presence as True\n",
    "\n",
    "# Convert training data into feature sets\n",
    "training_features = [(extract_features(sentence), label) for sentence, label in training_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c409887",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier: We train the classifier on the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94dfef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train Naive Bayes Classifier\n",
    "classifier = NaiveBayesClassifier.train(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b4bd8",
   "metadata": {},
   "source": [
    "Testing: We classify new sentences using the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4354c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test the classifier with new sentences\n",
    "test_sentences = [\n",
    "    \"I really enjoyed this movie\",\n",
    "    \"This movie was awful\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7683bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I really enjoyed this movie' => Predicted Sentiment: pos\n",
      "Sentence: 'This movie was awful' => Predicted Sentiment: pos\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    features = extract_features(sentence)\n",
    "    predicted_label = classifier.classify(features)\n",
    "    print(f\"Sentence: '{sentence}' => Predicted Sentiment: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00b88e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>4. Text Classification with SpaCy\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4f25344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875bead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline.textcat import Config, ConfigSchema, TextCategorizer\n",
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c35f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy's pre-trained English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Step 1: Create the text classifier component\n",
    "text_cat = nlp.create_pipe('textcat', config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"})\n",
    "nlp.add_pipe(text_cat, last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels (positive and negative)\n",
    "text_cat.add_label(\"pos\")\n",
    "text_cat.add_label(\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the training data\n",
    "# The data format is [(text, label), where label is 'pos' or 'neg']\n",
    "training_data = [\n",
    "    (\"I love this movie\", {\"cats\": {\"pos\": 1, \"neg\": 0}}),\n",
    "    (\"This film is amazing\", {\"cats\": {\"pos\": 1, \"neg\": 0}}),\n",
    "    (\"I hated this movie\", {\"cats\": {\"pos\": 0, \"neg\": 1}}),\n",
    "    (\"This film is terrible\", {\"cats\": {\"pos\": 0, \"neg\": 1}})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92773844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to SpaCy's format\n",
    "train_examples = []\n",
    "for text, annot in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annot)\n",
    "    train_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train the classifier\n",
    "optimizer = nlp.begin_training()\n",
    "for epoch in range(10):  # 10 epochs for training\n",
    "    losses = {}\n",
    "    # Shuffle and batch the examples\n",
    "    # Example usage of batching (make sure you shuffle the training data)\n",
    "    # this is a simple approach with no batch strategy.\n",
    "    for batch in spacy.util.minibatch(train_examples, size=2):  \n",
    "        nlp.update(batch, losses=losses)\n",
    "    print(f\"Epoch {epoch} Losses: {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81bb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# Step 4: Test the classifier on new sentences\n",
    "test_sentences = [\n",
    "    \"I really enjoyed this movie\",\n",
    "    \"This movie was awful\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    doc = nlp(sentence)\n",
    "    print(f\"Sentence: '{sentence}' => Predicted Sentiment: {'positive' if doc.cats['pos'] > doc.cats['neg'] else 'negative'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f6ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb6a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f44fa666",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>Comprehensive Text Classification with NLTK including feature engineering (Optional)\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af22ee1",
   "metadata": {},
   "source": [
    "**What We Will Cover**  \n",
    "1. Understanding Text Classification  \n",
    "2. Dataset Preparation  \n",
    "3. Preprocessing Steps  \n",
    "4. Feature Extraction  \n",
    "5. Training a Naive Bayes Classifier  \n",
    "6. Testing and Evaluating the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba5e436",
   "metadata": {},
   "source": [
    "**1. Understanding Text Classification**  \n",
    "Text classification involves assigning a category to a given piece of text. For example:  \n",
    "- Labeling emails as **\"Spam\"** or **\"Not Spam.\"**  \n",
    "- Categorizing movie reviews as **\"Positive\"** or **\"Negative.\"**  \n",
    "\n",
    "In this exercise, we will classify movie reviews as either **Positive** or **Negative** using NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9997e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d67b8e",
   "metadata": {},
   "source": [
    "**2. Dataset Preparation**  \n",
    "NLTK comes with a built-in dataset for movie reviews called `movie_reviews`. Each review in this dataset is pre-labeled as positive or negative.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca699b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary datasets\n",
    "nltk.download('movie_reviews')  # Movie review dataset\n",
    "nltk.download('stopwords')      # Stopwords for preprocessing\n",
    "nltk.download('punkt')          # Tokenizer for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b18ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# Extract movie reviews and their associated labels\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f8900",
   "metadata": {},
   "source": [
    "**3. Preprocessing Steps**  \n",
    "Before training the classifier, we need to preprocess the text:  \n",
    "1. **Tokenization:** Breaking text into individual words.  \n",
    "2. **Removing Stop Words:** Eliminating common, unimportant words like \"the\" and \"is.\"  \n",
    "3. **Lowercasing:** Converting all words to lowercase to maintain consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4220815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n",
      "Lowercased Words: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n",
      "Alphabetic Words: ['plot', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', 'drink', 'and', 'then', 'drive', 'they', 'get', 'into', 'an', 'accident', 'one', 'of']\n",
      "Filtered Words (No Stopwords): ['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'deal']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# Example of tokenizing one document for clarity\n",
    "sample_document = documents[0][0]  # Get the first document (words)\n",
    "print(f\"Original Document: {sample_document[:20]}\")  # Print the first 20 words for reference\n",
    "\n",
    "# Lowercasing\n",
    "# Convert all words to lowercase\n",
    "lowercased_words = [word.lower() for word in sample_document]\n",
    "print(f\"Lowercased Words: {lowercased_words[:20]}\")\n",
    "\n",
    "# Removing Non-Alphabetic Tokens\n",
    "# Remove words that are not purely alphabetic\n",
    "alphabetic_words = [word for word in lowercased_words if word.isalpha()]\n",
    "print(f\"Alphabetic Words: {alphabetic_words[:20]}\")\n",
    "\n",
    "# Stop Word Removal\n",
    "# Load stopwords and remove them from the dataset\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in alphabetic_words if word not in stop_words]\n",
    "print(f\"Filtered Words (No Stopwords): {filtered_words[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7144e",
   "metadata": {},
   "source": [
    "**4. Feature Extraction**  \n",
    "We will use the following method for feature extraction:  \n",
    "- Represent each review as a list of words and create a feature set where each word's presence is marked as **True** or **False.**  \n",
    "- The goal is to extract features from each document in the dataset after preprocessing. These features are required for training the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "097b83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Features\n",
    "\n",
    "# Create a list of all words in the dataset\n",
    "all_words = nltk.FreqDist(word.lower() for word in movie_reviews.words() if word.isalpha())\n",
    "\n",
    "# Use the 2000 most common words as features\n",
    "word_features = list(all_words.keys())[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf6e5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feature extractor function\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {word: (word in document_words) for word in word_features}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ff52033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all documents\n",
    "preprocessed_documents = []\n",
    "for (doc, category) in documents:\n",
    "    # Step 1: Lowercase and remove stopwords + non-alphabetic tokens\n",
    "    filtered_words = [word.lower() for word in doc if word.isalpha() and word.lower() not in stop_words]\n",
    "    # Step 2: Apply feature extraction\n",
    "    features = document_features(filtered_words)\n",
    "    preprocessed_documents.append((features, category))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea5ff2",
   "metadata": {},
   "source": [
    "**5. Training a Naive Bayes Classifier**  \n",
    "Naive Bayes is a simple yet powerful algorithm for text classification. It works well with small datasets and uses probabilities to predict the most likely category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5387bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "train_set = preprocessed_documents[:1600]\n",
    "test_set = preprocessed_documents[1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b455851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49db32",
   "metadata": {},
   "source": [
    "**6. Testing and Evaluating the Model**  \n",
    "After training, we'll test the classifier on new data to measure its accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08e9c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.25%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "print(f\"Accuracy: {accuracy(classifier, test_set) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec4e5c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test review: neg\n"
     ]
    }
   ],
   "source": [
    "# Test with New Data\n",
    "test_review = \"This movie was absolutely amazing, with great performances and a captivating story.\"\n",
    "test_tokens = nltk.word_tokenize(test_review)\n",
    "test_words = [word.lower() for word in test_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "test_features = document_features(test_words)\n",
    "print(f\"Prediction for test review: {classifier.classify(test_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2f81b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Informative Features:\n",
      "Most Informative Features\n",
      "                   chick = True              neg : pos    =      8.6 : 1.0\n",
      "                 frances = True              pos : neg    =      8.3 : 1.0\n",
      "              undercover = True              neg : pos    =      7.8 : 1.0\n",
      "              derivative = True              neg : pos    =      7.0 : 1.0\n",
      "                  inject = True              neg : pos    =      7.0 : 1.0\n",
      "                 justify = True              neg : pos    =      6.2 : 1.0\n",
      "                   banal = True              neg : pos    =      5.8 : 1.0\n",
      "                bothered = True              neg : pos    =      5.8 : 1.0\n",
      "                     ugh = True              neg : pos    =      5.8 : 1.0\n",
      "                   waste = True              neg : pos    =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Display the Most Informative Features\n",
    "print(\"\\nMost Informative Features:\")\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbe9fa",
   "metadata": {},
   "source": [
    "Feature (e.g., \"chick\"): These are the words found in the text that are most useful for predicting sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cca78",
   "metadata": {},
   "source": [
    "Association (e.g., \"neg : pos = 8.6 : 1.0\"): This shows how strongly the word is linked to one class. For example, \"chick\" is 8.6 times more likely to appear in negative reviews than positive ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e060c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72805e4c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Additional Tools used in NLP\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbddee5",
   "metadata": {},
   "source": [
    "**1. Preprocessing Tools**  \n",
    "- **NLTK (Natural Language Toolkit):** A comprehensive library for basic NLP tasks such as tokenization, stemming, lemmatization, and syntactic parsing.  \n",
    "- **SpaCy:** An efficient library for advanced NLP tasks like dependency parsing, named entity recognition (NER), and word embeddings.  \n",
    "\n",
    "**2. Text Representation Tools**  \n",
    "- **Gensim:** A library for creating word embeddings and topic modeling using techniques like Word2Vec and LDA.  \n",
    "- **Transformers (by Hugging Face):** Provides access to pre-trained models like BERT and GPT for text embeddings and advanced NLP tasks.  \n",
    "\n",
    "**3. Data Manipulation and Visualization Tools**  \n",
    "- **Pandas:** A powerful library for cleaning, organizing, and manipulating textual datasets.  \n",
    "- **Matplotlib/Seaborn:** Libraries for visualizing data trends, such as word frequencies and sentiment scores.  \n",
    "- **WordCloud:** A simple tool for creating visual word clouds to represent text data.  \n",
    "\n",
    "**4. Machine Learning and Deep Learning Tools**  \n",
    "- **Scikit-Learn:** A versatile library for training machine learning models like Naive Bayes and SVM on text data.  \n",
    "- **TensorFlow/Keras:** A deep learning framework for building and training neural networks for NLP tasks.  \n",
    "- **PyTorch:** A flexible deep learning library for implementing advanced NLP models.  \n",
    "\n",
    "**5. Data Acquisition Tools**  \n",
    "- **BeautifulSoup:** A library for web scraping and extracting text data from HTML pages.  \n",
    "- **Scrapy:** An advanced tool for building web scrapers to gather large amounts of textual data.  \n",
    "- **Tweepy:** A Python library for accessing Twitter data via the Twitter API.  \n",
    "\n",
    "**6. Annotation Tools**  \n",
    "- **Label Studio:** A user-friendly tool for manually annotating text datasets for training machine learning models.  \n",
    "- **Prodigy:** A more advanced annotation tool that incorporates active learning to improve dataset quality.  \n",
    "\n",
    "**7. Cloud NLP Tools (Optional)**  \n",
    "- **Google Cloud Natural Language API:** A cloud-based service for sentiment analysis, entity extraction, and more.  \n",
    "- **AWS Comprehend:** Amazon’s NLP service for text classification, entity recognition, and sentiment analysis.  \n",
    "- **Azure Text Analytics:** A cloud tool for performing key phrase extraction, sentiment analysis, and more.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2185430-3791-45df-824f-bdec6d7145e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Live Exercise</font> </b>\n",
    "</div>\n",
    "\n",
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d61af-f3e8-4afc-a66b-3814e160aaf3",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task 1: description of task\n",
    "\n",
    "    - instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9ddd9-5558-4b1d-a3e7-04c3dca33b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "1. topic:\n",
    "    - question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b86c1-64b0-4abd-8ba9-54746bdc9007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

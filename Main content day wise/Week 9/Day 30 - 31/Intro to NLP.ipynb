{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Introduction to NLP <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Basics of text preprocessing</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.  Introduction to NLP<p><p> \n",
    "2.  Text Preprocessing <p>\n",
    "3.  Text Preprocessing in Python<p>\n",
    "4.  Introduction to Twitter Sentiment Analyzer <p>\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "   If you could live in any fictional world, which one would it be and why?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Introduction to NLP\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aef031-1416-4707-b2a0-fc60ca4ee7ca",
   "metadata": {},
   "source": [
    "**Natural Language Processing (NLP)** is a subfield of Artificial Intelligence (AI) that focuses on the interaction between computers and human (natural) languages. \n",
    "\n",
    "It enables machines to understand, interpret, and generate human language in a way that is meaningful and useful.\n",
    "\n",
    "NLP lies at the intersection of **linguistics**, **computer science**, and **AI**, combining the rules and structure of **human language** with **computational algorithms** to bridge the gap between human communication and machine understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728d384",
   "metadata": {},
   "source": [
    "[A Quick Fun Introduction](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b68b2",
   "metadata": {},
   "source": [
    "**Domains of AI**\n",
    "\n",
    "- Computer Vision [Image and Video - Image Processing -> CNN and RNN]\n",
    "- Natural Language Processing [Text and Audio] Chatbots, Alexa. Siri\n",
    "- Statistical Data [ CSV,Excel -> Recommendation & Prediction using historical data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd171a",
   "metadata": {},
   "source": [
    "### **Why Do We Need NLP?**\n",
    "\n",
    "1. **Human Language is Complex:**\n",
    "- Human language is inherently ambiguous, nuanced, and diverse. Teaching machines to comprehend slang, context, sentiment, grammar, and syntax requires robust systems like NLP.\n",
    "\n",
    "\n",
    "\n",
    "2. **Exponential Growth of Data:**\n",
    "- With the explosion of digital communication, vast amounts of unstructured data (emails, social media, reviews, articles, etc.) are generated daily. NLP helps extract valuable insights from this data.\n",
    "\n",
    "3. **Efficient Human-Machine Interaction:**\n",
    "- NLP enables more natural and effective communication between humans and machines, eliminating the need for specialized commands or programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7853478",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*2HxRh65O96c_pv7v.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba805597",
   "metadata": {},
   "source": [
    "**Text Classification:**\n",
    "- Categorizing text into predefined labels, such as spam detection or sentiment analysis.  \n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/guides/text-classification/images/TextClassificationExample.png)\n",
    "\n",
    "**Information Retrieval:**\n",
    "- Extracting and ranking relevant data or insights from large datasets based on specific user queries.\n",
    "\n",
    "**Natural Language Understanding:** \n",
    "- Recognizing patterns, analyzing meaning, and deriving insights from text or speech.\n",
    "\n",
    "**Natural Language Generation:** \n",
    "- Crafting coherent and contextually accurate responses, such as **chatbots** and **virtual assistants**.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*qR1q1Nqc9c_snmmGgP9fXA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565734ac",
   "metadata": {},
   "source": [
    "### **Applications of NLP in the Real World**\n",
    "\n",
    "1. **Personal Assistants**: \n",
    "- Google Assistant, Siri, and Alexa rely on NLP to understand and respond to voice commands.  \n",
    "2. **Machine Translation**: \n",
    "- Tools like Google Translate leverage NLP for accurate translations across languages.  \n",
    "3. **Customer Support**: \n",
    "- Chatbots powered by NLP provide 24/7 assistance for customer queries.  \n",
    "4. **Sentiment Analysis**: \n",
    "- Businesses use NLP to analyze customer feedback and social media sentiment.  \n",
    "5. **Text Summarization**: \n",
    "- Automatically condensing large texts into summaries (e.g., news articles).  \n",
    "6. **Healthcare**: \n",
    "- Extracting information from patient records or assisting in diagnostics.  \n",
    "7. **Search Engines**: \n",
    "- Enhancing user queries for more relevant search results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ace57a",
   "metadata": {},
   "source": [
    "### **Historical Background**\n",
    "\n",
    "1. **1950s – Foundational Era**:  \n",
    "   - Alan Turing proposed the **Turing Test** to assess a machine's ability to exhibit intelligent behavior akin to humans.  \n",
    "   - Early machine translation efforts, such as the **Georgetown-IBM experiment**, aimed to translate Russian to English.  \n",
    "\n",
    "2. **1960s – Rule-Based Systems**:  \n",
    "   - Initial NLP systems were based on hand-crafted rules and linguistic grammar.  \n",
    "\n",
    "3. **1980s – Statistical Methods**:  \n",
    "   - The introduction of statistical and probabilistic models revolutionized NLP, making systems more scalable and robust.  \n",
    "\n",
    "4. **2000s – Data-Driven Approaches**:  \n",
    "   - The rise of machine learning (ML) techniques, particularly supervised learning, boosted NLP's capabilities.  \n",
    "\n",
    "5. **2010s – Deep Learning Revolution**:  \n",
    "   - Neural networks, transformers (like **BERT**, **GPT**), and large language models led to unprecedented advancements in NLP.  \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cb306",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe110ac",
   "metadata": {},
   "source": [
    "### **Importance of NLP for Data Analytics**\n",
    "\n",
    "1. **Handling Unstructured Data:**  \n",
    "   Over 80% of data generated is unstructured, like text in emails, reviews, or social media posts. NLP helps convert this data into structured formats for analysis.\n",
    "\n",
    "2. **Extracting Insights:**  \n",
    "   NLP enables sentiment analysis, topic modeling, and keyword extraction, helping analysts uncover trends, customer opinions, and patterns in textual data.\n",
    "\n",
    "3. **Automation of Analysis:**  \n",
    "   Automates repetitive tasks such as text classification, summarization, or tagging, improving efficiency and scalability.\n",
    "\n",
    "4. **Improved Decision-Making:**  \n",
    "   By analyzing textual feedback, NLP provides actionable insights for businesses to refine strategies and improve customer satisfaction.\n",
    "\n",
    "5. **Enhancing Data Queries:**  \n",
    "   NLP-powered search and query systems allow users to interact with data analytics tools in natural language, making analytics more accessible.\n",
    "\n",
    "Thus, NLP enhances data analytics by enabling meaningful analysis of textual data, thus broadening the scope and impact of insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b26b0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Processes in NLP\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787de69",
   "metadata": {},
   "source": [
    "There is no one pipeline tho:\n",
    "\n",
    "![](https://d2mk45aasx86xg.cloudfront.net/Natural_language_processing_pipeline_e3608ff95c.webp)\n",
    "\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20230301155815/Phases-of-Natural-Language-Processing.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MJD9QZYhdVvgyIrtLgIW5Q.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*xd4-nuFPrLz7cjdz2aEWyQ.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e10aec",
   "metadata": {},
   "source": [
    "### **Comprehensive NLP Pipeline**\n",
    "\n",
    "A well-defined NLP pipeline involves several processes that transform raw text into structured, meaningful data. Here's the sequence of processes commonly followed:\n",
    "\n",
    "\n",
    "#### 1. **Text Input** *Raw text data from documents, web pages, social media, etc.*  \n",
    "   - The text data is collected and fed into the system for processing.\n",
    "\n",
    "---\n",
    "#### 2. **Text Preprocessing** *The text is cleaned and prepared for further analysis.*  \n",
    "   - **Tokenization** The text is split into smaller units like words or sentences.  \n",
    "   - **Lowercasing** All characters are converted to lowercase for consistency.  \n",
    "   - **Stopword Removal** Common, non-informative words (like \"and\" or \"the\") are removed.  \n",
    "   - **Punctuation Removal**   Unnecessary punctuation marks (e.g., commas, periods) are removed.  \n",
    "   - **Stemming/Lemmatization**  Words are reduced to their root form (e.g., \"running\" → \"run\").  \n",
    "   - **Spell Correction** Misspelled words are corrected to their correct form.\n",
    "---\n",
    "\n",
    "\n",
    "#### 3. **Text Representation** *Converting text into a machine-readable format.*  \n",
    "   - **Bag of Words (BoW)** The text is represented as a vector with the frequency of each word.  \n",
    "   - **TF-IDF** Weights are applied to words based on their frequency and importance in the document.  \n",
    "   - **Word Embeddings (e.g., Word2Vec, GloVe)** Words are converted into dense vectors that capture their meaning.  \n",
    "   - **Sentence Embeddings (e.g., BERT)** Entire sentences are represented as vectors that capture the overall meaning.\n",
    "---\n",
    "\n",
    "#### 4. **Named Entity Recognition (NER)** *Identify entities like names, dates, organizations.*  \n",
    "   - Specific entities such as people, locations, and dates are recognized and classified.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Syntactic Analysis** *Analyzing the grammatical structure of the text.*  \n",
    "   - **Part-of-Speech (POS) Tagging** Each word is labeled with its grammatical role (e.g., noun, verb).  \n",
    "   - **Dependency Parsing**  The relationships between words in a sentence are identified.  \n",
    "   - **Constituency Parsing**  The sentence structure is broken down into nested components (e.g., noun phrases, verb phrases).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 6. **Feature Engineering** *Extracting and selecting meaningful features to enhance model performance.*  \n",
    "   - **N-gram Extraction**  Word combinations (e.g., bigrams, trigrams) are generated for analysis.  \n",
    "   - **TF-IDF Scaling**   Weights are applied to words to highlight their importance in the context.  \n",
    "   - **Custom Features** Specific features are created based on domain knowledge or problem requirements.  \n",
    "   - **Sentiment Scores** Emotional tone (positive, negative, neutral) is extracted from the text.  \n",
    "   - **POS Tags as Features** Part-of-speech tags are included as features to improve model predictions.  \n",
    "   - **Text Length Metrics** Features like sentence or document length are included for better analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Task-Specific Processes** *Processing based on the desired NLP task.*  \n",
    "   - **Sentiment Analysis** The emotional tone of the text is classified (positive, negative, neutral).  \n",
    "   - **Text Summarization** A concise version of the text is generated.  \n",
    "   - **Text Classification** The text is assigned to a predefined category or label.  \n",
    "   - **Machine Translation**   The text is translated from one language to another.\n",
    "---\n",
    "#### 8. **Output Generation** *Generating meaningful insights or predictions based on the processed text.*  \n",
    "   - The system produces structured output such as predictions, translations, summaries, or classifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b10a45",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Text Preprocessing\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b5802",
   "metadata": {},
   "source": [
    "Text preprocessing is the foundation of NLP, transforming raw text into a clean format suitable for analysis. \n",
    "\n",
    "Here are key steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb065377",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 1. Tokenization\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065d337",
   "metadata": {},
   "source": [
    "\n",
    "- Splitting text into smaller units, like words or sentences.  \n",
    "- Helps in analyzing text at the word or sentence level.  \n",
    "- **Example**:  \n",
    "  - Input: *\"Natural language processing is fascinating.\"*  \n",
    "  - Output: `[\"Natural\", \"language\", \"processing\", \"is\", \"fascinating\"]`  \n",
    "\n",
    "- Scenario:\n",
    "  - In real-time, when analyzing product reviews, tokenizing the sentences allows you to understand each word individually.\n",
    "  - For instance, in a customer review like \"The laptop is fast and efficient,\" tokenization separates the words \"laptop,\" \"fast,\" and \"efficient,\" which can help identify the core sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3c6db",
   "metadata": {},
   "source": [
    "![](https://smltar.com/diagram-files/tokenization-black-box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552128f",
   "metadata": {},
   "source": [
    "**Types of Tokenization**\n",
    "\n",
    "1. *Word Tokenization*:  \n",
    "   - Splitting text into individual words, typically by spaces or punctuation marks.  \n",
    "\n",
    "2. *Sentence Tokenization*:  \n",
    "   - Dividing text into individual sentences based on punctuation or predefined rules.  \n",
    "\n",
    "3. *Subword Tokenization*:  \n",
    "   - Breaking words into smaller units, such as prefixes, suffixes, or characters (used in models like BERT and GPT).  \n",
    "\n",
    "4. *Character Tokenization*:  \n",
    "   - Splitting text into individual characters, often used in languages with complex word structures.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8f655",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 2. Stopword Removal\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6fed1",
   "metadata": {},
   "source": [
    "\n",
    "- Removing common common, frequently occurring words that don’t carry much meaning (e.g., *is*, *the*, *and*).  \n",
    "- Reduces noise in text analysis. \n",
    "- **Example**:  \n",
    "  - Input: *\"The cat is on the mat.\"*  \n",
    "  - Output: `[\"cat\", \"mat\"]`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9003e11",
   "metadata": {},
   "source": [
    "![](https://ik.imagekit.io/Botpenguin/assets/website/Stop_Words_d01a1e75a3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c74ec2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Key Points to Consider:**\n",
    "1. **Language-Specific**: Stop words vary across different languages, and the list can be customized based on the context of the task.\n",
    "2. **Context Matters**: In some cases, stop words may carry important meaning (e.g., in sentiment analysis), and removing them may not always be ideal.\n",
    "3. **Efficiency**: Removing stop words can improve the efficiency of processing, especially for large datasets, by reducing dimensionality.\n",
    "4. **Custom Stop Words**: A user-defined stop word list can be created based on domain-specific knowledge, improving the accuracy of the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29aaf8b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 3. Stemming\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1dd4e",
   "metadata": {},
   "source": [
    "\n",
    "- Reducing words to their root form by chopping off suffixes.  \n",
    "- Groups similar words for analysis (e.g., *run*, *running* → *run*).  \n",
    "- **Example**:  \n",
    "  - Input: *\"running, runner\"*  \n",
    "  - Output: `[\"run\", \"run\"]`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcffaad",
   "metadata": {},
   "source": [
    "- To simplify words we cut off parts of the word, called *affixes*, to get to its root form or base word. \n",
    "\n",
    "**Affixes**:  \n",
    "- Affixes are parts that are added to the base word (or root) to change its meaning. There are two main types of affixes:\n",
    "  1. **Prefixes**: Added to the beginning of a word \n",
    "  - (e.g., \"*un-*\" in \"*undo*\").\n",
    "  2. **Suffixes**: Added to the end of a word \n",
    "  - (e.g., \"*-ing*\" in \"*running*\" or \"*-ed*\" in \"*played*\").\n",
    "\n",
    "3. **Infixes**:  \n",
    "   - These are inserted *within* a word. Infixes are rare in English but are found in some other languages.  \n",
    "   - Example: In some informal English, \"*abso-bloody-lutely*\" (inserting \"bloody\" for emphasis).\n",
    "\n",
    "4. **Circumfixes**:  \n",
    "   - These are added *around* a word (both at the beginning and end). Circumfixes are not common in English but can be found in other languages.  \n",
    "   - Example: *en-* and *-en* are added to verbs to form the past participle (e.g., *en*light*en* from *enlighten*, meaning \"to give someone greater knowledge or understanding\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d4dca",
   "metadata": {},
   "source": [
    "![](https://i.ytimg.com/vi/B9UP8P-TAXc/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17898b7e",
   "metadata": {},
   "source": [
    "**How Stemming Works**:  \n",
    "- Stemming removes these affixes to get to the base or root form of the word. For example:\n",
    "  - \"running\" → \"run\"\n",
    "  - \"happier\" → \"happi\"  \n",
    "  - \"quickly\" → \"quick\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9bdff",
   "metadata": {},
   "source": [
    "**Why Use Stemming?**  \n",
    "- Stemming helps in text analysis by treating different forms of the same word as equivalent, reducing the complexity of the text and improving tasks such as:\n",
    "  - **Search**: Finding related words or content regardless of their forms.\n",
    "  - **Text Classification**: Grouping similar meanings together, no matter how the word is written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944af2b",
   "metadata": {},
   "source": [
    "**Challenges**\n",
    "- Stemming can sometimes produce words that don’t exist or make sense (e.g., \"happier\" becomes \"happi\"), but it simplifies language for analysis, especially when accuracy isn’t the priority over generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320626aa",
   "metadata": {},
   "source": [
    "![](https://cdn.botpenguin.com/assets/website/Stemming_IR_346db34f6c.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45beb2d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 4. Lemmatization\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc2857",
   "metadata": {},
   "source": [
    "- Converting words to their base or dictionary form, considering grammar.  \n",
    "- Provides contextually meaningful base forms.  \n",
    "- **Example**:  \n",
    "  - Input: *\"better\"*  \n",
    "  - Output: `[\"good\"]` \n",
    "\n",
    "  Lemmatization is the process of reducing a word to its base form (called a *lemma*), ensuring that the word is returned to its correct dictionary form. \n",
    "  \n",
    "  Unlike stemming, which may cut off parts of words, lemmatization considers the meaning and context of the word to ensure the base form is grammatically correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc4a470",
   "metadata": {},
   "source": [
    "![](https://d2mk45aasx86xg.cloudfront.net/difference_between_Stemming_and_lemmatization_11zon_393419a8d0.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bd9c4",
   "metadata": {},
   "source": [
    "![](https://www.thinkdataanalytics.com/wp-content/uploads/2021/07/stemming-vs-lemmatization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25f0f6",
   "metadata": {},
   "source": [
    "**How Lemmatization Works:**\n",
    "- Lemmatization uses a dictionary and part-of-speech (POS) tagging to determine the correct base form of a word.\n",
    "- Part-of-Speech (POS) refers to the grammatical categories that words belong to, such as noun, verb, adjective, adverb, etc., based on their function in a sentence.\n",
    "  - Example: \"was\" (verb) → \"be\"\n",
    "  - Example: \"geese\" (plural noun) → \"goose\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309a709",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Why Use Lemmatization?**\n",
    "- Lemmatization is more accurate than stemming because it produces valid words, making it better suited for tasks like:\n",
    "  - **Text Classification**: Grouping similar words together without losing meaning.\n",
    "  - **Sentiment Analysis**: Understanding the context and ensuring proper interpretation of words.\n",
    "  - **Search Engines**: Improving results by matching terms to their base form.\n",
    "\n",
    "**Challenges**\n",
    "- Lemmatization requires more computational resources than stemming because it uses a dictionary and grammatical context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7fa9f2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Text Preprocessing in Python\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e158f",
   "metadata": {},
   "source": [
    "Python libraries like NLTK and spaCy are powerful tools for text preprocessing in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5ec62",
   "metadata": {},
   "source": [
    "1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66bb8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880781c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\devid/nltk_data'\n    - 'c:\\\\Users\\\\devid\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\devid\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\devid\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\devid\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNatural Language Processing is exciting! Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms learn it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Word Tokenization\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, words)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Sentence Tokenization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\devid/nltk_data'\n    - 'c:\\\\Users\\\\devid\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\devid\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\devid\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\devid\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing is exciting! Let's learn it.\"\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokens:\", words)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topwords are common words (e.g., the, is) that add little meaning.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already done\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove Stopwords\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"Filtered Words:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed82f85",
   "metadata": {},
   "source": [
    "Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ccdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Word Tokens\n",
    "print(\"Word Tokens:\", [token.text for token in doc])\n",
    "\n",
    "# Sentence Tokens\n",
    "print(\"Sentence Tokens:\", [sent.text for sent in doc.sents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de3656",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Remove Stopwords\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered Words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, filtered_words)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Filtered Words:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5865a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> sub heading 1\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0d089",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Text Classification Basics\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efd47c",
   "metadata": {},
   "source": [
    "Concept of classification in NLP.\n",
    "Using NLTK for basic classification (e.g., Naive Bayes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6b263",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> sub heading 1\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5373419",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>4. Introduction to Twitter Sentiment Analyzer\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3005b9",
   "metadata": {},
   "source": [
    "bjective: Analyze sentiment from a small dataset of Twitter posts (manually or pre-curated).\n",
    "Overview of the dataset: sentiment-labeled tweets (e.g., positive, neutral, negative).\n",
    "Steps to preprocess tweets (e.g., handle hashtags, mentions, emojis, and URLs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d7fbd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> sub heading 1\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14dd0a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>5. Dataset Preparation\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b38396",
   "metadata": {},
   "source": [
    "Downloading or providing a small, curated dataset of tweets.\n",
    "Applying preprocessing steps using NLTK and SpaCy:\n",
    "Tokenization, stopword removal, stemming/lemmatization.\n",
    "Cleaning text (removing special characters, URLs, and mentions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466dc8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e321b71",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> sub heading 1\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4b008",
   "metadata": {},
   "source": [
    "https://developers.google.com/machine-learning/guides/text-classification/\n",
    "http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n",
    "https://huggingface.co/learn/nlp-course/en/chapter1/1\n",
    "https://www.datacamp.com/tutorial/nlp-with-pytorch-a-comprehensive-guide\n",
    "https://campus.datacamp.com/courses/natural-language-processing-with-spacy/introduction-to-nlp-and-spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2185430-3791-45df-824f-bdec6d7145e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Live Exercise</font> </b>\n",
    "</div>\n",
    "\n",
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d61af-f3e8-4afc-a66b-3814e160aaf3",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task 1: description of task\n",
    "\n",
    "    - instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9ddd9-5558-4b1d-a3e7-04c3dca33b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "1. topic:\n",
    "    - question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b86c1-64b0-4abd-8ba9-54746bdc9007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

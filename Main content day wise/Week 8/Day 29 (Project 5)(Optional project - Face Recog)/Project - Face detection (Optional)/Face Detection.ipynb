{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 50px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 40px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif; border-radius: 100px;\">\n",
    "    Face Detection and Identification System <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\"></h3>\n",
    "</div\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100%; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3> PROJECT DESCRIPTION \n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb1cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python tensorflow keras scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515efbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries in the notebook\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a695a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam using OpenCV\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811ea883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture frames\n",
    "\n",
    "ret, frame = cap.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf589f",
   "metadata": {},
   "source": [
    "Save the captured images for labeled classes:\n",
    "- Display the live feed.\n",
    "- Use key events (`q`,`s`) to quit or save the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5e1d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 's' to save the image, 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# code for capturing and saving images\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Create directories for each person (label)\n",
    "label = \"person1\"  # Change this for different classes\n",
    "output_dir = f\"./data/{label}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 's' to save the image, 'q' to quit.\")\n",
    "\n",
    "image_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('s'):  # Save the frame\n",
    "        image_path = os.path.join(output_dir, f\"{label}_{image_count}.jpg\")\n",
    "        cv2.imwrite(image_path, frame)\n",
    "        image_count += 1\n",
    "        print(f\"Saved: {image_path}\")\n",
    "    \n",
    "    elif key == ord('q'):  # Quit the capture\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed41c1",
   "metadata": {},
   "source": [
    "**2. Detect and Crop Faces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26359f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory where images are saved\n",
    "image_dir = \"./data/person1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd950e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using image: ./data/person1\\person1_4.jpg\n"
     ]
    }
   ],
   "source": [
    "# Get the latest saved image from the directory\n",
    "images = sorted(os.listdir(image_dir), key=lambda x: os.path.getctime(os.path.join(image_dir, x)))\n",
    "if len(images) == 0:\n",
    "    print(\"No images found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "latest_image_path = os.path.join(image_dir, images[-1])\n",
    "print(f\"Using image: {latest_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9959ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the latest image\n",
    "image = cv2.imread(latest_image_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d563dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haar Cascade face detection model using OpenCV.\n",
    "# This model will be used to detect faces in the captured images.\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0108ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb8f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection with adjusted parameters\n",
    "faces = face_cascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.05,  # Reduce the scale step for finer detection\n",
    "    minNeighbors=6,    # Increase for stricter face detection\n",
    "    minSize=(50, 50)   # Minimum face size to detect\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e8cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes around detected faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14f22f",
   "metadata": {},
   "source": [
    "`cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)` \n",
    "- faces is a list of tuples where each tuple represents a detected face.\n",
    "- (x, y) - Coordinates of the top-left corner of the bounding box.\n",
    "- w and h - Width and Height of the bounding box.\n",
    "- (255, 0, 0),2: The color of the rectangle (in BGR format, this is blue) with 2 thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e45be",
   "metadata": {},
   "source": [
    "Example: `faces = [(50, 50, 100, 100), (200, 150, 80, 80)]`\n",
    "\n",
    "For the first face:\n",
    "- (x, y) = (50, 50)\n",
    "- (x + w, y + h) = (150, 150)\n",
    "\n",
    "For the second face:\n",
    "- (x, y) = (200, 150)\n",
    "- (x + w, y + h) = (280, 230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cdbb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image with bounding boxes\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae9913f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Draw bounding boxes and add labels\n",
    "if len(faces) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw rectangle with custom color and thickness\n",
    "        color = (0, 255, 0)  # Green color for the box\n",
    "        thickness = 2\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, thickness)\n",
    "        \n",
    "        # Add label above the rectangle\n",
    "        label = \"Face\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        font_thickness = 1\n",
    "        label_size, _ = cv2.getTextSize(label, font, font_scale, font_thickness)\n",
    "        \n",
    "        label_x = x\n",
    "        label_y = y - 10 if y - 10 > 10 else y + 10  # Adjust position if label goes out of bounds\n",
    "        cv2.rectangle(image, (label_x, label_y - label_size[1] - 2), (label_x + label_size[0], label_y + 2), color, cv2.FILLED)\n",
    "        cv2.putText(image, label, (label_x, label_y), font, font_scale, (0, 0, 0), font_thickness)\n",
    "\n",
    "# Display the image with bounding boxes and labels\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f655a7",
   "metadata": {},
   "source": [
    "**Preprocess and Crop Faces**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d15ca",
   "metadata": {},
   "source": [
    "- Load saved images\n",
    "- Detect faces in each image using Haar Cascade\n",
    "- Crop the face regions for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ec2dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed face: ./processed_faces/face_0.jpg\n"
     ]
    }
   ],
   "source": [
    "output_dir = './processed_faces/'  # Directory to save preprocessed faces\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if len(faces) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    # Loop through detected faces\n",
    "    for idx, (x, y, w, h) in enumerate(faces):\n",
    "        # Crop the face\n",
    "        cropped_face = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize to a uniform size (e.g., 160x160 for models like FaceNet)\n",
    "        resized_face = cv2.resize(cropped_face, (160, 160))\n",
    "        \n",
    "        # Save the preprocessed face\n",
    "        face_path = os.path.join(output_dir, f\"face_{idx}.jpg\")\n",
    "        cv2.imwrite(face_path, resized_face)\n",
    "        print(f\"Saved preprocessed face: {face_path}\")\n",
    "        \n",
    "        # Display the preprocessed face (optional)\n",
    "        cv2.imshow(\"Preprocessed Face\", resized_face)\n",
    "        cv2.waitKey(0) # The 0 argument tells OpenCV to wait indefinitely for the user to press any key\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94004bbf",
   "metadata": {},
   "source": [
    "**3. Generate Face Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8e1e4",
   "metadata": {},
   "source": [
    "Generating face embeddings is about converting a face image into a numerical representation (a vector of numbers) that *captures its unique features*.\n",
    "\n",
    "- Input: You give the model a preprocessed face image (e.g., cropped and resized).\n",
    "\n",
    "- Model Processing: The model analyzes the facial features, like the shape of the eyes, nose, and mouth.\n",
    "\n",
    "- Output (Embedding): The model outputs a set of numbers (embedding) that uniquely represents the face, making it easy to compare with other faces for tasks like recognition or clustering.\n",
    "\n",
    "Think of it as turning a face into a \"digital fingerprint\" for machines to understand.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5deef8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "model = MobileNetV2(weights='imagenet', \n",
    "                    include_top=False, \n",
    "                    input_shape=(160, 160, 3))\n",
    "\n",
    "# If you have downloaded a model\n",
    "# model = load_model('model.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bff04dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for embeddings and labels\n",
    "embeddings = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4bcd9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Generated embeddings for 1 faces.\n"
     ]
    }
   ],
   "source": [
    "# define the folder for preprocessed faces\n",
    "processed_faces_dir = './processed_faces'\n",
    "\n",
    "# Generate embeddings for all preprocessed faces\n",
    "\n",
    "# Iterate through all images in the directory\n",
    "for image_name in os.listdir(processed_faces_dir):\n",
    "    image_path = os.path.join(processed_faces_dir, image_name)\n",
    "\n",
    "    # Check if the file is a valid image (optional: you can adjust based on file types)\n",
    "    if image_name.endswith('.jpg') or image_name.endswith('.jpeg') or image_name.endswith('.png'):\n",
    "        # Load and preprocess image\n",
    "        face_image = cv2.imread(image_path)\n",
    "        face_image = cv2.resize(face_image, (160, 160))  # Ensure correct input size\n",
    "        face_preprocessed = face_image.astype('float32') / 255.0  # Normalize\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = model.predict(np.expand_dims(face_preprocessed, axis=0))[0]\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "        # Use the image filename as the label (without file extension)\n",
    "        label = os.path.splitext(image_name)[0]\n",
    "        labels.append(label)\n",
    "\n",
    "print(f\"Generated embeddings for {len(embeddings)} faces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a45de025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['face_0']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b78d6b",
   "metadata": {},
   "source": [
    "**4. Train a Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04baa124",
   "metadata": {},
   "source": [
    "- Collect embeddings and corresponding labels.\n",
    "- Train a KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9c2de36",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. KNeighborsClassifier expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train a KNN classifier\u001b[39;00m\n\u001b[0;32m      4\u001b[0m knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(embeddings, labels)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier trained.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:228\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# KNeighborsClassifier.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    209\u001b[0m )\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m        The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:456\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 456\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    457\u001b[0m             X, y, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m         )\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;66;03m# Classification targets require a specific format\u001b[39;00m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:953\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    951\u001b[0m     )\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    959\u001b[0m     _assert_all_finite(\n\u001b[0;32m    960\u001b[0m         array,\n\u001b[0;32m    961\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 4. KNeighborsClassifier expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(embeddings, labels)\n",
    "print(\"Classifier trained.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6236695",
   "metadata": {},
   "source": [
    "Test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e228c3c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test on a new face\u001b[39;00m\n\u001b[0;32m      2\u001b[0m new_face \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./processed_faces/test_face.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m new_embedding \u001b[38;5;241m=\u001b[39m get_embedding(facenet_model, new_face)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict the identity\u001b[39;00m\n\u001b[0;32m      6\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict([new_embedding])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "# Test on a new face\n",
    "new_face = cv2.imread('./processed_faces/test_face.jpg')\n",
    "new_embedding = get_embedding(facenet_model, new_face)\n",
    "\n",
    "# Predict the identity\n",
    "predicted_label = knn.predict([new_embedding])\n",
    "print(\"Predicted Identity:\", predicted_label[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51531ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0eb1bf25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93324ac3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

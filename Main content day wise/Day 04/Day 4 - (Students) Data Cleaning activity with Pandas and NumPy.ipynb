{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5078eee7-b0ef-4777-8657-00c57e7f82ea",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273f84a-ad59-4ad5-b826-12613ef16cc2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat<br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360f670-cd32-417c-be26-8438195b82df",
   "metadata": {},
   "source": [
    "<h1 style=\" background-color: #002147; color: White; padding: 30px; text-align:center\"> Application of NumPy and Pandas </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9ec83-cf6b-4000-aa3b-4b2194c2bbf1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 10px;\">\n",
    "    <h1> Data Cleaning\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "# Importance of Data Cleaning\n",
    "\n",
    "Data cleaning is a crucial step in the data analysis process, and here's why:\n",
    "\n",
    "1. **Improves Data Quality**: Clean data ensures accuracy and reliability. When data is free of errors, duplicates, and inconsistencies, you can trust the insights derived from it.\n",
    "\n",
    "2. **Enhances Decision-Making**: Businesses rely on data-driven decisions. Clean data provides a solid foundation for making informed choices, leading to better outcomes.\n",
    "\n",
    "3. **Increases Efficiency**: Time spent on analyzing messy data can be significantly reduced. By cleaning the data beforehand, analysts can focus on deriving insights rather than fixing issues.\n",
    "\n",
    "4. **Prevents Misleading Conclusions**: Outliers or erroneous entries can skew results, leading to incorrect interpretations. Data cleaning helps mitigate these risks, ensuring that conclusions are based on accurate representations.\n",
    "\n",
    "5. **Facilitates Collaboration**: Clean data is easier to share and understand. When teams work with standardized and accurate datasets, collaboration becomes smoother, fostering better communication and teamwork.\n",
    "\n",
    "6. **Regulatory Compliance**: Many industries have regulations regarding data accuracy and integrity. Cleaning data helps organizations comply with these standards, avoiding legal issues and penalties.\n",
    "\n",
    "In summary, data cleaning is not just a technical necessity; it's an essential practice for achieving reliable, actionable insights that drive success.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c67572-c7f8-45c7-9004-291d285901c8",
   "metadata": {},
   "source": [
    "### Skills Covered\n",
    "1. Understanding data cleaning concepts and importance.\n",
    "2. Using NumPy for basic array operations and handling missing data.\n",
    "3. Utilizing Pandas for data manipulation, including filtering, grouping, and sorting.\n",
    "4. Applying data transformation techniques such as renaming columns and handling duplicates.\n",
    "5. Implementing data visualization to identify outliers and data distribution.\n",
    "\n",
    "### Learning Outcomes\n",
    "1. Ability to clean and preprocess data for analysis.\n",
    "2. Proficiency in using NumPy and Pandas for data manipulation tasks.\n",
    "3. Understanding of how to handle missing values effectively.\n",
    "4. Skill in transforming data to make it suitable for analysis.\n",
    "5. Knowledge of visualizing data to aid in cleaning processes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h4> Simple Data Cleaning with Pandas \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b52451f-ae96-4100-8b42-51e406fc93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df= pd.read_csv('iris.csv') # Replace 'data.csv' with your actual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad678df8-48a1-4308-8e19-ce75f83e2cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n",
      " \n",
      " info \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             150 non-null    int64  \n",
      " 1   SepalLengthCm  150 non-null    float64\n",
      " 2   SepalWidthCm   150 non-null    float64\n",
      " 3   PetalLengthCm  150 non-null    float64\n",
      " 4   PetalWidthCm   150 non-null    float64\n",
      " 5   Species        150 non-null    object \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 7.2+ KB\n",
      "None\n",
      " \n",
      " describe\n",
      "\n",
      "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
      "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
      "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
      "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
      "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
      "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
      "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
      "max    150.000000       7.900000      4.400000       6.900000      2.500000\n"
     ]
    }
   ],
   "source": [
    "print(\" \\n head \\n\")\n",
    "# Display the first 5 rows\n",
    "print(df.head()) \n",
    "\n",
    "print(\" \\n info \\n\")\n",
    "# Display summary information\n",
    "print(df.info())\n",
    "\n",
    "print(\" \\n describe\\n\")\n",
    "# Display statistical summary of numerical columns\n",
    "print(df.describe())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4328f055-4016-4120-a343-90ac656425b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLong</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>142</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>126</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>111</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLong  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "141  142        6.9           3.1            5.1           2.3   \n",
       "125  126        7.2           3.2            6.0           1.8   \n",
       "110  111        6.5           3.2            5.1           2.0   \n",
       "51    52        6.4           3.2            4.5           1.5   \n",
       "80    81        5.5           2.4            3.8           1.1   \n",
       "\n",
       "             Species  \n",
       "141   Iris-virginica  \n",
       "125   Iris-virginica  \n",
       "110   Iris-virginica  \n",
       "51   Iris-versicolor  \n",
       "80   Iris-versicolor  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5) # to randomly select a specified number of rows or a fraction of rows from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f0fdbd2-9045-48b6-b995-e8ddeabc59d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id               0\n",
      "SepalLengthCm    0\n",
      "SepalWidthCm     0\n",
      "PetalLengthCm    0\n",
      "PetalWidthCm     0\n",
      "Species          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "missing_values = df.isnull().sum()  \n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea13a1-3341-4338-9ae3-fb78d7738dd6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: Black; padding: 10px; \">\n",
    "    <h4> Handling Missing Data\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e39e5-878b-449f-982e-2555d2b12122",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: Black;  \">\n",
    "    <h6> Remove rows with any missing values\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e36e6aef-426f-440b-8df8-734d00d2200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)  # Remove rows with any missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f36aa-6960-4f1c-b28d-9e6565ebd528",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: Black;  \">\n",
    "    <h6> Replace with 0\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7b10295-2333-497d-ad26-e729b52372b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value=0, inplace=True)  # Replace missing values with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20eefa-7912-48f3-b2bd-e1bcd106c1b0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: Black;  \">\n",
    "    <h6> Remove duplicate rows\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182b29f-2a34-403b-b615-9f258a3141e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)  # Remove duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519fb02-79d8-49df-b9fe-8eda979fa25a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: Black; padding: 10px; \">\n",
    "    <h4> Rename column\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2fffb76-f3b1-4ee3-b467-467f0d7ec215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'SepalLengthCm': 'SepalLong'}, inplace=True)  # Rename a column {'OldName': 'NewName'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7b502e3-054c-471f-8904-b98e71faa8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             150 non-null    int64  \n",
      " 1   SepalLong      150 non-null    float64\n",
      " 2   SepalWidthCm   150 non-null    float64\n",
      " 3   PetalLengthCm  150 non-null    float64\n",
      " 4   PetalWidthCm   150 non-null    float64\n",
      " 5   Species        150 non-null    object \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 7.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())  # Check the DataFrame again after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ede1e3-7668-4ca3-bb69-64344a1e070b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Live Exercise</font> </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1cea8-d999-464a-9ed1-d760c38a57fb",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task: Data Cleaning Exercise\n",
    "\n",
    "### Objective\n",
    "In this exercise, you will practice data cleaning techniques using NumPy and Pandas. You will work with a data of choice or oneset provided to you and perform various data cleaning methods\n",
    "\n",
    "## Instructions\n",
    "1. **Import the Necessary Libraries**\n",
    "   - Write the code to import the Pandas and NumPy libraries.\n",
    "\n",
    "2. **Load the Dataset**\n",
    "   - Write the code to load the provided dataset into a Pandas DataFrame.\n",
    "\n",
    "3. **Explore the Data**\n",
    "   - Use the appropriate methods to view the first few rows of the dataset, obtain summary information, and describe the datasetâ€™s statistical properties.\n",
    "\n",
    "4. **Identify Missing Values**\n",
    "   - Write code to check for any missing values in the dataset.\n",
    "\n",
    "5. **Handle Missing Values**\n",
    "   - Decide how to handle missing values (drop, fill with mean/median/mode) and write the corresponding code to apply your chosen method.\n",
    "\n",
    "6. **Detect Outliers**\n",
    "   - Use visualizations (like box plots) or statistical methods to iden t7 implement your chosen method.\n",
    "\n",
    "8. **Remove Duplicates**\n",
    "   - Write the code to check for and remove an8 duplicate rows in the DataFrame.\n",
    "\n",
    "9. **Standardize Data Formats**\n",
    "   - Ensure that categorical variables have consistent formats (e.g., lowercase). Write the code\n",
    "9 standardize the specified columns.\n",
    "\n",
    "10. **Sampling Data**\n",
    "    - Write the code to take a random sample o10the cleaned data for further analysis.\n",
    "\n",
    "11. **Save the Cleaned Data**\n",
    "    - Write the code to save the cleane#d DataFrame to a new CSV file.\n",
    "\n",
    "## Deliverables\n",
    "- Submit the Jupyter notebook containing your code and comments explaining each stepke sure to document your choices and methods clearly.\n",
    "port numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7c19f-cf90-4208-9be4-ee5c07e3a2c0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Advanced Exercise 1 (Optional)</font> </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55f4ec-7d7a-4651-84b2-58dcf060d89c",
   "metadata": {},
   "source": [
    "### Activity Motive: Enhancing the Titanic Dataset for Insightful Analysis\n",
    "\n",
    "In this activity, you will clean and manipulate the Titanic dataset to prepare it for analysis. This exercise is crucial as it helps you practice essential data cleaning skills and gain insights from historical data, which can inform future decisions in maritime safety.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. **Load the Titanic Dataset**: Use Pandas to load the dataset into a DataFrame.\n",
    "2. **Inspect the Data**: Use methods like `.head()`, `.info()`, and `.describe()` to understand the structure and content of the dataset.\n",
    "3. **Handle Missing Values**: Identify columns with missing values and decide on an appropriate strategy (e.g., imputation or removal).\n",
    "4. **Detect and Handle Outliers**: Analyze numerical features for outliers and apply techniques to manage them.\n",
    "5. **Feature Engineering**: Create new features that may enhance analysis, such as family size from 'SibSp' and 'Parch'.\n",
    "6. **Convert Categorical Variables**: Convert categorical variables into numerical formats using one-hot encoding or label encoding.\n",
    "7. **Sample the Data**: Use the `.sample()` method to randomly select a subset of the data for analysis.\n",
    "\n",
    "### Suggested Dataset:\n",
    "- [Titanic Dataset from Kaggle](https://www.kaggle.com/c/titanic/data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85982eed-0732-4e47-9627-caeba4c14400",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Advanced Exercise 2 (Optional)</font> </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3712c-0c7c-4ee9-a0f8-1a5ef051e281",
   "metadata": {},
   "source": [
    "### Next Steps after Data Cleaning and Manipulation:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)**:\n",
    "   - Visualize the cleaned data using libraries like Matplotlib or Seaborn.\n",
    "   - Analyze relationships between features (e.g., survival rates based on gender, class, or age).\n",
    "   - Create visual plots like histograms, box plots, and heatmaps to understand feature distributions and correlations.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Identify the most important features for your analysis or model by evaluating feature importance, correlations, or using statistical methods.\n",
    "   - Consider dimensionality reduction techniques like PCA if dealing with high-dimensional data.\n",
    "\n",
    "3. **Modeling**:\n",
    "   - Use the cleaned and prepared dataset to build predictive models using libraries such as Scikit-learn.\n",
    "   - Try different algorithms like logistic regression, decision trees, or random forests to pred\n",
    "  \n",
    " 4. Evaluation\n",
    "\n",
    "- **Split the dataset into training and testing sets**:  \n",
    "   This helps to assess how well the model generalizes to unseen data. Usually, a common split is 80% of the data for training and 20% for testing.\n",
    "   \n",
    "- **Evaluate model performance**:  \n",
    "   After training, the model should be evaluated on the test data using various metrics such as:\n",
    "   - **Accuracy**: Measures the proportion of correct predictions out of all predictions.\n",
    "   - **Precision**: Focuses on the proportion of true positives out of all positive predictions.\n",
    "   - **Recall**: Measures the proportion of true positives out of all actual p new data.\n",
    "r model interaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c7334-0d9f-403b-863a-58d07ac6ba23",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 10px; \">\n",
    "    <h4>Solution\n",
    "</h1> </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e69350-c215-4816-a196-240babb86113",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Web Scraping <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Using Beautiful Soup and Scrapy</h3>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: grey; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.   Understanding Scrapy<p>\n",
    "3.   Web Scraping activity with Scrapy<p>\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    Which animal do you think is the deadliest?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e02107-502d-4110-9e39-bd0192898182",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Understanding Scrapy\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259c3bd-1c72-4510-b2fe-4122276681b6",
   "metadata": {},
   "source": [
    "\r\n",
    "Scrapy is a powerful and flexibl**e open-source web crawling framewo**rk for Python, specifically designed for large-scale web scraping projects\n",
    "\n",
    ". It enables users to extract data from websites efficiently and store it in various formats like JSON, CSV, and XML\n",
    "\n",
    "Beautiful Soup is a **lightweight Python library** designed for parsing HTML and XML documents, making it ideal for simple scraping tasks where data extraction from single or a few pages is needed. \n",
    "In contrast, Scrapy is a **comprehensive web scraping framework** that automates crawling, manages requests, and supports large-scale scraping projects with features like middleware and asynchronous requests. \n",
    "\n",
    "While Beautiful Soup is **beginner-friendly** and **easy to set up**, Scrapy has a **steeper learning curve** due to its extensive capabilities and project structure. Ultimately, the choice between the two depends on the complexity and scale of the scraping project you are undertaking.ey intend to scrape.\r\n",
    "\r\n",
    "Scrapy is ideal for developers looking to build web scraping tools that require speed and efficiency, making it a popular choice among data scientists and web developers. For more detailed information on how to get started with Scrapy, consider visiting resources such as [Scrapy's official documentation](https://docs.scrapy.org/en/latest/) or articles on its usage in practical applications.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860439e-3245-4185-9cd3-f981581dc7a6",
   "metadata": {},
   "source": [
    "#### Resources to learn: \n",
    "- Free Code Camp Course: https://youtu.be/mBoX_JCKZTE?si=7iCd1OlPvmV7zjR2\n",
    "- Scrapy Tutorial Playlist: https://youtu.be/ve_0h4Y8nuI?si=veWnx47hfiLa508w\n",
    "- GitHub Resource: https://github.com/python-scrapy-playbook\n",
    "- Scrapy PlayBook: https://thepythonscrapyplaybook.com/freecodecamp-beginner-course/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571839b4-b557-4da1-bdc1-9ba347faea53",
   "metadata": {},
   "source": [
    "Below are some key features and aspects of Scrapy:\n",
    "\n",
    "1. **Efficient Crawling**: Scrapy can handle a large number of requests simultaneously, making it ideal for scraping data from multiple pages quickly. Its architecture is built on Twisted, an asynchronous networking library, which enhances its performance.\n",
    "\n",
    "2. **Flexible Data Extraction**: Scrapy allows for customized data extraction using XPath and CSS selectors, enabling users to precisely target the content they want to scrape. It also supports regular expressions for more complex data extraction scenarios.\n",
    "\n",
    "3. **Built-in Mechanisms**: It includes robust features like request scheduling, automatic handling of cookies and sessions, and built-in support for data storage, which streamlines the scraping process.\n",
    "\n",
    "4. **Pipeline and Middleware Support**: Scrapy provides a mechanism for processing scraped data (through item pipelines) and allows for the integration of middlewares to customize request and response handling, which can help in managing errors and retries.\n",
    "\n",
    "5. **Community and Extensibility**: Being open-source, Scrapy has a vibrant community that contributes to its ongoing development, adding plugins and extensions for additional functionality, such as support for different data storage services.\n",
    "\n",
    "6. **Debugging and Logging**: Scrapy comes with built-in logging features that help track the scraping process and debug issues easily.\n",
    "\n",
    "7. **Command-Line Interface (CLI)**: Users can run their scraping spiders via a dedicated CLI tool, making it easy to execute scrapers without needing complex scripts.\n",
    "\n",
    "8. **Spider Structure**: Users can create \"spiders,\" which are classes that define how a specific site (or group of sites) will be scraped. Spiders can be configured to follow links and scrape multiple pages, allowing for comprehensive data collection.\n",
    "\n",
    "9. **Integrated Shell**: Scrapy provides a shell for quick testing and experimenting with selectors and commands, allowing users to interactively explore the web pages they intend to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174ea8d-5d0a-4a5b-8231-fd720b2f42f4",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color: lightblue; color: #000; padding: 10px; text-align: left;\">Using Scrapy</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260fe1c-af99-46fa-b98e-a965ecab1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If scrapy is not already installed, install with this command\n",
    "#!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f32956-fde5-4693-913b-7a866fee5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3a293-29f1-4258-b265-9674c0c7e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any Scrapy projects exist already\n",
    "\n",
    "import os\n",
    "\n",
    "project_dir = 'test_scrapy_project'\n",
    "\n",
    "# Check if the project directory exists\n",
    "if os.path.exists(project_dir):\n",
    "    print(f\"Projects found in directory: {project_dir}\")\n",
    "    \n",
    "    # List all files and directories within the project directory\n",
    "    for root, dirs, files in os.walk(project_dir):\n",
    "        print(\"Directories:\", dirs)\n",
    "        print(\"Files:\", files)\n",
    "else:\n",
    "    print(f\"No Scrapy project found in the directory: {project_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f3bd0-2dbd-41f6-8101-8081db87cedc",
   "metadata": {},
   "source": [
    "#### Create a Scrapy Project\n",
    "\n",
    "On a terminal we use: ```scrapy startproject my_scrapy_project```\r\n",
    "\n",
    "This command creates a directory structure like this:\n",
    "\n",
    "\n",
    "```markdown\n",
    "my_scrapy_project/\n",
    "    scrapy.cfg\n",
    "    my_scrapy_project/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        middlewares.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320100e-d01b-450e-8db6-6f8f16bcb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your project name\n",
    "project_name = \"idrees_scrapy_project\"\n",
    "\n",
    "# Run the scrapy startproject command\n",
    "os.system(f'scrapy startproject {project_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25deda18-f005-4ba8-a413-44c8e0776b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the directory structure of the new project\n",
    "os.listdir(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157611a1-74fa-451c-8145-9defff6277ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the spider code as a string\n",
    "spider_code = \"\"\"\n",
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'my_spider'\n",
    "    start_urls = ['http://quotes.toscrape.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813867f-438e-45c8-a7f6-19c70e629020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the directory structure of the new project\n",
    "project_name = \"idrees_scrapy_project\"  # Adjust if necessary\n",
    "os.listdir(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ffb33-0aef-4a41-90d6-cf6236771052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the spider code to a file\n",
    "project_name = \"idrees_scrapy_project\"  # Make sure to use your project name\n",
    "spider_file_path = os.path.join(project_name,project_name, \"spiders\", \"my_spider.py\")\n",
    "\n",
    "with open(spider_file_path, 'w') as f:\n",
    "    f.write(spider_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553c770-ebed-4d0e-921e-ab63479c8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set the project name and the output file name\n",
    "project_name = \"idrees_scrapy_project\"\n",
    "output_file = os.path.join(project_name, \"quotes.json\")  # Output file for the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e7182-2f2b-466a-843d-49097fb44515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Scrapy spider and output the results to a JSON file\n",
    "subprocess.run([\n",
    "    'scrapy', \n",
    "    'crawl', \n",
    "    'my_spider',  # The name of your spider\n",
    "    '-o', \n",
    "    output_file,  # Specify the output file\n",
    "    '-t', \n",
    "    'json'  # Specify the output format\n",
    "], cwd=project_name)  # Set the working directory to the Scrapy project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e6a0a-0948-45f9-aa06-fceab248df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if the quotes.json file exists\n",
    "json_file_path = os.path.join(\"idrees_scrapy_project\",\"idrees_scrapy_project\", \"quotes.json\")\n",
    "if os.path.exists(json_file_path):\n",
    "    print(\"JSON file created successfully:\", json_file_path)\n",
    "else:\n",
    "    print(\"JSON file not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aa96c2-1904-4e64-80a7-e947821b18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the path to the JSON file\n",
    "#json_file_path = 'idrees_scrapy_project/idrees_scrapy_project/quotes.json'  # Adjust this path as needed\n",
    "\n",
    "json_file_path = os.path.join(project_name, project_name, 'quotes.json')  # Adjust this path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65dbdd-fff0-4a7e-81b4-7f29fd813cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the JSON file with specified encoding\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336adac-fc0e-4976-805d-36bc998aa706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data into a DataFrame\n",
    "df = pd.read_json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831a159-333a-4b15-bddc-3be197289de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1aa4bb-2bc7-450e-a82b-abae43dd0d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ec60f74-0850-4795-9510-bca69c6fec5a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Web crawling activity with Scrapy\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a177bac-b11f-4dc8-8c8f-e0e22cfcbdef",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "To scrape data from a popular website that lists movie information—IMDb. In this example, we'll extract the titles, years, and ratings of the top movies from the IMDb Top 250 list.\n",
    "\n",
    "#### Activity Overview\n",
    "    \n",
    "In this activity, we will:\n",
    "\n",
    "    1. Send an HTTP request to the IMDb Top 250 movies page.\n",
    "    2. Parse the HTML content using Beautiful Soup.\n",
    "    3. Extract movie titles, years, and ratings.\n",
    "    4. Store the data in a Pandas DataFrame.\n",
    "    5. Display the DataFrame.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ac6e8-e4cf-4983-b43e-fa08c529eea0",
   "metadata": {},
   "source": [
    "#### 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54017d69-fe26-4d7c-af86-a76db3fa16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import requests                  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup     # For parsing HTML content\n",
    "import pandas as pd               # For creating, storing and manipulating DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfba0a-960a-4c87-a3fe-26e767b1fc67",
   "metadata": {},
   "source": [
    "#### 2. Send a Request to IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba2e1f-560e-471b-a967-dca1caf67f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the IMDb Top 250 movies page\n",
    "url = 'https://www.imdb.com/chart/top/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd64e8-a325-45b9-98a5-7acbc374d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an HTTP GET request to fetch the page content\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481bd9b-06ca-4706-9def-d9ec904f2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the page!\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908bee2b-eaf6-440d-9c90-e0be4ef59749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes causes errors with some websites, so let's use headers\n",
    "\n",
    "# Set headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2676bc4-012b-4397-aa0c-3d51439e7c9a",
   "metadata": {},
   "source": [
    "The ```User-Agent``` string in the headers simulates a request from a web browser. This often helps bypass simple bot detection mechanisms that websites may have in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2e515-94bf-4e15-81dd-a5cda02d1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an HTTP GET request with the headers\n",
    "response = requests.get(url, headers=headers)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dd874-6743-4128-b38f-96102e5b1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the page!\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04623fc-fb36-43d8-834a-aa32cfe9b388",
   "metadata": {},
   "source": [
    "#### 3. Parse the HTML Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341cbbb-7495-41c3-beb4-1cc5ac13be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content of the page using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b68e25-c73d-4bf8-8619-d4b8abb775c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parsed HTML to verify\n",
    "#print(soup.prettify()[:2000]) # method in Beautiful Soup that formats the parsed HTML content into a more readable structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb440c7d-82ed-4f40-b236-03b8b34bab1a",
   "metadata": {},
   "source": [
    "#### 4. Extract Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d62373-ffd3-41df-ade0-1e8d64199504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store movie titles, years, ratings, and URLs\n",
    "titles = []\n",
    "years = []\n",
    "ratings = []\n",
    "urls = []\n",
    "\n",
    "# Find all the movie items in the HTML\n",
    "movie_items = soup.find_all('item')\n",
    "movie_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7750eb-cd9f-459e-8529-f49903bdef1c",
   "metadata": {},
   "source": [
    "```movie_items``` has come out to be empty\n",
    "- In this case we would observe the soup and try to identify how each element has been stored\n",
    "- Otherwise the flow of program would have been:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33ca6b-4b25-4df1-8f1a-0dec90d4297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the rows to extract movie information\n",
    "for row in movie_rows:\n",
    "    # Extract title\n",
    "    title_column = row.find('td', class_='titleColumn')\n",
    "    if title_column:  # Check if title_column is not None\n",
    "        title = title_column.a.text  # Get the movie title\n",
    "        titles.append(title)  # Append title to the list\n",
    "\n",
    "        # Extract year\n",
    "        year = title_column.span.text.strip('()')  # Remove parentheses\n",
    "        years.append(year)  # Append year to the list\n",
    "\n",
    "        # Extract rating\n",
    "        rating_column = row.find('td', class_='ratingColumn imdbRating')\n",
    "        if rating_column:\n",
    "            rating = rating_column.strong.text  # Get the rating\n",
    "            ratings.append(rating)  # Append rating to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a7032-e539-4588-b039-dcb68a1ab909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few movie titles, years, and ratings\n",
    "print(\"Titles:\", titles[:5])\n",
    "print(\"Years:\", years[:5])\n",
    "print(\"Ratings:\", ratings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f75722-cfcb-4453-bf6f-4bbbc7cd7456",
   "metadata": {},
   "source": [
    "##### Observe the empty outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2064f68-6bf0-4fd2-ad41-6956f2fc6853",
   "metadata": {},
   "source": [
    "#### Debugging\n",
    "\n",
    "Let's observe what's wrong. \n",
    "```python\n",
    "# Extract title\n",
    "    title_column = row.find('td', class_='titleColumn')\n",
    "```\n",
    "\n",
    "was not working because ```td``` is not used to encode data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e82fc7-e0b0-43fe-8bcd-40511d609337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check what soup is looking like\n",
    "#print(soup.prettify()[:2000]) # Print the first 500 characters of the prettified  HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5641b-95a1-46b9-9b3f-9bfa2ae787f7",
   "metadata": {},
   "source": [
    "<h4 style=\"background-color: yellow; color: #000; padding: 10px; text-align: left;\">Observing the meta data here</h4>\n",
    "\n",
    "- Data looks like enclosed in a ```<script type=\"application/ld+json\">```\n",
    "\n",
    "- So it is stored in JSON. Let's use the steps to parse data from JSON within an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2883ce-03fd-4d2f-81ee-9f6632f44da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the JSON data in the script tag\n",
    "json_data = soup.find('script', type='application/ld+json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a0807-4f49-4e84-abbb-6f12ba2c636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if json_data:\n",
    "    # Parse the JSON data\n",
    "    data = json.loads(json_data.string)\n",
    "# data # observe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75c348-433a-42c3-9da7-a75f159fafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract titles and ratings\n",
    "titles = []\n",
    "urls = []\n",
    "descriptions = []\n",
    "best_ratings = []\n",
    "worst_ratings = []\n",
    "ratings = []\n",
    "genres = []\n",
    "durations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcff85-a5e4-4277-8540-df0c427570c0",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a848e0e-41f7-4907-8085-a9ee22d2fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data contains the expected structure\n",
    "if 'itemListElement' in data:\n",
    "    for item in data['itemListElement']:\n",
    "        movie = item['item']\n",
    "        \n",
    "        # Extract movie details\n",
    "        titles.append(movie['name'])  # Movie name\n",
    "        urls.append(movie['url'])  # Movie URL\n",
    "        descriptions.append(movie['description'])  # Movie description\n",
    "        \n",
    "        # Extract ratings (best, worst, actual rating)\n",
    "        best_ratings.append(movie['aggregateRating']['bestRating'])  # Best rating\n",
    "        worst_ratings.append(movie['aggregateRating']['worstRating'])  # Worst rating\n",
    "        ratings.append(float(movie['aggregateRating']['ratingValue']))  # Rating value\n",
    "        \n",
    "        genres.append(movie['genre'])  # Movie genre\n",
    "        durations.append(movie['duration'])  # Movie duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9e65d-c1da-46b2-9d50-43b8817530a3",
   "metadata": {},
   "source": [
    "#### 5. Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee65bd-6af6-4a2f-80a8-8c79945a9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'URL': urls,\n",
    "    'Description': descriptions,\n",
    "    'Best Rating': best_ratings,\n",
    "    'Worst Rating': worst_ratings,\n",
    "    'Rating': ratings,\n",
    "    'Genre': genres,\n",
    "    'Duration': durations\n",
    "})\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3447e9d-1e34-4103-af7a-24109fa6166b",
   "metadata": {},
   "source": [
    "#### 6. Save Data to a CSV File (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd06cfa-ef1c-4014-8e5c-c144c62edfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('topIMBDmovies.csv', index=False) # Setting index=False excludes the row index from being written to the file.\n",
    "print(\"Data saved in CSV format'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2185430-3791-45df-824f-bdec6d7145e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Live Exercise</font> </b>\n",
    "</div>\n",
    "\n",
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d61af-f3e8-4afc-a66b-3814e160aaf3",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task 1: IMDb Most Popular Movies Web Scraping Activity\n",
    "\n",
    "#### Objective:\n",
    "In this exercise, you will scrape data from IMDb's \"Most Popular Movies\" chart using Python and Beautiful Soup. By the end of the activity, you will:\n",
    "- Successfully extract movie titles, URLs, descriptions, ratings, genres, and durations from the webpage.\n",
    "- Organize the scraped data into a pandas DataFrame.\n",
    "- Save the extracted data to a CSV file for further use.\n",
    "\n",
    "#### Instructions:\n",
    "- Step 1: Send a GET request to the IMDb \"Most Popular Movies\" page\n",
    "- Step 2: Parse the HTML using BeautifulSoup\n",
    "- Step 3: Extract the required data:\n",
    "    - You need to scrape the following data for each movie in the \"Most Popular Movies\" chart:\n",
    "\n",
    "    1. Title: The name of the movie.\n",
    "    2. URL: The URL to the movie's IMDb page.\n",
    "    3. Description: A short description of the movie.\n",
    "    4. Best Rating: The highest rating that a movie can achieve (usually 10).\n",
    "    5. Worst Rating: The lowest rating that a movie can receive (usually 1).\n",
    "    6. Rating Value: The actual rating value given by users (e.g., 8.5).\n",
    "    7. Genre: The genre of the movie (e.g., Drama, Comedy).\n",
    "    8. Duration: The duration of the movie (e.g., \"PT2H30M\" which means 2 hours and 30 minutes).\n",
    "- Step 4: Organize the data into a pandas DataFrame\n",
    "- Step 5: Save the data to a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98676925-ac0d-4a6e-8586-3a8d3b40edae",
   "metadata": {},
   "source": [
    "### Task 1: IMDb Most Popular Movies Web Scraping Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9ddd9-5558-4b1d-a3e7-04c3dca33b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests                  # Used to send HTTP requests to the website\n",
    "from bs4 import BeautifulSoup     # Used to parse and navigate through HTML content\n",
    "import pandas as pd               # Used to create a DataFrame and handle data manipulation\n",
    "import json                       # Used to work with JSON data\n",
    "\n",
    "# Step 1: Send a GET request to the IMDb \"Most Popular Movies\" page\n",
    "# Define the URL of the IMDb chart for most popular movies\n",
    "url = \"https://www.imdb.com/chart/moviemeter\"\n",
    "\n",
    "# Define headers to mimic a browser visit and avoid being blocked by the website\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Send a GET request to fetch the content of the webpage\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Step 2: Parse the HTML content using BeautifulSoup\n",
    "# Create a BeautifulSoup object and specify the parser (html.parser in this case)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Extract the JSON-LD structured data containing movie details\n",
    "# IMDb stores movie data in a JSON format inside a <script> tag, which we can extract\n",
    "script_tag = soup.find('script', type='application/ld+json')\n",
    "\n",
    "# Check if the <script> tag with JSON data was found\n",
    "if script_tag:\n",
    "    # Convert the JSON data (string) into a Python dictionary\n",
    "    data = json.loads(script_tag.string)\n",
    "\n",
    "# Step 4: Initialize lists to hold the extracted movie details\n",
    "# These lists will hold the movie titles, URLs, descriptions, ratings, etc.\n",
    "titles = []\n",
    "urls = []\n",
    "descriptions = []\n",
    "best_ratings = []\n",
    "worst_ratings = []\n",
    "rating_values = []\n",
    "genres = []\n",
    "durations = []\n",
    "\n",
    "# Step 5: Extract movie details from the JSON data\n",
    "# Check if the 'itemListElement' key exists in the JSON data (this contains the list of movies)\n",
    "if 'itemListElement' in data:\n",
    "    # Loop through each movie in the 'itemListElement'\n",
    "    for item in data['itemListElement']:\n",
    "        # Access the individual movie data inside 'item'\n",
    "        movie = item['item']\n",
    "        \n",
    "        # Extract movie details and append them to the corresponding lists\n",
    "        # If a specific field is missing, we use 'N/A' as a fallback\n",
    "        titles.append(movie.get('name', 'N/A'))  # Movie title\n",
    "        urls.append(movie.get('url', 'N/A'))     # IMDb page URL\n",
    "        descriptions.append(movie.get('description', 'N/A'))  # Movie description\n",
    "        best_ratings.append(movie.get('aggregateRating', {}).get('bestRating', 'N/A'))  # Best possible rating\n",
    "        worst_ratings.append(movie.get('aggregateRating', {}).get('worstRating', 'N/A'))  # Worst possible rating\n",
    "        rating_values.append(movie.get('aggregateRating', {}).get('ratingValue', 'N/A'))  # Current movie rating\n",
    "        genres.append(movie.get('genre', 'N/A'))  # Movie genre(s)\n",
    "        durations.append(movie.get('duration', 'N/A'))  # Movie duration\n",
    "\n",
    "# Step 6: Create a pandas DataFrame to store the extracted data\n",
    "# A DataFrame is a table-like structure that allows for easy manipulation and analysis of data\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,              # Column for movie titles\n",
    "    'URL': urls,                  # Column for IMDb URLs\n",
    "    'Description': descriptions,  # Column for movie descriptions\n",
    "    'Best Rating': best_ratings,  # Column for the best possible rating (usually 10)\n",
    "    'Worst Rating': worst_ratings, # Column for the worst possible rating (usually 1)\n",
    "    'Rating Value': rating_values, # Column for the actual user rating\n",
    "    'Genre': genres,              # Column for the movie's genre (e.g., Drama, Comedy)\n",
    "    'Duration': durations         # Column for the movie's duration (in ISO format, e.g., PT2H30M)\n",
    "})\n",
    "\n",
    "# Step 7: Save the DataFrame to a CSV file\n",
    "# This saves the DataFrame as a CSV file called 'most_popular_movies.csv'\n",
    "df.to_csv('most_popular_movies.csv', index=False)  # index=False means we do not save the index column\n",
    "print(\"Data saved to 'most_popular_movies.csv'\")   # Notify the user that the data has been saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b33a6-c049-45a9-aa12-c331aaa2876d",
   "metadata": {},
   "source": [
    "#### Alternate syntax has been used for extraction here;\n",
    "\n",
    "```python\n",
    "# Check if the data contains the expected structure\n",
    "if 'itemListElement' in data:\n",
    "    for item in data['itemListElement']:\n",
    "        movie = item['item']\n",
    "        \n",
    "        # Extract movie details\n",
    "        titles.append(movie.get('name', 'N/A'))\n",
    "        urls.append(movie.get('url', 'N/A'))\n",
    "        descriptions.append(movie.get('description', 'N/A'))\n",
    "        best_ratings.append(movie.get('aggregateRating', {}).get('bestRating', 'N/A'))\n",
    "        worst_ratings.append(movie.get('aggregateRating', {}).get('worstRating', 'N/A'))\n",
    "        rating_values.append(movie.get('aggregateRating', {}).get('ratingValue', 'N/A'))\n",
    "        genres.append(movie.get('genre', 'N/A'))\n",
    "        durations.append(movie.get('duration', 'N/A'))```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "\r\n",
    "1. Topic: BeautifulSoup vs. Selenium\r\n",
    "   Question: What are the key differences between using `BeautifulSoup` and `Selenium` for web scraping? When would you use each?\r\n",
    "\r\n",
    "2. Topic: Dynamic Content\r\n",
    "   Question: How do you handle web pages with dynamic content (e.g., loaded via JavaScript) while scraping using Pyh3 task?\r\n",
    "\r\n",
    "4. Topic: Navigating HTML\r\n",
    "   Question: What methods are available in `BeautifulSoup` to navigate through an HTML document? Can you explain the difference between `find`, `find_all`, and CSS 4electors?\r\n",
    "\r\n",
    "5. Topic: Efficiency\r\n",
    "   Question: When scraping data with `BeautifulSoup`, how would you handle large HTML documents efficiently to avoid m5mory issues?\r\n",
    "\r\n",
    "6. Topic: Pandas DataFrame\r\n",
    "   Question: How can you convert a list of extracted data into a Pandas DataFrame, and what are the common issues you might face 6hen doing this?\r\n",
    "\r\n",
    "7. Topic: Scraping Authentication\r\n",
    "   Question: What are some common challenges you face when scraping websites that require login? How would you ha7dle such websites?\r\n",
    "\r\n",
    "8. Topic: Nested HTML Tags\r\n",
    "   Question: How would you extract specific data, such as text from nested HTML tags, using `BeautifulSoup`? Provide an example of handling d8eply nested elements.\r\n",
    "\r\n",
    "9. Topic: Legal and Ethical Considerations\r\n",
    "   Question: What are the legal and ethical considerations to keep in mind whing missing or malformed data?\r\n",
    "r malformed data?**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf36ec-351e-4955-8577-8f0276d44099",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "\n",
    "1. **Topic: BeautifulSoup vs. Selenium**  \n",
    "   **Solution:** `BeautifulSoup` is a library used for parsing HTML and XML documents, making it easier to extract data. It is most effective for static pages. On the other hand, `Selenium` is used for automating web browsers and can handle dynamic content loaded via JavaScript. Use `BeautifulSoup` for simpler tasks and `Selenium` for more complex interactions with web pages.\n",
    "\n",
    "2. **Topic: Dynamic Content**  \n",
    "   **Solution:** To handle dynamic content, you can use `Selenium` to render the page and interact with elements. Alternatively, you can analyze network requests in the browser’s developer tools to directly access the APIs that load the data in the background.\n",
    "\n",
    "3. **Topic: Navigating HTML**  \n",
    "   **Solution:** In `BeautifulSoup`, `find()` retrieves the first matching tag, while `find_all()` returns a list of all matches. CSS selectors can also be used with `select()` to target elements more flexibly, which is useful for more complex HTML structures.\n",
    "\n",
    "4. **Topic: Efficiency**  \n",
    "   **Solution:** To handle large HTML documents efficiently, use `lxml` parser with `BeautifulSoup`, as it is faster and consumes less memory. You can also limit the scope of your search using specific tags or attributes to reduce the amount of data processed at once.\n",
    "\n",
    "5. **Topic: Pandas DataFrame**  \n",
    "   **Solution:** You can convert a list of data into a Pandas DataFrame by passing the list to `pd.DataFrame()`. Common issues may include mismatched lengths of lists or missing values, which can be handled by specifying the `columns` parameter and using DataFrame methods like `fillna()` for cleaning.\n",
    "\n",
    "6. **Topic: Scraping Authentication**  \n",
    "   **Solution:** When scraping websites requiring login, you can use `requests` to manage sessions. Start a session using `requests.Session()`, then log in with a POST request that submits your credentials. After authentication, you can scrape protected pages using the same session.\n",
    "\n",
    "7. **Topic: Nested HTML Tags**  \n",
    "   **Solution:** To extract data from nested HTML tags, you can use `find()` or `find_all()` multiple times. For example, if you want to extract text from `<div><span>text</span></div>`, you would first find the `<div>` and then find the `<span>` within that.\n",
    "\n",
    "8. **Topic: Legal and Ethical Considerations**  \n",
    "   **Solution:** Key considerations include checking the website's `robots.txt` file to see what is allowed to be scraped, respecting the website's terms of service, and not overwhelming the server with requests. Always ensure your scraping activities are compliant with local laws."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

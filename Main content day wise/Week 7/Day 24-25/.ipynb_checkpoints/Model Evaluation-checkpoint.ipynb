{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Model Evaluation <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Evaluation metrics, cross-validation methods.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.  Metrics for Evaluation <p><p> \n",
    "2.  Cross-Validation Techniques<p>\n",
    "3.  Hands-On Activity: Model Evaluation and Cross Validation  <p>  \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    What are examples of good vs. bad predictions you’ve seen in real life?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797c0ea",
   "metadata": {},
   "source": [
    "The purpose of machine learning models is to **analyze data**, **identify patterns**, and **make predictions or decisions** based on that data without being explicitly programmed for specific tasks.\n",
    "\n",
    "These models \"learn\" from historical data to solve various problems, such as:\n",
    "\n",
    "- **Classifying** objects or categories (e.g., spam vs. not spam emails)\n",
    "- **Predicting** outcomes (e.g., stock prices, customer behavior)\n",
    "- **Detecting patterns** or anomalies (e.g., fraud detection)\n",
    "\n",
    "The overall goal is to **create models** that can **generalize well** and make accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c0409",
   "metadata": {},
   "source": [
    "---\n",
    "**Why Evaluate Models?**\n",
    "\n",
    "Model evaluation is **critical** before deployment because it ensures that the model can perform its task accurately and reliably in real-world settings. \n",
    "\n",
    "Without proper evaluation, we risk deploying a model that could cause unintended consequences, especially in sensitive or high-stakes areas. \n",
    "\n",
    "Let’s look at a couple of relatable examples to see why this is important:\n",
    "\n",
    "1. **Spam Detection:**\n",
    "\n",
    "    If a spam detection model is inaccurate, it might incorrectly *label important emails as spam* (false positives) or miss actual spam emails (false negatives).\n",
    "    \n",
    "    **Consequence:** While inconvenient, a few missed emails won’t cause major harm. Here, **accuracy** is important, but *we may tolerate* a small margin of error.\n",
    "\n",
    "2. **Medical Diagnoses:**\n",
    "\n",
    "    Imagine a model predicting the **likelihood of disease** based on patient data. If this model is not evaluated thoroughly, it might *misdiagnose patients*, leading to missed treatments (false negatives) or unnecessary stress and procedures (false positives).\n",
    "    \n",
    "    **Consequence:** In medical contexts, errors can have *serious implications* on health and lives, so even a small mistake rate is critical to address.\n",
    "\n",
    "Evaluation allows us to identify any weaknesses, fine-tune its accuracy, and decide if it's ready for real-world use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Metrics for Evaluation\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aef031-1416-4707-b2a0-fc60ca4ee7ca",
   "metadata": {},
   "source": [
    "Evaluation metrics are tools that allow us to measure how well a machine learning model performs its tasks. \n",
    "\n",
    "They provide insight into a model's **accuracy**, **reliability**, and **ability to generalize to new data**, helping us identify where improvements are needed before deployment.\n",
    "\n",
    "The choice of evaluation metrics depends on the type of task the model is performing:\n",
    "\n",
    "- **Classification** tasks (like spam detection) use metrics that assess how well the model assigns data to categories, focusing on how often it’s correct versus how often it makes errors.\n",
    "- **Regression** tasks (like predicting house prices) rely on metrics that quantify the difference between predicted and actual values, giving us a sense of the model’s accuracy in numerical predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440bc992",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Classification Metrics\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd9a29",
   "metadata": {},
   "source": [
    "It's important to first understand the Confusion Matrix, as it forms the foundation for these metrics.\n",
    "\n",
    "**Confusion Matrix**\n",
    "\n",
    "The Confusion Matrix is a table that helps us visualize the performance of a classification algorithm. It shows the number of correct and incorrect predictions broken down by class.\n",
    "\n",
    "Components of a Confusion Matrix:\n",
    "- **True Positives (TP)**: Correctly predicted positive instances.\n",
    "- **True Negatives (TN)**: Correctly predicted negative instances.\n",
    "- **False Positives (FP)**: Incorrectly predicted as positive (Type I error).\n",
    "- **False Negatives (FN)**: Incorrectly predicted as negative (Type II error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d936a",
   "metadata": {},
   "source": [
    "![](https://uploads-ssl.webflow.com/6266b596eef18c1931f938f9/644aea65cefe35380f198a5a_class_guide_cm08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec2e5b",
   "metadata": {},
   "source": [
    "![](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/636b91ad565f3b141540cd86_32323.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef3618",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "![](https://th.bing.com/th/id/R.9e4564758d69b7b70bc51e8e50032bdd?rik=BEVPo5DT9c3VjA&riu=http%3a%2f%2fstatic1.squarespace.com%2fstatic%2f5fbb9dabe7157107b90d84b0%2ft%2f607127f7e7d4c31b0d31eee7%2f1618028535960%2fConfusion%2bMatrix%2bfigure%2b1.png%3fformat%3d1500w&ehk=eMd2GgYCttpF95cfcXfotYz2u3JpOjyiLN5deFO2wgI%3d&risl=&pid=ImgRaw&r=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900ad22",
   "metadata": {},
   "source": [
    "![](https://www.fticonsulting.com/emea/insights/articles/-/media/ec68c768d8314ee9bd1d00109c2b603c.ashx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a7b166",
   "metadata": {},
   "source": [
    "---\n",
    "- https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall\n",
    "\n",
    "\n",
    "### 1. Accuracy\n",
    "- Accuracy is the ratio of correct predictions (both true positives and true negatives) to the total number of predictions.\n",
    "- **Analogy**: Imagine a medical test designed to detect a disease. If out of 100 patients, 90 are correctly classified (either as having or not having the disease), the accuracy is 90%.\n",
    "- **Key Point**: Accuracy is useful for an overall sense of correctness, but it can be misleading if there is an imbalance in the data (e.g., very few people actually have the disease).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce30b0",
   "metadata": {},
   "source": [
    "![](https://uploads-ssl.webflow.com/6266b596eef18c1931f938f9/644af6a24701d43aaecd8771_classification_guide_apc09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c46913",
   "metadata": {},
   "source": [
    "**Let's use a running example:** (Spam classification problem gives this result)\n",
    "\n",
    "|                     | Predicted Positive (Spam) | Predicted Negative (Not Spam) |\n",
    "|---------------------|---------------------------|-------------------------------|\n",
    "| **Actual Positive (Spam)**  | True Positive (TP) = 50       | False Negative (FN) = 10        |\n",
    "| **Actual Negative (Not Spam)** | False Positive (FP) = 5      | True Negative (TN) = 100        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59d471",
   "metadata": {},
   "source": [
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "measures the overall percentage of correct predictions \n",
    "\n",
    "= (50 + 100)/(50+100+5+10)\n",
    "\n",
    "= 150/165\n",
    "\n",
    "= 0.909\n",
    "\n",
    "or Accuracy = 90.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058f42e",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- Accuracy (90.9%) is good, but we must be cautious if the dataset is imbalanced (e.g., if there are more non-spam emails than spam emails, accuracy alone may not reflect performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41a0d9",
   "metadata": {},
   "source": [
    "### 2. Precision\n",
    "- Precision measures how many of the positive predictions made by the model are actually correct.\n",
    "- **Analogy**: Think of precision like the reliability of a positive test result. If a medical test has high precision, when it says a person has a disease, it’s likely correct. High precision is important in situations where false positives could lead to unnecessary stress or treatment.\n",
    "- **Example**: If out of 10 people flagged by the test as “positive,” only 7 actually have the disease, then precision is 70%.\n",
    "- **Key Point**: \n",
    "    - Precision focuses on minimizing **false positives** and is especially useful when false positives are costly or harmful.\n",
    "\n",
    "    - Precision focuses on **how accurate the positive predictions** are\n",
    "\n",
    "    - Precision: How many of the patients diagnosed with the disease actually have it (correct diagnoses)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a9866",
   "metadata": {},
   "source": [
    "![](https://uploads-ssl.webflow.com/6266b596eef18c1931f938f9/644af6c4d573412369556f6e_classification_guide_apc10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146743a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|                     | Predicted Positive (Spam) | Predicted Negative (Not Spam) |\n",
    "|---------------------|---------------------------|-------------------------------|\n",
    "| **Actual Positive (Spam)**  | True Positive (TP) = 50       | False Negative (FN) = 10        |\n",
    "| **Actual Negative (Not Spam)** | False Positive (FP) = 5      | True Negative (TN) = 100        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f21f01",
   "metadata": {},
   "source": [
    "Precision = TP/(TP + FP)\n",
    "\n",
    "measures how accurate the positive (spam) predictions are. It’s important when the cost of false positives (misclassifying non-spam as spam) is high\n",
    "\n",
    "= 50/(50+5)\n",
    "= 0.909\n",
    "\n",
    "Precision = 90.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57866196",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- Precision (90.9%) tells us that when the model predicts an email as spam, it’s correct 90.9% of the time, which is great for preventing false alarms (non-spam emails being flagged as spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229d3d0",
   "metadata": {},
   "source": [
    "### 3. Recall (Sensitivity)\n",
    "- Recall is the ratio of correctly identified positives to the total actual positives in the dataset.\n",
    "\n",
    "- **Analogy**: \n",
    "Recall is like the test’s ability to catch everyone with the disease. \n",
    "A high-recall test is thorough in finding all cases, so it minimizes **false negatives** (cases where people have the disease but the test misses it).\n",
    "\n",
    "- **Example**: If there are 20 patients with the disease, but the test correctly identifies only 15 of them, recall is 75%.\n",
    "\n",
    "- **Key Point**: \n",
    "    - Recall is crucial in scenarios where **missing a positive case** (false negative) is highly risky, such as in disease screening.\n",
    "\n",
    "    - Recall focuses on **how well the model identifies** all the **positive instances**\n",
    "\n",
    "    - Recall: How many of the patients who actually have the disease were correctly identified by the test (true positives)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122813c",
   "metadata": {},
   "source": [
    "![](https://cdn.prod.website-files.com/660ef16a9e0687d9cc27474a/662c426738658d748af1b1ef_644af7a5c21ca563bd25204a_classification_guide_apc15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697ef66",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|                     | Predicted Positive (Spam) | Predicted Negative (Not Spam) |\n",
    "|---------------------|---------------------------|-------------------------------|\n",
    "| **Actual Positive (Spam)**  | True Positive (TP) = 50       | False Negative (FN) = 10        |\n",
    "| **Actual Negative (Not Spam)** | False Positive (FP) = 5      | True Negative (TN) = 100        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41f176",
   "metadata": {},
   "source": [
    "Recall = TP/(TP+FN)\n",
    "\n",
    "measures the ability of the model to correctly identify all positive (spam) cases. It’s critical when the cost of false negatives (misclassifying spam as not spam) is high.\n",
    "\n",
    "Recall = 50/(50+10)\n",
    "= 50/60\n",
    "=0.833\n",
    "\n",
    "Recall = 83.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf93d4",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- Recall (83.3%) shows that the model correctly identifies 83.3% of all the spam emails, but it misses 16.7% of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072cc3f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "However, these two metrics may not always give a full picture. \n",
    "\n",
    "For example:\n",
    "\n",
    "- **Precision** could be **high** if the model **rarely predicts positives**, but it may miss many actual positives (**low Recall**).\n",
    "\n",
    "- **Recall** could be **high** if the model **predicts most positives**, but it could also include a lot of false positives (**low Precision)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab8c1b",
   "metadata": {},
   "source": [
    " trade-off between precision and recall is why the F1-Score is useful — it combines the two and provides a more balanced measure of performance.\n",
    "\n",
    " ---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f8fa0",
   "metadata": {},
   "source": [
    "### 4. F1-Score\n",
    "- The F1-Score is the harmonic mean of Precision and Recall. It  takes both into account in a way that balances both metrics into a single score, especially useful when there’s an uneven class distribution.\n",
    "\n",
    "- **Analogy**: \n",
    "\n",
    "Imagine you’re designing a test for a rare disease. You need a balance: \n",
    "\n",
    "the test should not miss cases (high recall), but you also don’t want it flagging too many healthy people as sick (high precision). \n",
    "\n",
    "The F1-Score helps balance these competing needs to give an overall performance measure. \n",
    "\n",
    "- **Example**: If a model has high recall but low precision, or vice versa, the F1-score provides a middle-ground metric that considers both.\n",
    "\n",
    "- **Key Point**: \n",
    "    - The F1-score is useful when you need a model that maintains both high precision and recall\n",
    "    - When you want a single measure that reflects a balance between Precision (minimizing false positives) and Recall (minimizing false negatives)\n",
    "    -  In imbalanced datasets, where one class is much more common than the other (e.g., fraud detection), accuracy can be misleading. \n",
    "    \n",
    "        A model that always predicts the majority class can have high accuracy but poor performance on the minority class. \n",
    "    \n",
    "        The F1-Score is a better reflection of how the model handles both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98482f0",
   "metadata": {},
   "source": [
    "![](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/639c3d2a22f93657640ef19f_f1-score-eqn.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a3b4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "331987fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|                     | Predicted Positive (Spam) | Predicted Negative (Not Spam) |\n",
    "|---------------------|---------------------------|-------------------------------|\n",
    "| **Actual Positive (Spam)**  | True Positive (TP) = 50       | False Negative (FN) = 10        |\n",
    "| **Actual Negative (Not Spam)** | False Positive (FP) = 5      | True Negative (TN) = 100        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48755296",
   "metadata": {},
   "source": [
    "F1 Score = 2* (Precision * Recall)/(Precision + Recall)\n",
    "\n",
    "F1-Score is the harmonic mean of Precision and Recall, providing a balanced measure of the two. It’s useful when you need to balance both false positives and false negatives.\n",
    "\n",
    "F1 = 2 * (0.909*0.833)/(0.909+0.833)\n",
    "= 2* 0.758/1.742\n",
    "= 0.869\n",
    "\n",
    "F1 = 86.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a4519",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- F1-Score (86.9%) balances both precision and recall, which is important when we care about both types of errors (false positives and false negatives) equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eeb207",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e3e95b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Regression Metrics\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0984285",
   "metadata": {},
   "source": [
    "### 1. Mean Absolute Error (MAE)\n",
    "\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html\n",
    "\n",
    "\n",
    "\n",
    "- MAE is the average of the absolute differences between predicted and actual values. It gives us an idea of how far off, on average, the predictions are from the actual values.\n",
    "- **Formula**:\n",
    "\n",
    "  ![formulaMAE](https://wikimedia.org/api/rest_v1/media/math/render/svg/3ef87b78a9af65e308cf4aa9acf6f203efbdeded)\n",
    "\n",
    "- **Analogy**: Think of predicting the daily temperature for a week. If you predict that tomorrow’s temperature will be 30°C, but the actual temperature is 28°C, the error is 2°C. MAE adds up all such errors over the week and gives the average of those errors.\n",
    "- **Example**: If the predicted temperatures for a week (in °C) are [30, 32, 28, 35, 33], and the actual temperatures are [31, 30, 29, 34, 32], the MAE is the average of the absolute differences:  \n",
    "  \n",
    "  MAE = (|31-30| + |30-32| + |29-28| + |34-35| + |32-33|)/5 \n",
    "  \n",
    "  = (1 + 2 + 1 + 1 + 1)/5 = 1.2 °C\n",
    "  \n",
    "- **Key Point**: MAE is simple to understand and interprets the average error in the same units as the original data (e.g., °C for temperatures). It does not penalize larger errors more than smaller ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741d46d",
   "metadata": {},
   "source": [
    "### 2. Mean Squared Error (MSE)\n",
    "- **Definition**: MSE is the average of the squared differences between predicted and actual values. It penalizes larger errors more heavily than smaller ones because of the squaring operation.\n",
    "- **Formula**:\n",
    "\n",
    "  ![formulaMSE](https://wikimedia.org/api/rest_v1/media/math/render/svg/92ea807c3147d94e8762772be5d12511f1d55938)\n",
    "\n",
    "- **Analogy**: Imagine you're predicting the temperatures for each day in a week. If one prediction is off by 10°C, MSE will give a much higher penalty than if it's off by just 1°C, because squaring the difference makes larger errors much more costly.\n",
    "\n",
    "- **Example**: Using the same temperature predictions:\n",
    "  \n",
    "  MSE = {(31-30)^2 + (30-32)^2 + (29-28)^2 + (34-35)^2 + (32-33)^2}/5 \n",
    "  \n",
    "  = {1 + 4 + 1 + 1 + 1}/5 \n",
    "  = 1.6 °C^2\n",
    "- **Key Point**: MSE is more sensitive to large errors (outliers), as it squares the differences. It’s useful when large errors are particularly undesirable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a2e25",
   "metadata": {},
   "source": [
    "### 3. R-Squared (R²)\n",
    "- R-Squared is a measure of how well the predicted values match the actual values, expressed as a percentage. It tells us the proportion of the variance in the dependent variable that is predictable from the independent variables. R² ranges from 0 to 1, where 1 indicates perfect prediction and 0 indicates no predictive power.\n",
    "- **Formula**:\n",
    "\n",
    "\n",
    "- **Analogy**: Imagine predicting temperatures over a week. If the predicted temperatures are almost the same as the actual ones, R² will be close to 1 (a perfect fit). If the predictions are way off, R² will be close to 0, indicating the model has little predictive power.\n",
    "\n",
    "- **Example**: If the model predicts temperature perfectly every day, the error between predicted and actual values is 0, and \\( R^2 = 1 \\). If the predictions are worse than just using the mean temperature for the week, \\( R^2 \\) might be close to 0.\n",
    "  \n",
    "- **Key Point**: R² is useful for assessing the overall goodness of fit of a model. It tells you how well your model explains the variance in the data, but doesn’t indicate how much error there is in the predictions themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c76343e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- **MAE**: Simple and interprets the error in the same units as the original data (e.g., °C for temperature). It’s less sensitive to outliers.\n",
    "- **MSE**: Gives more weight to larger errors (outliers), making it useful when you want to penalize bigger mistakes more heavily.\n",
    "- **R²**: Tells you how well the model explains the variance in the data. A higher R² means better explanatory power.\n",
    "\n",
    "Each metric has its use case, and selecting the right one depends on the nature of your problem and what aspect of performance you care most about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25a1bc",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Hands-On: Manual Metric Calculation \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916474f0",
   "metadata": {},
   "source": [
    "### Confusion Matrix Example: Medical Diagnosis for COVID-19 Disease Detection\n",
    "\n",
    "A model is used to predict whether individuals are COVID-19 positive based on test results. The confusion matrix is as follows:\n",
    "\n",
    "\n",
    "|                     | Predicted Positive | Predicted Negative |\n",
    "|---------------------|--------------------|---------------------|\n",
    "| **Actual Positive** | TP = 1200           | FN = 300            |\n",
    "| **Actual Negative** | FP = 250            | TN = 6250           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f94d2",
   "metadata": {},
   "source": [
    "### Answers (for instructor reference): Reveal the cell!!!\n",
    "<!---\n",
    "\n",
    "1. **Accuracy**: 0.88 or 88%\n",
    "2. **Precision**: 0.83 or 83%\n",
    "3. **Recall**: 0.80 or 80%\n",
    "4. **F1-Score**: 0.81 or 81%\n",
    "---!>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6698d",
   "metadata": {},
   "source": [
    "**Which metric do you choose now, to evaluate this model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69faaee7",
   "metadata": {},
   "source": [
    "- accuracy alone might not provide a full picture in a COVID-19 testing scenario.\n",
    "\n",
    "- Precision is important in situations where false positives carry a high cost. For instance, if a false positive leads to unnecessary quarantine, this might be acceptable in comparison to missing actual cases.\n",
    "- But in a public health crisis, the focus on precision alone may overlook positive cases, especially when the disease is highly transmissible.\n",
    "\n",
    "- In COVID-19 testing, recall is often prioritized because missing a positive case (false negative) can lead to further spread of the virus.\n",
    "- Since recall measures how well the model catches actual positive cases, a higher recall reduces the likelihood of missing infected individuals.\n",
    "\n",
    "\n",
    "- F1-score helps maintain a balance, ensuring that the model is reasonably accurate in predicting positives without too many false positives.\n",
    "\n",
    "Thus, for this situation:\n",
    "- Recall should be prioritized to minimize false negatives (missing actual COVID-19 cases).  Higher recall means more infected individuals are identified, which is crucial for containment. \n",
    "Recall is critical to reduce undetected cases and prevent virus spread.\n",
    " \n",
    "- F1-Score is a good secondary metric to provide a balance, ensuring the model isn’t overly focused on recall alone but maintains reasonable precision. To maintain some level of precision to avoid overburdening healthcare resources with false positives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b10a45",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Cross-Validation Techniques\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757efda",
   "metadata": {},
   "source": [
    "In machine learning, validating models is essential to assess their performance before deployment. \n",
    "\n",
    "This helps ensure that the model generalizes well to new, unseen data rather than just performing well on the data it was trained on. \n",
    "\n",
    "Without validation, we risk deploying models that might perform poorly in real-world applications, leading to unreliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5865a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Introduction to Cross-Validation\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d27b014",
   "metadata": {},
   "source": [
    "Cross-validation divides the data into multiple subsets, training and testing the model on different combinations of these subsets. \n",
    "\n",
    "This provides a better estimate of model performance across the dataset, helping identify if the model is overfitting or if there are patterns it consistently misses.\n",
    "\n",
    "Cross-validation is especially useful when we have limited data, as it makes the most out of all available data by rotating through different training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae965c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Overfitting and Underfitting\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becba4b8",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/overfitting-and-underfitting-visually-explained-like-youre-five-8a389b511751"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5c5ab",
   "metadata": {},
   "source": [
    "Overfitting is a concept in machine learning and statistics, where a model learns the data a bit too well. \n",
    "\n",
    "It learns the data so much that it captures noise and random fluctuations in the data rather than just the underlying pattern.\n",
    "\n",
    "It’s like memorizing answers to specific questions without understanding the broader concept—so the model performs well on the training data but poorly on new data because it hasn’t learned the underlying pattern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e358454",
   "metadata": {},
   "source": [
    "**Example:** Imagine a student who memorizes every word in a specific book to prepare for an exam. If the exam questions come directly from the book, the student does great. But if the questions are on the general topic, the student struggles because they only memorized details rather than learning the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb09014",
   "metadata": {},
   "source": [
    "In a machine learning context, overfitting might look like a very complex model (e.g., a decision tree that is extremely deep and captures every small detail) that fits perfectly on the training data but makes poor predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902fd27",
   "metadata": {},
   "source": [
    "![img](https://miro.medium.com/v2/resize:fit:1200/1*YQ5tjb1TqNHenYMFk2tPog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86973544",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Underfitting** occurs when a model is too simple to capture the patterns in the data. \n",
    "\n",
    "It’s like trying to solve complex math problems with only basic arithmetic knowledge—the model can’t capture the structure of the data, resulting in poor performance on both training and new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219fb8a",
   "metadata": {},
   "source": [
    "**Example:** Imagine a student who doesn’t study much and only learns the most basic information about a topic. During the exam, they struggle to answer most questions, even the simpler ones, because they lack a sufficient understanding of the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e87d3",
   "metadata": {},
   "source": [
    "Finding the right balance—learning just enough patterns without noise—is key to building a model that performs well on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a4879",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Common Cross-Validation Methods\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135028b",
   "metadata": {},
   "source": [
    "### **Train-Test Split**\n",
    "\n",
    "The Train-Test Split is one of the simplest methods for model validation. In this approach, the dataset is split into two parts:\n",
    "\n",
    "- Training Set: Used to train the model on a subset of data.\n",
    "- Testing Set: Used to evaluate the model’s performance on a separate subset that the model hasn’t seen during training.\n",
    "\n",
    "Usually, the data is split in a ratio like 80-20 or 70-30 (training to testing) to ensure the model has enough data to learn from while reserving some data to test its generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ecdf39",
   "metadata": {},
   "source": [
    "**Limitations of Train-Test Split:**\n",
    "\n",
    "1. **Single Data Split**: The train-test split is based on just one division of the data, which may not be fully representative. \n",
    "Because, in a single split, there’s no guarantee that both the training and testing sets capture all the patterns and variations present in the full dataset. \n",
    "\n",
    "- For example, if the test set happens to contain a specific pattern or noise, the performance could be biased. \n",
    "Like, let's suppose for a house price prediction model, the test set includes only houses from high-priced neighborhoods. In this case, the model might look like it’s performing well on high-value homes but could actually struggle with low- or mid-priced properties because it hasn’t encountered that pattern in the test set.\n",
    "\n",
    "2. **Limited Information:** Since we only test the model on one subset, we may not get a comprehensive understanding of how it would perform on other unseen data.\n",
    "\n",
    "3. **Risk of Overfitting or Underfitting:** If the training or testing set is too small, there’s a higher chance the model could overfit or underfit, leading to less reliable predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372112b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108412a",
   "metadata": {},
   "source": [
    "### **K-Fold Cross Validation**\n",
    "\n",
    "It is a method used to evaluate a model's performance by splitting the data into \"K\" equal parts, or folds, and testing the model multiple times to ensure a more reliable estimate of its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e938f",
   "metadata": {},
   "source": [
    "![](https://dataaspirant.com/wp-content/uploads/2020/12/10-K-Fold-Cross-Validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e10b6",
   "metadata": {},
   "source": [
    "**How It Works:**\n",
    "\n",
    "**1. Divide the Data:**\n",
    "\n",
    "- Split the entire dataset into K equal-sized \"folds\" (subsets).\n",
    "- For example, in 5-Fold Cross-Validation, you divide the data into 5 parts.\n",
    "\n",
    "**2. Train and Test:**\n",
    "\n",
    "- First Round: Use 4 folds for training the model, and keep 1 fold for testing.\n",
    "- Second Round: Use a different fold for testing and the remaining folds for training.\n",
    "- Repeat this for each fold, so each part of the dataset gets a turn as the test set while the other folds are used for training.\n",
    "\n",
    "**3. Average the Results:**\n",
    "\n",
    "- After all the rounds, you get multiple performance scores (like accuracy, F1-Score, etc.).\n",
    "- You average these scores to get a more reliable estimate of how the model will perform on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbb069",
   "metadata": {},
   "source": [
    "```python\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56df089",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Leave-One-Out Cross-Validation (LOO)**\n",
    "\n",
    "It is a special case of K-Fold Cross-Validation where the number of folds is equal to the number of data points in the dataset. \n",
    "\n",
    "In each iteration, the model is trained on all the data points except for one, which is used as the test set. \n",
    "\n",
    "This process is repeated for every single data point in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462cc35",
   "metadata": {},
   "source": [
    "![](https://dataaspirant.com/wp-content/uploads/2020/12/7-LOOCV-Leave-One-Out-Cross-Validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0efba16",
   "metadata": {},
   "source": [
    "**How It Works:**\n",
    "\n",
    "1. Divide the dataset: For N data points, LOO uses N folds. In each iteration, 1 data point is used for testing, and the remaining N-1 data points are used for training.\n",
    "\n",
    "2. Train and Test: In each iteration, train the model on N-1 points and test it on the single remaining point.\n",
    "\n",
    "3. Repeat: Repeat the process for all N data points.\n",
    "\n",
    "4. Average Results: After all iterations, average the results (e.g., accuracy) to get a final performance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fc385",
   "metadata": {},
   "source": [
    "**Example:** For Iris dataset:\n",
    "\n",
    "1. The dataset has 150 samples.\n",
    "\n",
    "2. LOO will perform 150 iterations\n",
    "    - 149 samples are used for training, and 1 sample is used for testing\n",
    "    - In the first iteration, the model is trained on samples 2-150, and tested on sample 1.\n",
    "    - In the second iteration, the model is trained on samples 1, 3-150, and tested on sample 2.\n",
    "    - This continues for all 150 samples.\n",
    "\n",
    "3. Performance metrics calculated and averaged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8b321",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Stratified K-Fold Cross-Validation**\n",
    "\n",
    "It is a variation of K-Fold Cross-Validation that ensures that each fold has the same proportion of class labels as the original dataset.\n",
    "\n",
    "It is especially useful when dealing with imbalanced datasets, where some classes have significantly more samples than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f24b89",
   "metadata": {},
   "source": [
    "![](https://dataaspirant.com/wp-content/uploads/2020/12/8-Stratified-K-Fold-Cross-Validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ae988",
   "metadata": {},
   "source": [
    "**How It Works:**\n",
    "\n",
    "1. Divide the dataset: Similar to K-Fold, the dataset is split into K folds. However, the split is done in such a way that each fold maintains the same class distribution as the original dataset.\n",
    "\n",
    "2. Train and Test: The process of training the model on K-1 folds and testing on the remaining fold is the same, but now each fold has a representative proportion of class labels.\n",
    "\n",
    "3. Repeat: The process is repeated for all K folds.\n",
    "\n",
    "4. Average Results: The results from each fold are averaged to get a final performance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335b8d1",
   "metadata": {},
   "source": [
    "**Example:** For Iris dataset:\n",
    "\n",
    "1. The dataset has 150 samples. Let’s choose K = 5 as an example\n",
    "\n",
    "2.  The dataset will be split into 5 folds, with 30 samples per fold\n",
    "    - the samples are split in such a way that each fold has the same proportion of classes as the original dataset.\n",
    "    - Since the Iris dataset has three classes (setosa, versicolor, virginica), each fold will contain approximately 10 samples from each class.\n",
    "    - First Fold: The model will train on the remaining 4 folds (120 samples), and test on the first fold (30 samples), ensuring it contains an equal number of setosa, versicolor, and virginica samples.\n",
    "    - Second Fold: The model will train on the remaining 4 folds (after excluding the second fold) and test on the second fold, again ensuring class balance in both the training and testing sets.\n",
    "\n",
    "3. Performance metrics calculated and averaged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93111cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In both methods, the model gets trained multiple times, but **Stratified K-Fold is typically faster** since it doesn’t need to train the model for every single data point (unlike LOO, which trains on N-1 samples upto N iterations). \n",
    "\n",
    "Stratified K-Fold also **ensures each fold has a similar distribution** of the classes, which is especially useful if the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0d089",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Hands-On Activity: Model Evaluation and Cross Validation\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d503cd",
   "metadata": {},
   "source": [
    "Refer to the notebook: Hands-On Model Evaluation and Cross Validation.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6b263",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Metric Selection Based on Problem Type\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256967f2",
   "metadata": {},
   "source": [
    "In this part of the class, we focus on selecting the appropriate evaluation metric for different real-world problems. It's important to choose the right metric because different problems have different priorities, and using the wrong metric can lead to misleading conclusions.\n",
    "\n",
    "To make this interactive, let's ask students to choose the most appropriate metric for the given scenarios. After each question, encourage students to justify their answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario-Based Questions\n",
    "\n",
    "### 1. Scenario: Detecting Credit Card Fraud\n",
    "- **Question:** \"For detecting credit card fraud, would accuracy be enough?\"\n",
    "- **Answer:** **No.** In credit card fraud detection, the dataset is often imbalanced (very few fraudulent transactions compared to legitimate ones). In this case, accuracy is misleading because even if the model predicts \"no fraud\" for most transactions, it can still achieve high accuracy. Instead, **Precision**, **Recall**, or **F1-Score** are more informative metrics. Precision measures how many of the predicted fraudulent transactions were actually frauds, while Recall measures how many fraudulent transactions were detected. F1-Score balances Precision and Recall.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Scenario: Predicting Disease in a Population\n",
    "- **Question:** \"If you are building a model to predict a disease in a population, would accuracy be a good metric?\"\n",
    "- **Answer:** **No.** Similar to the credit card fraud example, disease prediction often deals with imbalanced classes. If the disease is rare, a model that predicts \"no disease\" for everyone can still have high accuracy. **Recall** would be a more important metric in this case, as you want to minimize the false negatives (missed diagnoses). In some cases, **F1-Score** might be appropriate if you want a balance between Recall and Precision.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Scenario: Customer Churn Prediction\n",
    "- **Question:** \"For predicting customer churn (whether a customer will leave a service), would **Accuracy** be a reliable metric?\"\n",
    "- **Answer:** **No.** In churn prediction, the class distribution can be skewed (more customers staying than leaving). Using **Accuracy** might give a false sense of good performance. **Precision** and **Recall** would be more helpful to understand how well the model predicts churners and non-churners, and **F1-Score** could provide a balanced evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Scenario: Email Spam Detection\n",
    "- **Question:** \"For detecting spam emails, which metric would you prioritize?\"\n",
    "- **Answer:** **Precision** and **Recall** would both be important. You don't want too many legitimate emails (false positives) marked as spam, so **Precision** is crucial. However, you also want to make sure that as many spam emails as possible are detected, so **Recall** is also important. **F1-Score** can help balance both Precision and Recall.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Scenario: Movie Recommendation System\n",
    "- **Question:** \"For a movie recommendation system, would **Accuracy** be the most useful metric?\"\n",
    "- **Answer:** **No.** In a recommendation system, **Accuracy** doesn’t capture the essence of a good recommendation. You’d want to focus on metrics like **Precision**, which measures how many relevant items (movies) are recommended in the top K predictions. **Recall@K** can also be used to measure how many relevant movies are found among the top recommendations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd37519",
   "metadata": {},
   "source": [
    "### 6. Scenario: Predicting Student Performance\n",
    "- **Question:** \"For predicting whether a student will pass or fail a course, would **Accuracy** be a good metric to use?\"\n",
    "- **Answer:** **No.** In cases like this, there could be an imbalance in the number of students passing versus failing. **Accuracy** could be misleading because the model might predict \"pass\" for most students and still achieve high accuracy. Instead, **Precision** and **Recall** would be better metrics. **Recall** ensures you capture most of the failing students (to help intervene in time), and **Precision** would ensure you’re correctly identifying those who are at risk of failing. **F1-Score** could provide a balanced view of both metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Scenario: Predicting Loan Default\n",
    "- **Question:** \"For predicting loan defaults (whether a borrower will default on a loan), would **Accuracy** alone give a reliable assessment of the model?\"\n",
    "- **Answer:** **No.** In a loan default prediction problem, defaults may be much less frequent than non-defaults, creating a class imbalance. A model predicting \"no default\" for everyone could still show high **Accuracy**, but it wouldn’t be useful. Instead, **Recall** would be more important, as we don’t want to miss any potential defaults, and **Precision** would be crucial to avoid falsely classifying customers as likely to default. **F1-Score** balances Precision and Recall.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Scenario: Image Classification for Medical Imaging\n",
    "- **Question:** \"For classifying whether an X-ray image shows signs of a disease (like pneumonia), should **Accuracy** be the primary evaluation metric?\"\n",
    "- **Answer:** **No.** Medical images often present a **high class imbalance** where healthy images outnumber diseased ones. In this case, **Recall** is the most important metric, as we want to ensure that as many cases of disease as possible are detected, even if it means a higher number of false positives. **Precision** is also important to avoid unnecessary treatments. **F1-Score** helps balance both Precision and Recall, offering a more comprehensive metric for evaluating the model's performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Scenario: Predicting Housing Prices\n",
    "- **Question:** \"For predicting housing prices based on features like location, square footage, etc., would **Accuracy** be a good fit?\"\n",
    "- **Answer:** **No.** **Accuracy** isn't suitable for regression tasks like predicting housing prices. Instead, we use metrics like **Mean Absolute Error (MAE)** or **Mean Squared Error (MSE)** to evaluate how well the model's predictions match the actual housing prices. These metrics measure how far off predictions are from the true values.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Scenario: Recommending Products to Users\n",
    "- **Question:** \"For recommending products to users in an e-commerce platform, would **Accuracy** be a useful metric?\"\n",
    "- **Answer:** **No.** In recommendation systems, **Accuracy** doesn’t provide much insight because it’s not about whether a user clicked on a product or not, but about how relevant those recommendations are. **Precision@K** and **Recall@K** are more appropriate because they measure how many relevant products are included in the top K recommendations. These metrics focus on providing the most valuable items to users, leading to higher engagement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2185430-3791-45df-824f-bdec6d7145e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Live Exercise</font> </b>\n",
    "</div>\n",
    "\n",
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d61af-f3e8-4afc-a66b-3814e160aaf3",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task 1: Based on the given data, perform the following tasks:\n",
    "\n",
    "    - Calculate Accuracy, Precision, Recall and F1 Score\n",
    "    - Perform Model training and evaluation using the K-Fold Cross Validation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef922ef",
   "metadata": {},
   "source": [
    "Toy Dataset: Diabetes Prediction\n",
    "\n",
    "For predicting whether a patient has a disease (e.g., predicting diabetes based on certain health parameters)\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "Age: The age of the patient.\n",
    "BMI: Body Mass Index.\n",
    "Blood Pressure: Blood pressure level.\n",
    "Insulin: Insulin level (fasting blood sugar).\n",
    "Glucose: Glucose level.\n",
    "Pregnancies: Number of pregnancies (for women).\n",
    "Outcome: 1 if the patient has diabetes, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Toy Dataset for Diabetes Prediction\n",
    "data = {\n",
    "    'Age': [45, 50, 60, 30, 70, 35, 40, 60, 25, 55],\n",
    "    'BMI': [30.1, 32.2, 35.0, 28.5, 31.8, 25.6, 29.3, 33.5, 24.5, 27.2],\n",
    "    'Blood Pressure': [130, 140, 150, 120, 160, 125, 135, 145, 118, 142],\n",
    "    'Insulin': [180, 220, 300, 140, 350, 200, 210, 280, 120, 250],\n",
    "    'Glucose': [120, 140, 160, 100, 180, 110, 115, 150, 95, 145],\n",
    "    'Pregnancies': [2, 3, 0, 1, 4, 2, 1, 3, 0, 2],\n",
    "    'Outcome': [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb8a79",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9ddd9-5558-4b1d-a3e7-04c3dca33b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample SOlution to the problem \n",
    "# This is not an exhaustive solution\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df.drop(columns=['Outcome'])\n",
    "y = df['Outcome']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model: Logistic Regression\n",
    "model = LogisticRegression()\n",
    "\n",
    "# K-Fold Cross Validation (5 folds)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracies.append(accuracy_score(y_val_fold, y_pred))\n",
    "    precisions.append(precision_score(y_val_fold, y_pred))\n",
    "    recalls.append(recall_score(y_val_fold, y_pred))\n",
    "    f1_scores.append(f1_score(y_val_fold, y_pred))\n",
    "\n",
    "# Average metrics across all folds\n",
    "print(f\"Average Accuracy: {np.mean(accuracies):.2f}\")\n",
    "print(f\"Average Precision: {np.mean(precisions):.2f}\")\n",
    "print(f\"Average Recall: {np.mean(recalls):.2f}\")\n",
    "print(f\"Average F1-Score: {np.mean(f1_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "1. topic:\n",
    "    - question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b86c1-64b0-4abd-8ba9-54746bdc9007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

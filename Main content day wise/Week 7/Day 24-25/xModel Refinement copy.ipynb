{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "     Model Refinement <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Improving model performance based on evaluation results</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.  Understanding Model Training Errors<p><p> \n",
    "2.  Techniques for Model Refinement <p>\n",
    "3.  Advanced Refinement Techniques <p>\n",
    "4.  Hands-On: Model Refinement Practice  <p>  \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    If you were a superhero, what would your superpower be?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c5cf4",
   "metadata": {},
   "source": [
    "By the end of this session, students will be able to:\n",
    "\n",
    "- Evaluate key performance metrics for model assessment.\n",
    "- Identify and address issues of underfitting and overfitting in models.\n",
    "- Apply a structured process of data preprocessing, hyperparameter tuning, and ensemble methods for model improvement.\n",
    "- Confidently implement refinement techniques to boost model accuracy and robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6508920",
   "metadata": {},
   "source": [
    "**Evaluation Metrics Recap:**\n",
    "\n",
    "**Core Metrics**\n",
    "- Accuracy: The proportion of correct predictions out of the total predictions. Often used when class distributions are balanced.\n",
    "- Precision: The ratio of correctly predicted positive observations to the total predicted positives. Precision is critical when false positives are costly (e.g., spam detection).\n",
    "- Recall: The ratio of correctly predicted positive observations to all observations in the actual class. Useful in cases where capturing all positives is crucial, like detecting diseases.\n",
    "- F1-Score: The harmonic mean of precision and recall. Balances both metrics, making it ideal when both false positives and false negatives are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e72c9d",
   "metadata": {},
   "source": [
    "**Choosing the Right Metric**\n",
    "It is important to select the right metric that ensures the model aligns with the real-world requirements of a given problem:\n",
    "\n",
    "- **Classification**: Use AUC in credit fraud detection to balance detection rates with false positives.\n",
    "- **Regression**: Use RMSE in forecasting where larger errors need penalization, such as in stock price predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e654fd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Understanding Model Training Errors\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089eb8d0",
   "metadata": {},
   "source": [
    "In machine learning, errors are an inevitable part of building and refining models.\n",
    "\n",
    "Understanding where these errors come from and how to manage them is essential for creating effective models that perform well on new, unseen data. \n",
    "\n",
    "When we train a machine learning model, we want it to be as accurate as possible—not just on the training data, but also on the data it has never seen before. This is where the generalization of a model comes into play. \n",
    "\n",
    "Good generalization means the model can perform well on unseen data, and is not just memorizing the training examples.\n",
    "\n",
    "Some of the most important model training errors are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb04c1",
   "metadata": {},
   "source": [
    "1. **Bias:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf93c83",
   "metadata": {},
   "source": [
    "Imagine you're trying to guess the height of a group of people based on their age. \n",
    "\n",
    "If you always guess the same height for everyone, no matter what their age is, that would be a biased guess. \n",
    "\n",
    "You're not considering their actual ages and just making a simple guess based on some wrong assumption.\n",
    "\n",
    "\n",
    "*Simple Example*\n",
    "\n",
    "Let's say you’re using a machine learning model to predict house prices based on their size (in square feet). If your model always predicts the same price for every house, regardless of its size, that would be high bias. The model is too simple and doesn't take enough details into account. It’s not learning the true relationship between house size and price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105eafb",
   "metadata": {},
   "source": [
    "- Bias refers to systematic errors in the model, introduced by approximating a real-world problem, which may be complex, by a simplified model.\n",
    "\n",
    "- It occurs when the model makes assumptions that are too simple, often ignoring important features or relationships in the data.\n",
    "\n",
    "- High bias typically results in underfitting, where the model is too simple to capture the underlying patterns of the data.\n",
    "\n",
    "- Example: Using a linear regression model to predict house prices when the true relationship is non-linear (e.g., polynomial)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a9184",
   "metadata": {},
   "source": [
    "![](https://static.javatpoint.com/tutorial/machine-learning/images/bias-and-variance-in-machine-learning6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05084e1e",
   "metadata": {},
   "source": [
    "2. **Variance:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db50c35",
   "metadata": {},
   "source": [
    "Variance is like a model's sensitivity to small changes in the data. \n",
    "\n",
    "Imagine you're trying to guess the height of people based on their age. If you adjust your guess every time you see a new person, even when they’re about the same age, then your guesses might be all over the place — that’s high variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e61fd",
   "metadata": {},
   "source": [
    "In machine learning, high variance means the model is trying too hard to fit every little detail in the training data, even the noise or random patterns that aren’t important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30cf035",
   "metadata": {},
   "source": [
    "*Simple Example*\n",
    "Suppose you have a model that predicts house prices based on the square footage and several other features. If your model is high in variance, it might make big adjustments for every single small change in these features. So, a house with just a tiny difference in square footage might get a drastically different price prediction, even if that change shouldn't really matter much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63312e3a",
   "metadata": {},
   "source": [
    "- Variance refers to how much the model's predictions fluctuate when trained on different subsets of data.\n",
    "- It occurs when the model is too complex and overly sensitive to the noise or fluctuations in the training data.\n",
    "- High variance typically leads to overfitting, where the model fits the training data too well but fails to generalize to new, unseen data.\n",
    "- Example: A decision tree with many branches that memorizes the training data instead of learning the general patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791294c7",
   "metadata": {},
   "source": [
    "So, you want to find a balance where the model is not too sensitive (high variance) but also not too simple (high bias). This helps the model perform well on both the training data and any new data it sees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b3357",
   "metadata": {},
   "source": [
    "**Bias-Variance Tradeoff**\n",
    "\n",
    "- The Bias-Variance Tradeoff refers to the tension between these two sources of error. \n",
    "\n",
    "As you reduce bias (by using a more complex model), variance tends to increase (because the model is more sensitive to fluctuations in the training data), and vice versa.\n",
    "\n",
    "- Goal: Find the optimal balance between bias and variance, where the model has both low bias and low variance, resulting in good generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c898db",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*wPUGn4buYw4LYISGL-TUuA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f5b2f3",
   "metadata": {},
   "source": [
    "**3. Underfitting vs. Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b831b1",
   "metadata": {},
   "source": [
    "Underfitting occurs when the model is too simple and cannot capture the underlying patterns in the data. This leads to poor performance on both the training set and the test set.\n",
    "- High Bias, Low Variance: Simple models (underfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554da65",
   "metadata": {},
   "source": [
    "Overfitting occurs when the model is too complex and captures not just the true patterns but also the noise in the data. This leads to excellent performance on the training set but poor generalization to new, unseen data\n",
    "- Low Bias, High Variance: Complex models (overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eed610",
   "metadata": {},
   "source": [
    "**4. Data Imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988c4af",
   "metadata": {},
   "source": [
    "Data imbalance happens when you have unequal numbers of examples in different classes of your dataset. \n",
    "\n",
    "For example, if you’re building a model to detect fraud, you might have a dataset where 95% of transactions are normal and only 5% are fraudulent. This difference is called imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a812f2",
   "metadata": {},
   "source": [
    "- Bias Toward Majority Class: When data is imbalanced, the model may become biased towards the majority class (the larger group). So, in the fraud example, the model might mostly predict \"normal\" because it’s the majority. This can cause the model to ignore or perform poorly on the minority class (fraudulent transactions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0773fa2",
   "metadata": {},
   "source": [
    "- Poor Accuracy on on Minority Class: If the minority class (fraud) is more important to predict, imbalance can be especially problematic. Even if the model achieves high overall accuracy by predicting the majority class, it might fail to detect the minority class, which is often the class we care most about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9bd38c",
   "metadata": {},
   "source": [
    "In real life, we often fix data imbalance by resampling the data \n",
    "\n",
    "— either by **oversampling** the minority class or **undersampling** the majority class \n",
    "\n",
    "— and instead of just accuracy, use metrics like Precision, Recall, and the F1 score, which give a better picture of how well the model is performing on each class. \n",
    "\n",
    "Additionally, special algorithms like SMOTE (Synthetic Minority Over-sampling Technique) can create synthetic samples to help balance the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f8f7e",
   "metadata": {},
   "source": [
    "**5. Data Drift**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c73d97",
   "metadata": {},
   "source": [
    "Data Drift happens when the data used to train a model changes over time, so it no longer matches the new data the model sees in the real world. This can lead to poor performance because the model is making predictions based on outdated patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60d12f",
   "metadata": {},
   "source": [
    "**Reasons:**\n",
    "\n",
    "- Changing User Behavior: For example, in a recommendation system, user preferences might shift over time.\n",
    "- Seasonal Changes: Data may change with the seasons or economic conditions (like shopping trends during holidays).\n",
    "- System Updates: Changes in how data is collected or processed (like updates to sensors or apps) can also cause drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3faa0c",
   "metadata": {},
   "source": [
    "**Types of Data Drift:**\n",
    "\n",
    "1. **Covariate Drift:** When the input data (features) distribution changes but the relationship to the target variable remains the same.\n",
    "\n",
    "**Example:** \n",
    "Let’s say a model was trained to assess creditworthiness based on features like age, income, and spending habits. \n",
    "\n",
    "Over time, if economic conditions change (like during a recession), people's spending habits and income levels may shift, even though the relationship between these features and credit risk (the target variable) remains the same.\n",
    "\n",
    "2. **Concept Drift:** When the relationship between inputs and the target variable itself changes. \n",
    "\n",
    "**Example:**\n",
    "Suppose a model is trained to classify emails as spam based on specific keywords and patterns. Over time, as spammers develop new tactics, the patterns in spam emails change (e.g., new words, different structures).\n",
    "\n",
    "Now, the original relationship between certain keywords and the label \"spam\" no longer holds, meaning the model becomes less effective. This change in the underlying relationship between inputs (keywords) and the target (spam/not spam) is concept drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6525e",
   "metadata": {},
   "source": [
    "In real life, we handle data drift by regularly monitoring model performance, retraining the model with updated data, and using adaptive models that adjust to new data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c762cd6-ee35-4e4e-aa54-600db6a9c23d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Model Refinement\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aef031-1416-4707-b2a0-fc60ca4ee7ca",
   "metadata": {},
   "source": [
    "Model Refinement in machine learning refers to the process of improving a model's performance by iterating on it through various techniques. \n",
    "\n",
    "This step is crucial to ensure that the model generalizes well to new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb2e43-d552-4f52-b5e7-c4d8af63b29e",
   "metadata": {},
   "source": [
    "The model refinement process typically includes:\n",
    "\n",
    "1. **Evaluating Model Performance:** Carefully analyzing model metrics to identify where the model is falling short.\n",
    "2. **Identifying Key Issues:** Pinpointing problems like high bias (indicating underfitting), high variance (overfitting), or data imbalance.\n",
    "3. **Applying Improvement Techniques:** Using targeted methods, such as tuning hyperparameters, changing model architecture, or refining the dataset, to tackle identified issues.\n",
    "4. **Iterative Testing and Validation:** Running and evaluating improvements in multiple rounds to ensure they lead to better and more stable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b10a45",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Techniques for Model Refinement\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8332a5",
   "metadata": {},
   "source": [
    "Some Techniques to systematically refine your models and make their outputs and predictions better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351bb65",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Step 1: Data Preprocessing Refinement\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5884eb",
   "metadata": {},
   "source": [
    "In this first step of model refinement, we’ll focus on preparing the data to improve model performance. Well-preprocessed data ensures that models learn effectively, reducing bias and variance in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8c42f",
   "metadata": {},
   "source": [
    "### **Data Scaling and Normalization:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965f668",
   "metadata": {},
   "source": [
    "In scaling (Standardization), the goal is to standardize features so they have a mean of 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08858e03",
   "metadata": {},
   "source": [
    "\n",
    "- **When to Scale:** Scaling techniques like StandardScaler or MinMaxScaler are critical when using algorithms sensitive to feature magnitudes, such as distance-based algorithms (e.g., K-Nearest Neighbors) and linear models.\n",
    "- **How it Helps:** Scaling or normalizing features to the same range or distribution can improve model convergence, stability, and predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c51ed4",
   "metadata": {},
   "source": [
    "In Normalization, the goal is to transforms features so they fall within a certain range, typically [0, 1] or [-1, 1].\n",
    "\n",
    "More or less the same, but normalization **adjusts the range of data** to a specific interval (e.g., [0, 1]), while **scaling** **adjusts the spread** or **distribution** of the data, typically by changing the mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815041d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6e5e787",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Suppose we have a dataset with the following features to predict house prices:\n",
    "\n",
    "- **Size** (in square feet)\n",
    "- **Number of Rooms**\n",
    "- **Distance to City Center** (in miles)\n",
    "\n",
    "Here’s a sample of what the dataset might look like:\n",
    "\n",
    "| Size (sq ft) | Number of Rooms | Distance to City Center (miles) | Price ($) |\n",
    "|--------------|-----------------|----------------------------------|-----------|\n",
    "| 2500         | 4               | 8                                | 500,000   |\n",
    "| 1500         | 3               | 15                               | 300,000   |\n",
    "| 1200         | 2               | 20                               | 200,000   |\n",
    "| 1800         | 3               | 10                               | 350,000   |\n",
    "\n",
    "Without scaling, the large range of values (e.g., Size in thousands vs. Distance in tens) may lead some models to interpret Size as more important than other features. Here’s how scaling can address this:\n",
    "\n",
    "**Standard Scaling**\n",
    "\n",
    "Standard scaling (using `StandardScaler`) adjusts each feature so that it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Transformed dataset (Standard Scaled):**\n",
    "\n",
    "| Size (scaled) | Number of Rooms (scaled) | Distance to City Center (scaled) |\n",
    "|---------------|--------------------------|-----------------------------------|\n",
    "| 1.14          | 1.0                      | -0.6                             |\n",
    "| -0.2          | 0.0                      | 0.9                              |\n",
    "| -0.8          | -1.0                     | 1.5                              |\n",
    "| -0.1          | 0.0                      | -0.3                             |\n",
    "\n",
    "**Min-Max Scaling**\n",
    "\n",
    "Min-Max scaling (using `MinMaxScaler`) transforms each feature to a specified range, typically [0, 1].\n",
    "\n",
    "**Transformed dataset (Min-Max Scaled):**\n",
    "\n",
    "| Size (scaled) | Number of Rooms (scaled) | Distance to City Center (scaled) |\n",
    "|---------------|--------------------------|-----------------------------------|\n",
    "| 1.0           | 1.0                      | 0.0                              |\n",
    "| 0.3           | 0.5                      | 0.54                             |\n",
    "| 0.0           | 0.0                      | 1.0                              |\n",
    "| 0.5           | 0.5                      | 0.31                             |\n",
    "\n",
    "**Why This Matters**\n",
    "\n",
    "- **Improved Model Performance**: Models sensitive to feature scales, like K-Nearest Neighbors and Linear Regression, perform better with scaled features.\n",
    "- **Faster Convergence**: Gradient-based models, like logistic regression or neural networks, converge faster when features are on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec68c6",
   "metadata": {},
   "source": [
    "*Find code implementation in the \"DataScaling and Normalisation.ipynb\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8193bf",
   "metadata": {},
   "source": [
    "**Techniques Used in Scaling**\n",
    "\n",
    "- Standard Scaling (Z-score Normalization)\n",
    "- Robust Scaling\n",
    "- Max Abs Scaling\n",
    "\n",
    "**Techniques Used in Normalization**\n",
    "\n",
    "- Min-Max Normalization (Rescaling)\n",
    "- L2 Normalization (Euclidean norm)\n",
    "- L1 Normalization (Manhattan norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54b310",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd74964",
   "metadata": {},
   "source": [
    "### **Feature Engineering:**\n",
    "\n",
    "- **Feature Selection:** Removing features with high correlation or low variance can reduce redundancy and prevent the model from overemphasizing irrelevant or repetitive information.\n",
    "- **Feature Creation/Extraction:** New features can be engineered by transforming or combining existing ones, potentially uncovering valuable patterns that the model might otherwise miss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb75536",
   "metadata": {},
   "source": [
    "**Handling Class Imbalance:**\n",
    "- **Why it Matters:** In classification problems, class imbalance can cause models to perform poorly on the minority class, as they may simply predict the majority class to achieve high accuracy.\n",
    "- **Techniques to Address Imbalance:**\n",
    "    - **Oversampling** (e.g., duplicating instances of the minority class).\n",
    "    - **Undersampling** (e.g., reducing the majority class).\n",
    "    - **SMOTE (Synthetic Minority Over-sampling Technique)**, which creates synthetic samples for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5865a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Hyperparameter Tuning\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f70d82",
   "metadata": {},
   "source": [
    "Hyperparameters are configurations **external to the model** that cannot be learned from data during training. They differ from parameters, which are learned by the model.\n",
    "\n",
    "For instance, in a **decision tree**, **parameters** include **split points** within nodes, whereas **hyperparameters** may include the **tree depth** or **minimum number of samples** required to split a node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d7a9b",
   "metadata": {},
   "source": [
    "Proper tuning of hyperparameters can significantly enhance the model's accuracy, generalizability, and training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4c96d",
   "metadata": {},
   "source": [
    "**Hyperparameters vs. Parameters**\n",
    "\n",
    "- **Parameters**: Learned directly from the data (e.g., weights in a linear regression model).\n",
    "- **Hyperparameters**: Set manually before training begins and control the behavior of the learning algorithm (e.g., k in k-nearest neighbors).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0528d",
   "metadata": {},
   "source": [
    "**Hyperparametes for some commonly used algorithms**\n",
    "\n",
    "1. **Linear Regression**\n",
    "   - **Hyperparameters**: \n",
    "     - Regularization strength (for Ridge and Lasso: alpha)\n",
    "     - Solver type (e.g., ‘lbfgs’, ‘saga’)\n",
    "\n",
    "2. **Logistic Regression**\n",
    "   - **Hyperparameters**: \n",
    "     - Regularization strength (C)\n",
    "     - Solver type (e.g., ‘liblinear’, ‘newton-cg’)\n",
    "     - Max iterations\n",
    "\n",
    "3. **Decision Trees**\n",
    "   - **Hyperparameters**:\n",
    "     - Maximum depth (max_depth)\n",
    "     - Minimum samples split (min_samples_split)\n",
    "     - Minimum samples leaf (min_samples_leaf)\n",
    "     - Max features\n",
    "\n",
    "4. **Random Forest**\n",
    "   - **Hyperparameters**:\n",
    "     - Number of trees (n_estimators)\n",
    "     - Max depth (max_depth)\n",
    "     - Max features\n",
    "     - Minimum samples per leaf (min_samples_leaf)\n",
    "     - Bootstrap (for sampling)\n",
    "\n",
    "5. **Support Vector Machines (SVM)**\n",
    "   - **Hyperparameters**:\n",
    "     - Regularization parameter (C)\n",
    "     - Kernel type (e.g., ‘linear’, ‘rbf’, ‘poly’)\n",
    "     - Gamma (for RBF kernel)\n",
    "     - Degree (for polynomial kernel)\n",
    "\n",
    "6. **K-Nearest Neighbors (KNN)**\n",
    "   - **Hyperparameters**:\n",
    "     - Number of neighbors (n_neighbors)\n",
    "     - Distance metric (e.g., ‘euclidean’, ‘manhattan’)\n",
    "     - Weights (uniform or distance)\n",
    "\n",
    "7. **Gradient Boosting Machines (GBM) / XGBoost / LightGBM**\n",
    "   - **Hyperparameters**:\n",
    "     - Number of trees (n_estimators)\n",
    "     - Learning rate (learning_rate)\n",
    "     - Maximum depth (max_depth)\n",
    "     - Subsample\n",
    "     - Min child weight (XGBoost)\n",
    "\n",
    "8. **Naive Bayes**\n",
    "   - **Hyperparameters**:\n",
    "     - Smoothing parameter (alpha)\n",
    "\n",
    "9. **K-Means Clustering**\n",
    "   - **Hyperparameters**:\n",
    "     - Number of clusters (n_clusters)\n",
    "     - Initialization method (init: ‘k-means++’ or ‘random’)\n",
    "     - Max iterations (max_iter)\n",
    "     - Tolerance for convergence (tol)\n",
    "\n",
    "10. **Neural Networks (e.g., MLPClassifier)**\n",
    "    - **Hyperparameters**:\n",
    "      - Number of hidden layers (hidden_layer_sizes)\n",
    "      - Number of neurons per layer\n",
    "      - Activation function (e.g., ‘relu’, ‘tanh’)\n",
    "      - Learning rate (learning_rate)\n",
    "      - Batch size\n",
    "      - Epochs (max_iter)\n",
    "      - Solver type (e.g., ‘adam’, ‘sgd’)\n",
    "\n",
    "11. **Principal Component Analysis (PCA)**\n",
    "    - **Hyperparameters**:\n",
    "      - Number of components (n_components)\n",
    "      - Whitening (True/False)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585a471",
   "metadata": {},
   "source": [
    "**Common Hyperparameter Tuning Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75cb22",
   "metadata": {},
   "source": [
    "**1. Grid Search:**\n",
    "\n",
    "This method tests all possible combinations of a predefined set of hyperparameter values exhaustively. Though powerful, it can be computationally expensive.\n",
    "\n",
    "Example: Testing combinations of max_depth and min_samples_split for a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55eb903",
   "metadata": {},
   "source": [
    "**2. Random Search:**\n",
    "\n",
    "Instead of testing all combinations, random search samples a fixed number of hyperparameter combinations randomly from a specified distribution. This often finds optimal settings more efficiently than grid search.\n",
    "\n",
    "Example: Randomly selecting learning_rate and n_estimators values for a gradient boosting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a114a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23d068bd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Regularization Techniques\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ef1fa",
   "metadata": {},
   "source": [
    "What is Regularization? (Brief overview)\n",
    "Types of Regularization:\n",
    "L1 Regularization (Lasso): Tends to produce sparse models by penalizing high coefficients.\n",
    "L2 Regularization (Ridge): Helps reduce overfitting by penalizing large coefficients but keeps all features.\n",
    "Activity: Students add L1 or L2 regularization to a model in code and compare results before and after regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0d089",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Advanced Refinement Techniques\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d758075",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Ensemble Methods \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe3a40",
   "metadata": {},
   "source": [
    "Step 3: Ensemble Methods\n",
    "Ensemble Learning Introduction:\n",
    "Explain how ensembles (e.g., Bagging, Boosting, Stacking) can improve model performance by combining multiple models.\n",
    "Examples:\n",
    "Bagging: Random Forest\n",
    "Boosting: AdaBoost, Gradient Boosting, XGBoost\n",
    "Stacking: Combining different models (e.g., Decision Tree, SVM) into a final meta-model.\n",
    "Practical Exercise: Implement a simple ensemble model and observe performance gains over individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d47cc1",
   "metadata": {},
   "source": [
    "Step 4: Cross-Validation and Model Validation Techniques\n",
    "K-Fold Cross-Validation:\n",
    "Recap the importance of cross-validation in ensuring that model performance is not overestimated.\n",
    "Practical Exercise: Have students apply K-Fold cross-validation and calculate average performance metrics.\n",
    "Advanced Cross-Validation (e.g., Stratified K-Fold, Time Series Split):\n",
    "Discuss when and how to use these techniques for imbalanced or time-based data.\n",
    "Train-Test-Split Best Practices:\n",
    "Ensure students understand the need for a proper train-test split and common pitfalls (e.g., leakage between training and test sets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5b5ed",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> 4. Hands-On Mini-Project: Model Refinement (Optional) </font> </b>\n",
    "</div>\n",
    "\n",
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b51b49",
   "metadata": {},
   "source": [
    "### **Project Title: Customer Churn Prediction Model Refinement Challenge**\n",
    "\n",
    "**Description**\n",
    "\n",
    "This project centers on **predicting customer churn**, a popular real-world problem in industries like **telecom**, **SaaS**, or **e-commerce**. The dataset contains information about customer demographics, subscription details, service usage, and customer support interactions. You will build a machine learning model to predict whether a customer will churn (i.e., stop using the service) based on these features.\n",
    "\n",
    "This project is modern, widely applicable, and gives you insight into solving a business-critical problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f71127",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "1. **Define the Problem**: Predict customer churn using the Telco Customer Churn dataset.  \n",
    "2. **Baseline Model**: Train a simple classifier with minimal preprocessing.  \n",
    "3. **Evaluate Baseline**: Analyze accuracy, recall, and other key metrics.  \n",
    "4. **Data Preprocessing**: Handle missing values, encode categorical features, and scale numerical data.  \n",
    "5. **Feature Engineering**: Create new features, remove redundant ones, and analyze correlations.  \n",
    "6. **Hyperparameter Tuning**: Optimize model parameters using GridSearchCV or similar tools.  \n",
    "7. **Model Ensembling**: Use ensemble techniques like Gradient Boosting or Random Forest to enhance performance.  \n",
    "8. **Final Evaluation**: Compare refined model metrics with baseline results and assess business value.  \n",
    "9. **Presentation**: Summarize findings, improvements, and rationale for techniques used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d61af-f3e8-4afc-a66b-3814e160aaf3",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task 1: description of task\n",
    "\n",
    "    - instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e67385",
   "metadata": {},
   "source": [
    "**Step 1: Define the Problem**\n",
    "\n",
    "**Objective**: Predict whether a customer will churn (`Churn = 1`) or not (`Churn = 0`) using the Telco Customer Churn dataset.\n",
    "\n",
    "- Dataset: Telco Customer Churn (`Telco-Customer-Churn.csv`).\n",
    "- Target Variable: `Churn`.\n",
    "- Features: Various customer attributes such as contract type, payment method, and internet service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051c0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefaec9d",
   "metadata": {},
   "source": [
    "Use the line below to download and import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9d5fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install kagglehub # if kagglehub is not installed yet\n",
    "#import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "#path = kagglehub.dataset_download(\"blastchar/telco-customer-churn\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96078227",
   "metadata": {},
   "source": [
    "- A link to the dataset : [Telco Customer Churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e53d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load dataset\n",
    "\n",
    "# Load the dataset\n",
    "# Use the path to get to the dataset\n",
    "# Rename the dataset and choose the right path\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('c:\\\\Users\\\\devid\\\\.cache\\\\kagglehub\\\\datasets\\\\blastchar\\\\telco-customer-churn\\\\versions\\\\1\\\\Telco-Customer-Churn.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36439e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preprocessing\n",
    "\n",
    "# Minimal preprocessing: Handle missing values and encode target variable\n",
    "data = data.dropna()  # Drop missing values\n",
    "data['Churn'] = data['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)  # Encode target variable\n",
    "\n",
    "# Replace empty strings or whitespace in string columns with NaN and drop them\n",
    "data.replace(r'^\\s*$', pd.NA, regex=True, inplace=True)  # Replace empty strings with NaN\n",
    "# data.dropna(inplace=True)  # Drop any rows with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a14d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define column categories\n",
    "categorical_cols = ['Contract', 'PaymentMethod', 'InternetService', 'MultipleLines']\n",
    "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', \n",
    "               'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n",
    "               'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling']\n",
    "\n",
    "# Define preprocessing for binary columns\n",
    "binary_preprocessor = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values\n",
    "    ('ordinal', OrdinalEncoder())  # Ordinal encode binary columns\n",
    "])\n",
    "\n",
    "# Define preprocessing for other categorical columns\n",
    "categorical_preprocessor = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encode\n",
    "])\n",
    "\n",
    "# Define preprocessing for numerical columns\n",
    "numerical_preprocessor = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),           # Handle missing values\n",
    "    ('scaler', StandardScaler())                           # Scale numerical features\n",
    "])\n",
    "\n",
    "# Combine into a single preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_preprocessor, numerical_cols),\n",
    "        ('cat', categorical_preprocessor, categorical_cols),\n",
    "        ('binary', binary_preprocessor, binary_cols)  # Add binary columns processing\n",
    "    ],\n",
    "    remainder='passthrough'  # Leave any remaining columns unchanged\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfc9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the model pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949c0caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to your data\n",
    "# Note: Ensure 'customerID' is removed before fitting\n",
    "X = data.drop(['Churn', 'customerID'], axis=1)\n",
    "y = data['Churn']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1aefc80",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'NAType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25300\\1406512733.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[0mPipeline\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \"\"\"\n\u001b[0;32m    415\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    366\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    371\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[0mbe\u001b[0m \u001b[0mmultiplied\u001b[0m \u001b[0mby\u001b[0m \u001b[1;33m`\u001b[0m\u001b[1;33m`\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m     \"\"\"\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             return_tuple = (\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fitted_transformers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    677\u001b[0m                     \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"Expected 2D array, got 1D array instead\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ERR_MSG_1DCOLUMN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     61\u001b[0m         iterable_with_config = (\n\u001b[0;32m     62\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0m_with_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1914\u001b[0m             \u001b[1;31m# If n_jobs==1, run the computation sequentially and return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m             \u001b[1;31m# immediately to avoid overheads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \u001b[1;31m# call is interrupted early and that the same instance is immediately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1858\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1859\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_running\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[0mUserWarning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[0mbe\u001b[0m \u001b[0mmultiplied\u001b[0m \u001b[0mby\u001b[0m \u001b[1;33m`\u001b[0m\u001b[1;33m`\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m     \"\"\"\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_transformed_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[0mTransformed\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m    463\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    366\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    371\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[0mbe\u001b[0m \u001b[0mmultiplied\u001b[0m \u001b[0mby\u001b[0m \u001b[1;33m`\u001b[0m\u001b[1;33m`\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m     \"\"\"\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             return_tuple = (\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    914\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \"\"\"\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;31m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;31m# otherwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m                 \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"could not convert\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                 new_ve = ValueError(\n\u001b[0;32m    323\u001b[0m                     \"Cannot use {} strategy with non-numeric data:\\n{}\".format(\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    600\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    914\u001b[0m                         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m                 raise ValueError(\n\u001b[0;32m    920\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                 \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\devid\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'NAType'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dbd2652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender              0\n",
      "SeniorCitizen       0\n",
      "Partner             0\n",
      "Dependents          0\n",
      "tenure              0\n",
      "PhoneService        0\n",
      "MultipleLines       0\n",
      "InternetService     0\n",
      "OnlineSecurity      0\n",
      "OnlineBackup        0\n",
      "DeviceProtection    0\n",
      "TechSupport         0\n",
      "StreamingTV         0\n",
      "StreamingMovies     0\n",
      "Contract            0\n",
      "PaperlessBilling    0\n",
      "PaymentMethod       0\n",
      "MonthlyCharges      0\n",
      "TotalCharges        8\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.isnull().sum())  # Check for missing values in the features\n",
    "print(y_train.isnull().sum())  # Check for missing values in the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef2e1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devid\\AppData\\Local\\Temp\\ipykernel_25300\\2174509298.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['TotalCharges'].fillna(data['TotalCharges'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Convert 'TotalCharges' to numeric, coercing errors into NaN\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Now fill missing values with the mean\n",
    "data['TotalCharges'].fillna(data['TotalCharges'].mean(), inplace=True)\n",
    "\n",
    "# Double-check to ensure there are no missing values left in 'TotalCharges'\n",
    "print(data['TotalCharges'].isnull().sum())  # Should print 0 after filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a78afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to your data\n",
    "# Note: Ensure 'customerID' is removed before fitting\n",
    "X = data.drop(['Churn', 'customerID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035bd0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88      1539\n",
      "           1       0.69      0.57      0.62       574\n",
      "\n",
      "    accuracy                           0.81      2113\n",
      "   macro avg       0.77      0.74      0.75      2113\n",
      "weighted avg       0.81      0.81      0.81      2113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "826765ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Churn\n",
       "0    3635\n",
       "1    1295\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee524d",
   "metadata": {},
   "source": [
    "### **Let's now try to improve these results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e612db",
   "metadata": {},
   "source": [
    "5. Feature Engineering:\n",
    "Interaction Features: Create new features that might capture relationships between existing features. For example, interactions between tenure and MonthlyCharges might reveal insights about customer retention.\n",
    "Polynomial Features: You can try adding polynomial features for numerical variables. This can help capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eff5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Add polynomial features for numerical columns\n",
    "# Create a pipeline to process numerical columns by adding polynomial features\n",
    "polynomial_preprocessor = Pipeline(steps=[\n",
    "\n",
    "    # Step 1: Imputer\n",
    "    # This step handles missing values in the numerical data by replacing them with the mean value of the column.\n",
    "    # It ensures that the polynomial feature generation doesn't fail due to missing values.\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "\n",
    "    # Step 2: PolynomialFeatures\n",
    "    # This step generates polynomial features (up to degree 2 in this case) from the numerical data.\n",
    "    # It allows the model to capture non-linear relationships by creating interaction terms and squared terms for each feature.\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "\n",
    "    # Step 3: StandardScaler\n",
    "    # This step standardizes the features by scaling them to have a mean of 0 and standard deviation of 1.\n",
    "    # This is important for many machine learning algorithms, as it ensures that all features are on the same scale.\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a545e",
   "metadata": {},
   "source": [
    "**6. Hyperparameter Tuning:**\n",
    "\n",
    "Grid Search or Randomized Search: This is a very important step in improving model performance. Grid search helps identify the best hyperparameters for the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c68865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'classifier__C': 10, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "# create a parameter grid for logistic regression\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],  # Regularization strength\n",
    "    'classifier__solver': ['liblinear', 'saga'],  # Solvers to try\n",
    "    'classifier__penalty': ['l1', 'l2'],  # Regularization type\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and fit the model\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2a0310a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88      1539\n",
      "           1       0.69      0.57      0.62       574\n",
      "\n",
      "    accuracy                           0.81      2113\n",
      "   macro avg       0.77      0.74      0.75      2113\n",
      "weighted avg       0.81      0.81      0.81      2113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directly use the best estimator from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fit the best model (if not already fitted during grid search)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Display the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f31cc4c",
   "metadata": {},
   "source": [
    "The classification report gives a summary of key metrics for evaluating classification models, including precision, recall, f1-score, and support for each class in the dataset.\n",
    "\n",
    "| Class   | Precision | Recall | F1-Score | Support |\n",
    "|---------|-----------|--------|----------|---------|\n",
    "| **0**   | 0.85      | 0.90   | 0.88     | 1539    |\n",
    "| **1**   | 0.69      | 0.57   | 0.62     | 574     |\n",
    "| **Accuracy**   |       |        | 0.81     | 2113    |\n",
    "| **Macro avg**  | 0.77      | 0.74   | 0.75     | 2113    |\n",
    "| **Weighted avg** | 0.81      | 0.81   | 0.81     | 2113    |\n",
    "\n",
    "\n",
    "- For class 0, precision is 0.85, meaning 85% of predicted 0s were actually 0s.\n",
    "- For class 1, precision is 0.69, meaning 69% of predicted 1s were actually 1s.\n",
    "\n",
    "- Support refers to the number of occurrences of each class in the dataset.\n",
    " - Class 0 has 1539 instances, while class 1 has 574 instances. Which suggests class imbalance.\n",
    "\n",
    "- Macro average computes the metrics for each class and then averages them. It does not take class imbalance into account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df2dde",
   "metadata": {},
   "source": [
    "Let's improve class balance and observe what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d994d",
   "metadata": {},
   "source": [
    "By setting `class_weight='balanced'`, you're telling the model to adjust the weights inversely proportional to class frequencies. \n",
    "\n",
    "This means that the model will treat misclassifications of the minority class (class 1) as more costly and attempt to improve its performance on that class. \n",
    "\n",
    "However, this does not alter the support values in the output, which are based on the actual distribution of the classes in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "403559b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "best_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],  # Regularization strength\n",
    "    'classifier__solver': ['liblinear', 'saga'],  # Solvers to try\n",
    "    'classifier__penalty': ['l1', 'l2'],  # Regularization type\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and fit the model\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "best_model2 = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab4fe6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81      1539\n",
      "           1       0.54      0.84      0.65       574\n",
      "\n",
      "    accuracy                           0.76      2113\n",
      "   macro avg       0.73      0.78      0.73      2113\n",
      "weighted avg       0.82      0.76      0.77      2113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directly use the best estimator from GridSearchCV\n",
    "best_model2 = grid_search.best_estimator_\n",
    "\n",
    "# Fit the best model (if not already fitted during grid search)\n",
    "best_model2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_model2.predict(X_test)\n",
    "\n",
    "# Display the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2b172",
   "metadata": {},
   "source": [
    "Let's Handle Class Imbalance More Effectively with \n",
    "SMOTE (Synthetic Minority Over-sampling Technique):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7f86c",
   "metadata": {},
   "source": [
    "SMOTE requires numeric values for resampling. So let's fix our values in `y`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19afaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Step 1: Apply SMOTE to balance the classes in the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82026838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_train:\n",
      "Churn\n",
      "0    3635\n",
      "1    1295\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Class distribution in y_train\n",
    "import pandas as pd\n",
    "\n",
    "# If y_train_smote is a NumPy array, convert it to a pandas Series for easier inspection\n",
    "y_train_series = pd.Series(y_train)\n",
    "\n",
    "# Get class distribution\n",
    "class_distribution = y_train_series.value_counts()\n",
    "\n",
    "print(\"Class distribution in y_train:\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bc9c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in y_train_smote:\n",
      "Churn\n",
      "0    3635\n",
      "1    3635\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Class distribution in y_train_smote\n",
    "import pandas as pd\n",
    "\n",
    "# If y_train_smote is a NumPy array, convert it to a pandas Series for easier inspection\n",
    "y_train_smote_series = pd.Series(y_train_smote)\n",
    "\n",
    "# Get class distribution\n",
    "class_distribution = y_train_smote_series.value_counts()\n",
    "\n",
    "print(\"Class distribution in y_train_smote:\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e2138ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.73      0.81      1539\n",
      "           1       0.53      0.83      0.65       574\n",
      "\n",
      "    accuracy                           0.76      2113\n",
      "   macro avg       0.73      0.78      0.73      2113\n",
      "weighted avg       0.82      0.76      0.77      2113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the model with balanced class weights\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Step 3: Train the model on the SMOTE-balanced data\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Step 4: Predict on the transformed test set\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "y_pred_smote = model.predict(X_test_transformed)\n",
    "\n",
    "# Step 5: Evaluate the model and print the classification report to check support values\n",
    "print(classification_report(y_test, y_pred_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527217a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14236cde",
   "metadata": {},
   "source": [
    "**7. Ensemble Methods:**\n",
    "\n",
    "Random Forest: A Random Forest classifier or another ensemble model might work better since it can capture more complex patterns.\n",
    "Gradient Boosting: A boosting method like GradientBoostingClassifier or XGBoost often provides better results by focusing on the mistakes made by previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb6cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "217b5f51",
   "metadata": {},
   "source": [
    "Example: Replacing Logistic Regression with RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436aba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01464b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e89ea4",
   "metadata": {},
   "source": [
    " use Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a84f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250eeb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "1. topic:\n",
    "    - question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b86c1-64b0-4abd-8ba9-54746bdc9007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

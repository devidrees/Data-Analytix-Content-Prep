{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "     Model Refinement <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Improving model performance based on evaluation results</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.  Understanding Model Training Errors<p><p> \n",
    "2.  Techniques for Model Refinement <p>\n",
    "3.  Advanced Refinement Techniques (Optional/Advanced) <p>\n",
    "4.  Hands-On: Model Refinement Practice  <p>  \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    If you were a superhero, what would your superpower be?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c5cf4",
   "metadata": {},
   "source": [
    "By the end of this session, students will be able to:\n",
    "\n",
    "- Evaluate key performance metrics for model assessment.\n",
    "- Identify and address issues of underfitting and overfitting in models.\n",
    "- Apply a structured process of data preprocessing, hyperparameter tuning, and ensemble methods for model improvement.\n",
    "- Confidently implement refinement techniques to boost model accuracy and robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6508920",
   "metadata": {},
   "source": [
    "**Evaluation Metrics Recap:**\n",
    "\n",
    "**Core Metrics**\n",
    "- Accuracy: The proportion of correct predictions out of the total predictions. Often used when class distributions are balanced.\n",
    "- Precision: The ratio of correctly predicted positive observations to the total predicted positives. Precision is critical when false positives are costly (e.g., spam detection).\n",
    "- Recall: The ratio of correctly predicted positive observations to all observations in the actual class. Useful in cases where capturing all positives is crucial, like detecting diseases.\n",
    "- F1-Score: The harmonic mean of precision and recall. Balances both metrics, making it ideal when both false positives and false negatives are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e72c9d",
   "metadata": {},
   "source": [
    "**Choosing the Right Metric**\n",
    "It is important to select the right metric that ensures the model aligns with the real-world requirements of a given problem:\n",
    "\n",
    "- **Classification**: Use AUC in credit fraud detection to balance detection rates with false positives.\n",
    "- **Regression**: Use RMSE in forecasting where larger errors need penalization, such as in stock price predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e654fd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Understanding Model Training Errors\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089eb8d0",
   "metadata": {},
   "source": [
    "In machine learning, errors are an inevitable part of building and refining models.\n",
    "\n",
    "Understanding where these errors come from and how to manage them is essential for creating effective models that perform well on new, unseen data. \n",
    "\n",
    "When we train a machine learning model, we want it to be as accurate as possible—not just on the training data, but also on the data it has never seen before. This is where the generalization of a model comes into play. \n",
    "\n",
    "Good generalization means the model can perform well on unseen data, and is not just memorizing the training examples.\n",
    "\n",
    "Some of the most important model training errors are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb04c1",
   "metadata": {},
   "source": [
    "1. **Bias:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf93c83",
   "metadata": {},
   "source": [
    "Imagine you're trying to guess the height of a group of people based on their age. \n",
    "\n",
    "If you always guess the same height for everyone, no matter what their age is, that would be a biased guess. \n",
    "\n",
    "You're not considering their actual ages and just making a simple guess based on some wrong assumption.\n",
    "\n",
    "\n",
    "*Simple Example*\n",
    "\n",
    "Let's say you’re using a machine learning model to predict house prices based on their size (in square feet). If your model always predicts the same price for every house, regardless of its size, that would be high bias. The model is too simple and doesn't take enough details into account. It’s not learning the true relationship between house size and price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105eafb",
   "metadata": {},
   "source": [
    "- Bias refers to systematic errors in the model, introduced by approximating a real-world problem, which may be complex, by a simplified model.\n",
    "\n",
    "- It occurs when the model makes assumptions that are too simple, often ignoring important features or relationships in the data.\n",
    "\n",
    "- High bias typically results in underfitting, where the model is too simple to capture the underlying patterns of the data.\n",
    "\n",
    "- Example: Using a linear regression model to predict house prices when the true relationship is non-linear (e.g., polynomial)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a9184",
   "metadata": {},
   "source": [
    "![](https://static.javatpoint.com/tutorial/machine-learning/images/bias-and-variance-in-machine-learning6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05084e1e",
   "metadata": {},
   "source": [
    "2. **Variance:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db50c35",
   "metadata": {},
   "source": [
    "Variance is like a model's sensitivity to small changes in the data. \n",
    "\n",
    "Imagine you're trying to guess the height of people based on their age. If you adjust your guess every time you see a new person, even when they’re about the same age, then your guesses might be all over the place — that’s high variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e61fd",
   "metadata": {},
   "source": [
    "In machine learning, high variance means the model is trying too hard to fit every little detail in the training data, even the noise or random patterns that aren’t important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30cf035",
   "metadata": {},
   "source": [
    "*Simple Example*\n",
    "Suppose you have a model that predicts house prices based on the square footage and several other features. If your model is high in variance, it might make big adjustments for every single small change in these features. So, a house with just a tiny difference in square footage might get a drastically different price prediction, even if that change shouldn't really matter much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63312e3a",
   "metadata": {},
   "source": [
    "- Variance refers to how much the model's predictions fluctuate when trained on different subsets of data.\n",
    "- It occurs when the model is too complex and overly sensitive to the noise or fluctuations in the training data.\n",
    "- High variance typically leads to overfitting, where the model fits the training data too well but fails to generalize to new, unseen data.\n",
    "- Example: A decision tree with many branches that memorizes the training data instead of learning the general patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791294c7",
   "metadata": {},
   "source": [
    "So, you want to find a balance where the model is not too sensitive (high variance) but also not too simple (high bias). This helps the model perform well on both the training data and any new data it sees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b3357",
   "metadata": {},
   "source": [
    "**Bias-Variance Tradeoff**\n",
    "\n",
    "- The Bias-Variance Tradeoff refers to the tension between these two sources of error. \n",
    "\n",
    "As you reduce bias (by using a more complex model), variance tends to increase (because the model is more sensitive to fluctuations in the training data), and vice versa.\n",
    "\n",
    "- Goal: Find the optimal balance between bias and variance, where the model has both low bias and low variance, resulting in good generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c898db",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*wPUGn4buYw4LYISGL-TUuA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40859196",
   "metadata": {},
   "source": [
    "- [Click here to learn more](https://medium.com/@shiny_jay/ml-bias-variance-2164cb994842)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f5b2f3",
   "metadata": {},
   "source": [
    "**3. Underfitting vs. Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b831b1",
   "metadata": {},
   "source": [
    "Underfitting occurs when the model is too simple and cannot capture the underlying patterns in the data. This leads to poor performance on both the training set and the test set.\n",
    "- High Bias, Low Variance: Simple models (underfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554da65",
   "metadata": {},
   "source": [
    "Overfitting occurs when the model is too complex and captures not just the true patterns but also the noise in the data. This leads to excellent performance on the training set but poor generalization to new, unseen data\n",
    "- Low Bias, High Variance: Complex models (overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eed610",
   "metadata": {},
   "source": [
    "**4. Data Imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988c4af",
   "metadata": {},
   "source": [
    "Data imbalance happens when you have unequal numbers of examples in different classes of your dataset. \n",
    "\n",
    "For example, if you’re building a model to detect fraud, you might have a dataset where 95% of transactions are normal and only 5% are fraudulent. This difference is called imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a812f2",
   "metadata": {},
   "source": [
    "- Bias Toward Majority Class: When data is imbalanced, the model may become biased towards the majority class (the larger group). So, in the fraud example, the model might mostly predict \"normal\" because it’s the majority. This can cause the model to ignore or perform poorly on the minority class (fraudulent transactions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0773fa2",
   "metadata": {},
   "source": [
    "- Poor Accuracy on on Minority Class: If the minority class (fraud) is more important to predict, imbalance can be especially problematic. Even if the model achieves high overall accuracy by predicting the majority class, it might fail to detect the minority class, which is often the class we care most about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9bd38c",
   "metadata": {},
   "source": [
    "In real life, we often fix data imbalance by resampling the data \n",
    "\n",
    "— either by **oversampling** the minority class or **undersampling** the majority class \n",
    "\n",
    "— and instead of just accuracy, use metrics like Precision, Recall, and the F1 score, which give a better picture of how well the model is performing on each class. \n",
    "\n",
    "Additionally, special algorithms like SMOTE (Synthetic Minority Over-sampling Technique) can create synthetic samples to help balance the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f8f7e",
   "metadata": {},
   "source": [
    "**5. Data Drift**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c73d97",
   "metadata": {},
   "source": [
    "Data Drift happens when the data used to train a model changes over time, so it no longer matches the new data the model sees in the real world. This can lead to poor performance because the model is making predictions based on outdated patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60d12f",
   "metadata": {},
   "source": [
    "**Reasons:**\n",
    "\n",
    "- Changing User Behavior: For example, in a recommendation system, user preferences might shift over time.\n",
    "- Seasonal Changes: Data may change with the seasons or economic conditions (like shopping trends during holidays).\n",
    "- System Updates: Changes in how data is collected or processed (like updates to sensors or apps) can also cause drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3faa0c",
   "metadata": {},
   "source": [
    "**Types of Data Drift:**\n",
    "\n",
    "1. **Covariate Drift:** When the input data (features) distribution changes but the relationship to the target variable remains the same.\n",
    "\n",
    "**Example:** \n",
    "Let’s say a model was trained to assess creditworthiness based on features like age, income, and spending habits. \n",
    "\n",
    "Over time, if economic conditions change (like during a recession), people's spending habits and income levels may shift, even though the relationship between these features and credit risk (the target variable) remains the same.\n",
    "\n",
    "2. **Concept Drift:** When the relationship between inputs and the target variable itself changes. \n",
    "\n",
    "**Example:**\n",
    "Suppose a model is trained to classify emails as spam based on specific keywords and patterns. Over time, as spammers develop new tactics, the patterns in spam emails change (e.g., new words, different structures).\n",
    "\n",
    "Now, the original relationship between certain keywords and the label \"spam\" no longer holds, meaning the model becomes less effective. This change in the underlying relationship between inputs (keywords) and the target (spam/not spam) is concept drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6525e",
   "metadata": {},
   "source": [
    "In real life, we handle data drift by regularly monitoring model performance, retraining the model with updated data, and using adaptive models that adjust to new data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c762cd6-ee35-4e4e-aa54-600db6a9c23d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Model Refinement\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aef031-1416-4707-b2a0-fc60ca4ee7ca",
   "metadata": {},
   "source": [
    "Model Refinement in machine learning refers to the process of improving a model's performance by iterating on it through various techniques. \n",
    "\n",
    "This step is crucial to ensure that the model generalizes well to new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb2e43-d552-4f52-b5e7-c4d8af63b29e",
   "metadata": {},
   "source": [
    "The model refinement process typically includes:\n",
    "\n",
    "1. **Evaluating Model Performance:** Carefully analyzing model metrics to identify where the model is falling short.\n",
    "2. **Identifying Key Issues:** Pinpointing problems like high bias (indicating underfitting), high variance (overfitting), or data imbalance.\n",
    "3. **Applying Improvement Techniques:** Using targeted methods, such as tuning hyperparameters, changing model architecture, or refining the dataset, to tackle identified issues.\n",
    "4. **Iterative Testing and Validation:** Running and evaluating improvements in multiple rounds to ensure they lead to better and more stable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b10a45",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Techniques for Model Refinement\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8332a5",
   "metadata": {},
   "source": [
    "Some Techniques to systematically refine your models and make their outputs and predictions better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351bb65",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Step 1: Data Preprocessing Refinement\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5884eb",
   "metadata": {},
   "source": [
    "In this first step of model refinement, we’ll focus on preparing the data to improve model performance. Well-preprocessed data ensures that models learn effectively, reducing bias and variance in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8c42f",
   "metadata": {},
   "source": [
    "### **Data Scaling and Normalization:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965f668",
   "metadata": {},
   "source": [
    "In scaling (Standardization), the goal is to standardize features so they have a mean of 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08858e03",
   "metadata": {},
   "source": [
    "\n",
    "- **When to Scale:** Scaling techniques like StandardScaler or MinMaxScaler are critical when using algorithms sensitive to feature magnitudes, such as distance-based algorithms (e.g., K-Nearest Neighbors) and linear models.\n",
    "- **How it Helps:** Scaling or normalizing features to the same range or distribution can improve model convergence, stability, and predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c51ed4",
   "metadata": {},
   "source": [
    "In Normalization, the goal is to transforms features so they fall within a certain range, typically [0, 1] or [-1, 1].\n",
    "\n",
    "More or less the same, but normalization **adjusts the range of data** to a specific interval (e.g., [0, 1]), while **scaling** **adjusts the spread** or **distribution** of the data, typically by changing the mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5e787",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Suppose we have a dataset with the following features to predict house prices:\n",
    "\n",
    "- **Size** (in square feet)\n",
    "- **Number of Rooms**\n",
    "- **Distance to City Center** (in miles)\n",
    "\n",
    "Here’s a sample of what the dataset might look like:\n",
    "\n",
    "| Size (sq ft) | Number of Rooms | Distance to City Center (miles) | Price ($) |\n",
    "|--------------|-----------------|----------------------------------|-----------|\n",
    "| 2500         | 4               | 8                                | 500,000   |\n",
    "| 1500         | 3               | 15                               | 300,000   |\n",
    "| 1200         | 2               | 20                               | 200,000   |\n",
    "| 1800         | 3               | 10                               | 350,000   |\n",
    "\n",
    "Without scaling, the large range of values (e.g., Size in thousands vs. Distance in tens) may lead some models to interpret Size as more important than other features. Here’s how scaling can address this:\n",
    "\n",
    "**Standard Scaling**\n",
    "\n",
    "Standard scaling (using `StandardScaler`) adjusts each feature so that it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Transformed dataset (Standard Scaled):**\n",
    "\n",
    "| Size (scaled) | Number of Rooms (scaled) | Distance to City Center (scaled) |\n",
    "|---------------|--------------------------|-----------------------------------|\n",
    "| 1.14          | 1.0                      | -0.6                             |\n",
    "| -0.2          | 0.0                      | 0.9                              |\n",
    "| -0.8          | -1.0                     | 1.5                              |\n",
    "| -0.1          | 0.0                      | -0.3                             |\n",
    "\n",
    "**Min-Max Scaling**\n",
    "\n",
    "Min-Max scaling (using `MinMaxScaler`) transforms each feature to a specified range, typically [0, 1].\n",
    "\n",
    "**Transformed dataset (Min-Max Scaled):**\n",
    "\n",
    "| Size (scaled) | Number of Rooms (scaled) | Distance to City Center (scaled) |\n",
    "|---------------|--------------------------|-----------------------------------|\n",
    "| 1.0           | 1.0                      | 0.0                              |\n",
    "| 0.3           | 0.5                      | 0.54                             |\n",
    "| 0.0           | 0.0                      | 1.0                              |\n",
    "| 0.5           | 0.5                      | 0.31                             |\n",
    "\n",
    "**Why This Matters**\n",
    "\n",
    "- **Improved Model Performance**: Models sensitive to feature scales, like K-Nearest Neighbors and Linear Regression, perform better with scaled features.\n",
    "- **Faster Convergence**: Gradient-based models, like logistic regression or neural networks, converge faster when features are on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec68c6",
   "metadata": {},
   "source": [
    "*Find code implementation in the \"DataScaling and Normalisation.ipynb\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8193bf",
   "metadata": {},
   "source": [
    "**Techniques Used in Scaling**\n",
    "\n",
    "- Standard Scaling (Z-score Normalization)\n",
    "- Robust Scaling\n",
    "- Max Abs Scaling\n",
    "\n",
    "**Techniques Used in Normalization**\n",
    "\n",
    "- Min-Max Normalization (Rescaling)\n",
    "- L2 Normalization (Euclidean norm)\n",
    "- L1 Normalization (Manhattan norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54b310",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd74964",
   "metadata": {},
   "source": [
    "### **Feature Engineering:**\n",
    "\n",
    "- **Feature Selection:** Removing features with high correlation or low variance can reduce redundancy and prevent the model from overemphasizing irrelevant or repetitive information.\n",
    "- **Feature Creation/Extraction:** New features can be engineered by transforming or combining existing ones, potentially uncovering valuable patterns that the model might otherwise miss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb75536",
   "metadata": {},
   "source": [
    "**Handling Class Imbalance:**\n",
    "- **Why it Matters:** In classification problems, class imbalance can cause models to perform poorly on the minority class, as they may simply predict the majority class to achieve high accuracy.\n",
    "- **Techniques to Address Imbalance:**\n",
    "    - **Oversampling** (e.g., duplicating instances of the minority class).\n",
    "    - **Undersampling** (e.g., reducing the majority class).\n",
    "    - **SMOTE (Synthetic Minority Over-sampling Technique)**, which creates synthetic samples for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5865a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Step 2: Hyperparameter Tuning\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f70d82",
   "metadata": {},
   "source": [
    "Hyperparameters are configurations **external to the model** that cannot be learned from data during training. They differ from parameters, which are learned by the model.\n",
    "\n",
    "For instance, in a **decision tree**, **parameters** include **split points** within nodes, whereas **hyperparameters** may include the **tree depth** or **minimum number of samples** required to split a node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d7a9b",
   "metadata": {},
   "source": [
    "Proper tuning of hyperparameters can significantly enhance the model's accuracy, generalizability, and training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4c96d",
   "metadata": {},
   "source": [
    "**Hyperparameters vs. Parameters**\n",
    "\n",
    "- **Parameters**: Learned directly from the data (e.g., weights in a linear regression model).\n",
    "- **Hyperparameters**: Set manually before training begins and control the behavior of the learning algorithm (e.g., k in k-nearest neighbors).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0528d",
   "metadata": {},
   "source": [
    "**Hyperparametes for some commonly used algorithms**\n",
    "\n",
    "1. **Linear Regression**\n",
    "   - **Hyperparameters**: \n",
    "     - Regularization strength (for Ridge and Lasso: alpha)\n",
    "     - Solver type (e.g., ‘lbfgs’, ‘saga’)\n",
    "\n",
    "2. **Logistic Regression**\n",
    "   - **Hyperparameters**: \n",
    "     - Regularization strength (C)\n",
    "     - Solver type (e.g., ‘liblinear’, ‘newton-cg’)\n",
    "     - Max iterations\n",
    "\n",
    "3. **Decision Trees**\n",
    "   - **Hyperparameters**:\n",
    "     - Maximum depth (max_depth)\n",
    "     - Minimum samples split (min_samples_split)\n",
    "     - Minimum samples leaf (min_samples_leaf)\n",
    "     - Max features\n",
    "\n",
    "4. **Random Forest**\n",
    "   - **Hyperparameters**:\n",
    "     - Number of trees (n_estimators)\n",
    "     - Max depth (max_depth)\n",
    "     - Max features\n",
    "     - Minimum samples per leaf (min_samples_leaf)\n",
    "     - Bootstrap (for sampling)\n",
    "\n",
    "5. **Support Vector Machines (SVM)**\n",
    "   - **Hyperparameters**:\n",
    "     - Regularization parameter (C)\n",
    "     - Kernel type (e.g., ‘linear’, ‘rbf’, ‘poly’)\n",
    "     - Gamma (for RBF kernel)\n",
    "     - Degree (for polynomial kernel)\n",
    "\n",
    "6. **K-Nearest Neighbors (KNN)**\n",
    "   - **Hyperparameters**:\n",
    "     - Number of neighbors (n_neighbors)\n",
    "     - Distance metric (e.g., ‘euclidean’, ‘manhattan’)\n",
    "     - Weights (uniform or distance)\n",
    "\n",
    "7. **Gradient Boosting Machines (GBM) / XGBoost / LightGBM**\n",
    "   - **Hyperparameters**:\n",
    "     - Number of trees (n_estimators)\n",
    "     - Learning rate (learning_rate)\n",
    "     - Maximum depth (max_depth)\n",
    "     - Subsample\n",
    "     - Min child weight (XGBoost)\n",
    "\n",
    "8. **Naive Bayes**\n",
    "   - **Hyperparameters**:\n",
    "     - Smoothing parameter (alpha)\n",
    "\n",
    "9. **K-Means Clustering**\n",
    "   - **Hyperparameters**:\n",
    "     - Number of clusters (n_clusters)\n",
    "     - Initialization method (init: ‘k-means++’ or ‘random’)\n",
    "     - Max iterations (max_iter)\n",
    "     - Tolerance for convergence (tol)\n",
    "\n",
    "10. **Neural Networks (e.g., MLPClassifier)**\n",
    "    - **Hyperparameters**:\n",
    "      - Number of hidden layers (hidden_layer_sizes)\n",
    "      - Number of neurons per layer\n",
    "      - Activation function (e.g., ‘relu’, ‘tanh’)\n",
    "      - Learning rate (learning_rate)\n",
    "      - Batch size\n",
    "      - Epochs (max_iter)\n",
    "      - Solver type (e.g., ‘adam’, ‘sgd’)\n",
    "\n",
    "11. **Principal Component Analysis (PCA)**\n",
    "    - **Hyperparameters**:\n",
    "      - Number of components (n_components)\n",
    "      - Whitening (True/False)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585a471",
   "metadata": {},
   "source": [
    "**Common Hyperparameter Tuning Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75cb22",
   "metadata": {},
   "source": [
    "**1. Grid Search:**\n",
    "\n",
    "This method tests all possible combinations of a predefined set of hyperparameter values exhaustively. Though powerful, it can be computationally expensive.\n",
    "\n",
    "Example: Testing combinations of max_depth and min_samples_split for a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55eb903",
   "metadata": {},
   "source": [
    "**2. Random Search:**\n",
    "\n",
    "Instead of testing all combinations, random search samples a fixed number of hyperparameter combinations randomly from a specified distribution. This often finds optimal settings more efficiently than grid search.\n",
    "\n",
    "Example: Randomly selecting learning_rate and n_estimators values for a gradient boosting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0d089",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Advanced Refinement Techniques (Optional/Advanced)\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d758075",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Ensemble Methods \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe3a40",
   "metadata": {},
   "source": [
    "- **Ensemble learning** refers to the technique of combining multiple individual models to create a stronger, more accurate predictive model.\n",
    "- The goal is to leverage the strengths of different models while mitigating their weaknesses. \n",
    "- By combining models, we can often achieve better generalization and performance than any single model in isolation.\n",
    "\n",
    "Types of Ensemble Methods:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):**\n",
    "   - Involves training multiple models on different random subsets of the training data (with replacement) and combining their predictions.\n",
    "\n",
    "   - Example: Random Forest, where multiple decision trees are trained and their predictions are averaged (regression) or voted on (classification).\n",
    "\n",
    "2. **Boosting:**\n",
    "   - Involves training models sequentially, where each new model tries to correct the errors of the previous one by focusing on misclassified instances.\n",
    "\n",
    "   - Examples:\n",
    "     - **AdaBoost**: Adjusts the weight of misclassified instances to improve subsequent models.\n",
    "     - **Gradient Boosting**: Trains models in a sequential manner, where each model reduces the residual errors from previous ones.\n",
    "     - **XGBoost**: An optimized version of gradient boosting, known for its efficiency and speed.\n",
    "\n",
    "3. **Stacking (Stacked Generalization)**:\n",
    "   - Involves training multiple models of different types (e.g., decision trees, SVMs) and combining their predictions using a final model (meta-model) that learns how to best combine them.\n",
    "   - Example: A model that combines the predictions of a decision tree, a support vector machine, and a neural network into a final prediction using a logistic regression or another classifier as the meta-model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34451318",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Cross-Validation and Model Validation Techniques \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d47cc1",
   "metadata": {},
   "source": [
    "**K-Fold Cross-Validation:**\n",
    "- Cross-validation helps ensure that model performance is not overestimated by testing it on different subsets of data, rather than just one train-test split. This technique provides a more reliable estimate of model performance.\n",
    "\n",
    "**Advanced Cross-Validation (e.g., Stratified K-Fold, Time Series Split):**\n",
    "- **Stratified K-Fold**: This variation ensures that each fold of the data has a proportionate representation of the target classes, which is especially important for imbalanced datasets.\n",
    "- **Time Series Split**: Used for time-dependent data where the order of the data is crucial (e.g., stock prices, weather data). This method avoids future data being used to predict past data, which is crucial for time-based datasets.\n",
    "  \n",
    "**Train-Test-Split Best Practices:**\n",
    "- **Recap**: Emphasize the importance of a proper **train-test split**. The split should be done randomly to ensure diversity, but students should also be cautious of **data leakage**, which occurs when information from outside the training dataset is used in the model. This would lead to overly optimistic performance estimates.\n",
    "\n",
    "**Conclusion**\n",
    "- **K-Fold Cross-Validation** ensures that the model is tested multiple times on different subsets of the data, providing a more robust performance estimate.\n",
    "- **Advanced Techniques** like **Stratified K-Fold** and **Time Series Split** help tackle challenges posed by imbalanced datasets or time-sensitive data.\n",
    "- **Proper Train-Test Split** practices should be followed to avoid overfitting and data leakage, ensuring that models are validated correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5908701",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"5\"> 4. Hands-On Mini-Project: Model Refinement Practice </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f711bd0",
   "metadata": {},
   "source": [
    "### **Project Title: Customer Churn Prediction Model Refinement Challenge**\n",
    "\n",
    "**Description**\n",
    "\n",
    "This project centers on **predicting customer churn**, a popular real-world problem in industries like **telecom**, **SaaS**, or **e-commerce**. The dataset contains information about customer demographics, subscription details, service usage, and customer support interactions. You will build a machine learning model to predict whether a customer will churn (i.e., stop using the service) based on these features.\n",
    "\n",
    "This project is modern, widely applicable, and gives you insight into solving a business-critical problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd9d8b4",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "1. **Define the Problem**: Predict customer churn using the Telco Customer Churn dataset.  \n",
    "2. **Data Preprocessing**: Handle missing values, encode categorical features, and scale numerical data.  \n",
    "3. **Baseline Model**: Train a simple classifier with minimal preprocessing. \n",
    "4. **Evaluate Baseline**: Analyze accuracy, recall, and other key metrics.  \n",
    "Then attempt improve the metrics of the model.  \n",
    "5. **Feature Engineering**: Create new features, remove redundant ones, and analyze correlations.  \n",
    "6. **Hyperparameter Tuning**: Optimize model parameters using GridSearchCV or similar tools.  \n",
    "7. **Model Ensembling**: Use ensemble techniques like Gradient Boosting or Random Forest to enhance performance.  \n",
    "8. **Final Evaluation**: Compare refined model metrics with baseline results and assess business value.  \n",
    "9. **Presentation**: Summarize findings, improvements, and rationale for techniques used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "1. topic:\n",
    "    - question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b86c1-64b0-4abd-8ba9-54746bdc9007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

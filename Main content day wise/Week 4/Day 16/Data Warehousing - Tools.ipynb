{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Data Warehousing - Tools <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">ETL processes with Apache Airflow and Amazon Redshift</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8978fea-cbf0-4fc7-b62e-58e2f0297b44",
   "metadata": {},
   "source": [
    "## AGENDA\n",
    "1. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "   Whatâ€™s a skill you wish you could magically acquire overnight?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27720c-32cb-44c7-a037-fc7a77b6fcc9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>Data Pipeline\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98308a57",
   "metadata": {},
   "source": [
    "A data pipeline is a method in which raw data is ingested from various data sources, transformed and then ported to a data store, such as a data lake or data warehouse, for analysis. \n",
    "\n",
    "Before data flows into a data repository, it usually undergoes some data processing.\n",
    "\n",
    "data transformations, such as filtering, masking, and aggregations \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2842daa",
   "metadata": {},
   "source": [
    "![image.png](https://media.geeksforgeeks.org/wp-content/uploads/20240923183059/data-pipeline-Overview.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de287146",
   "metadata": {},
   "source": [
    "![image.png](https://estuary.dev/static/66e6cb47ada993cb3d0e713e1300968d/64ce4/a546ea_02_data_pipeline_architecture_ETL_a016c380be.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8bdab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49dcabe0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28962ca7-3365-43cc-8378-ecd0c5fe0917",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Types of Data Pipelines\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7080b3",
   "metadata": {},
   "source": [
    "- **Batch Data Pipelines:** Interact with large portions of data all at once and at some specific time of the day.\n",
    "\n",
    "- **Real-Time Data Pipelines:** Interact with data at the time of its creation for almost real-time outcome.\n",
    "\n",
    "- **Cloud-Native Data Pipelines:** Built for running in cloud environments which are more malleable and more flexible.\n",
    "\n",
    "- **Open-Source Data Pipelines:** Created with the implementation of the open-source technologies like Apache Kafka, Airflow or Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285386e-c68b-4c9b-af08-0d18fe2dc368",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc9b4e9-bc75-48d4-853e-b176194e1581",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> ETL Process\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7010d",
   "metadata": {},
   "source": [
    "![image-2.png](https://media.geeksforgeeks.org/wp-content/uploads/ETL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3dfd5-4d36-4ade-9584-975425db32e1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Apache Airflow\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acebae",
   "metadata": {},
   "source": [
    "an open-source platform designed to programmatically author, schedule, and monitor workflows(a series of tasks in a particular order).\n",
    "\n",
    "\n",
    "\n",
    "well-suited for managing complex data pipelines and is widely used in the fields of data warehousing, data analytics, and data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877d9b2",
   "metadata": {},
   "source": [
    "Linux environment using WSL (Windows Subsystem for Linux) because Airflow was originally designed to run on Linux,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104cfaf3",
   "metadata": {},
   "source": [
    "Running Airflow in WSL allows you to leverage the stability and compatibility of Linux while still using your Windows machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50adafb0",
   "metadata": {},
   "source": [
    "What is WSL?\n",
    "\n",
    "Windows Subsystem for Linux (WSL) is a compatibility layer that enables you to run Linux distributions (like Ubuntu) directly on Windows without the need for a virtual machine or dual-booting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c1c5c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19c960f5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Used to Build Schedule Run data pipelines at scale\n",
    "\n",
    "- https://airflow.apache.org/use-cases/etl_analytics/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0875d8",
   "metadata": {},
   "source": [
    "- Apache airflow documentation - https://airflow.apache.org/docs/apache-airflow/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bfa66a",
   "metadata": {},
   "source": [
    "### **Key Terms:**\n",
    "\n",
    "#### **DAG (Directed Acyclic Graph):**\n",
    "A collection of tasks organized in a way that clearly defines their dependencies and execution order. DAGs form the backbone of Airflow workflows.\n",
    "\n",
    "#### **Operator:**\n",
    "An abstraction that defines what kind of action a task performs, such as executing a Python script (PythonOperator), running a bash command (BashOperator), or transferring data between systems (TransferOperator).\n",
    "\n",
    "#### **Task:**\n",
    "A unit of work in a DAG. Each task represents a specific action (like extracting data or performing transformations) that Airflow will execute according to the DAG's structure.\n",
    "\n",
    "#### **Task Instance:**\n",
    "A specific run of a task within a DAG, identified by its execution date. It represents the combination of a DAG, a task, and a point in time.\n",
    " \n",
    "#### **Scheduler:**\n",
    "The component of Airflow responsible for triggering tasks based on the scheduling defined in the DAG (such as time intervals or dependencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e6a0a",
   "metadata": {},
   "source": [
    "## **Installation Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae95cc4",
   "metadata": {},
   "source": [
    "Apache airflow is not supported by windows natively\n",
    "\n",
    "Let's use docker to run it through a linux kernal.\n",
    "\n",
    "This is the easiest way to get started with Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb6c58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f081f55c",
   "metadata": {},
   "source": [
    "### **Step-by-Step Guide to Install Apache Airflow on Windows (Using WSL)**\n",
    "\n",
    "### Step 1: Launch the WSL Terminal\n",
    "Open **Ubuntu** (or another Linux distribution) from the Start menu.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Update the System\n",
    "Before installing anything, update the package lists and the system itself.\n",
    "\n",
    "Run the following commands to update your package lists and upgrade installed packages:\n",
    "- `sudo apt update`\n",
    "- `sudo apt upgrade -y`\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187b6a1",
   "metadata": {},
   "source": [
    "### Step 3: Install Python and Pip\n",
    "Apache Airflow runs on Python, so you need to install Python and Pip (Python's package manager).\n",
    "\n",
    "To install Python 3 and Pip, use the following command:\n",
    "- `sudo apt install python3 python3-pip -y`\n",
    "\n",
    "After installation, verify the version of Python and Pip using:\n",
    "- `python3 --version`\n",
    "- `pip3 --version`\n",
    "\n",
    "You should see something like Python `3.x.x` and Pip `20.x.x` or higher.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e3535a",
   "metadata": {},
   "source": [
    "### Step 4: Install Virtual Environment (Optional but Recommended)\n",
    "It is recommended to use a virtual environment to isolate the dependencies for your Python projects.\n",
    "\n",
    "First, install the virtual environment package:\n",
    "- `sudo apt install python3-venv -y`\n",
    "\n",
    "Then, create a virtual environment by running:\n",
    "- `python3 -m venv airflow_venv`\n",
    "\n",
    "Activate the virtual environment:\n",
    "- `source airflow_venv/bin/activate`\n",
    "\n",
    "When activated, you should see `(airflow_venv)` at the start of the terminal prompt, indicating the virtual environment is active.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5884e5",
   "metadata": {},
   "source": [
    "### Step 5: Install Apache Airflow\n",
    "Set the Airflow home directory where all configurations and logs will be stored:\n",
    "- `export AIRFLOW_HOME=~/airflow`\n",
    "\n",
    "Now, install Apache Airflow using pip:\n",
    "- `pip install apache-airflow`\n",
    "\n",
    "You can also install additional dependencies for PostgreSQL, Amazon integrations, or others by running:\n",
    "- `pip install apache-airflow[postgres,amazon]`\n",
    "\n",
    "This step may take some time as it downloads and installs the necessary components.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a5bbb",
   "metadata": {},
   "source": [
    "### Step 6: Initialize the Airflow Database\n",
    "Before you can start using Airflow, you need to initialize the metadata database. Run:\n",
    "- `airflow db init`\n",
    "\n",
    "This will set up the necessary database for Airflow.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f977647",
   "metadata": {},
   "source": [
    "### Step 7: Start the Airflow Web Server\n",
    "To start the web server that serves the Airflow UI, run the following command in your terminal:\n",
    "- `airflow webserver --port 8080`\n",
    "\n",
    "The web server will start on port 8080, and logs will appear in your terminal. Leave this terminal window open.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05047c8",
   "metadata": {},
   "source": [
    "### Step 8: Open the Airflow Web Interface\n",
    "Open your web browser and go to:\n",
    "- `http://localhost:8080`\n",
    "\n",
    "You should see the Apache Airflow web interface.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9d621",
   "metadata": {},
   "source": [
    "### Step 9: Start the Airflow Scheduler\n",
    "The scheduler is responsible for executing the tasks within your workflows. To start the scheduler, open **another terminal window** and run:\n",
    "- `airflow scheduler`\n",
    "\n",
    "Leave this window open to allow the scheduler to keep running.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb1f37",
   "metadata": {},
   "source": [
    "## **Steps for Deploying a Simple ETL Pipeline:**\n",
    "\n",
    "#### **1. Define a DAG:** \n",
    "\n",
    "The DAG will represent the workflow of our ETL process. It will have tasks for extracting, transforming, and loading data.\n",
    "\n",
    "#### **2. Extract Task:**\n",
    "\n",
    "This task will simulate extracting data from a source (e.g., reading a CSV file or fetching data from an API). In this case, we'll generate some sample data.\n",
    "\n",
    "#### **3. Transform Task:**\n",
    "\n",
    "This task will take the extracted data and perform some basic transformation (like cleaning or aggregating data).\n",
    "\n",
    "#### **4. Load Task:**\n",
    "\n",
    "The transformed data will be saved to a file or database, simulating a data warehouse or target system.\n",
    "\n",
    "#### **5.Testing the Pipeline:**\n",
    "\n",
    "After the DAG is created, we'll add it to Airflow, trigger it manually, and observe the flow of tasks in the web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61db77a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f26a4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "540c821e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bd4f662",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81111e01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1cdce8",
   "metadata": {},
   "source": [
    "### **Install Docker**\n",
    "\n",
    "- https://www.docker.com/products/docker-desktop/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c38c063",
   "metadata": {},
   "source": [
    "#### **Resources:**\n",
    "\n",
    " - https://youtu.be/eq4o26Hpuac\n",
    "\n",
    "Setting up Amazon redshift CLI - https://www.geeksforgeeks.org/setting-up-the-amazon-redshift-cli\n",
    "\n",
    "Guide to create a data warehouse on Amazon redshift - https://www.datacamp.com/tutorial/guide-to-data-warehousing-on-aws-with-redshift\n",
    "\n",
    "Amazon Redshift Documentation - https://docs.aws.amazon.com/redshift/?icmpid=docs_homepage_analytics\n",
    "\n",
    "Getting started with Amazon Redshift - https://docs.aws.amazon.com/redshift/latest/gsg/new-user-serverless.html\n",
    "\n",
    "Airflow documentation - https://airflow.apache.org/docs/apache-airflow/stable/index.html\n",
    "\n",
    "Learning AWS - https://d1.awsstatic.com/training-and-certification/ramp-up_guides/Ramp-Up_Guide_Data_Analytics.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DABatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

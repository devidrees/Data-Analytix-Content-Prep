{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Supervised Learning 2<br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Classification Algorithms</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.  Classification in Supervised Learning<p><p> \n",
    "2.  Classification Algorithms <p>\n",
    "3.  Hands-On Coding Activity <p>\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    If you could create a new national holiday, what would it be, and how would people celebrate?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68049ddf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Classification in Supervised Learning \n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2387c",
   "metadata": {},
   "source": [
    "Supervised learning is a type of machine learning where the model is trained on a labeled dataset. This means that each training example includes input data and the corresponding output (label). \n",
    "\n",
    "The goal is for the model to learn the relationship between inputs and outputs so it can make predictions on unseen data.\n",
    "\n",
    "### Role in Prediction\n",
    "\n",
    "In prediction tasks, supervised learning algorithms use the patterns learned from the training data to predict outcomes for new, unseen data. This makes it a powerful tool in various fields, allowing us to automate decision-making processes and gain insights from data.\n",
    "\n",
    "### Introduction to Classification Tasks\n",
    "\n",
    "Classification is a specific type of supervised learning task where the goal is to assign input data to predefined categories or classes. For example, given an email, the model might classify it as \"spam\" or \"not spam.\"\n",
    "\n",
    "### Typical Applications\n",
    "\n",
    "1. **Spam Detection**: Email services use classification algorithms to identify and filter out spam messages, ensuring that users receive only relevant emails.\n",
    "\n",
    "2. **Image Recognition**: Classification models can identify objects, faces, or activities in images, enabling applications like facial recognition and automated tagging on social media.\n",
    "\n",
    "3. **Customer Segmentation**: Businesses use classification to group customers based on behavior or preferences, allowing for targeted marketing strategies and personalized recommendations.\n",
    "\n",
    "### Categorical Data\n",
    "\n",
    "Categorical data is essential for classification tasks, allowing models to classify observations into distinct groups based on features that may be nominal or ordinal.\n",
    "\n",
    "Categorical data refers to variables that represent distinct categories or groups. Each observation belongs to one of these categories and is often used in classification tasks. Categorical data can be further divided into two main types:\n",
    "\n",
    "#### 1. Nominal Data\n",
    "- **Definition**: Nominal data has no inherent order or ranking among the categories.\n",
    "- **Examples**: \n",
    "  - Types of fruits (e.g., \"apple,\" \"banana,\" \"orange\")\n",
    "  - Colors (e.g., \"red,\" \"blue,\" \"green\")\n",
    "  - Email classification (e.g., \"spam\" or \"not spam\")\n",
    "\n",
    "#### 2. Ordinal Data\n",
    "- **Definition**: Ordinal data has a clear order or ranking among the categories, but the intervals between the categories are not uniform.\n",
    "- **Examples**:\n",
    "  - Customer satisfaction ratings (e.g., \"poor,\" \"fair,\" \"good,\" \"excellent\")\n",
    "  - Education levels (e.g., \"high school,\" \"bachelor's,\" \"master's\")\n",
    "  - Movie ratings (e.g., \"one star,\" \"two stars,\" \"three stars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f436c5",
   "metadata": {},
   "source": [
    "![Classification work flow](https://images.datacamp.com/image/upload/f_auto,q_auto:best/v1543836883/image_2_rrxvol.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73b8af",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Core Concepts in Classification\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d914c",
   "metadata": {},
   "source": [
    "In classification tasks, understanding certain key concepts is crucial for developing effective machine learning models. \n",
    "\n",
    "These concepts include decision boundaries, types of classifiers, and the nature of classification problems (binary vs. multi-class).\n",
    "\n",
    "#### 1. Decision Boundary and Separating Hyperplanes\n",
    "\n",
    "- **Decision Boundary**: A decision boundary is the line or surface that separates different classes in a feature space. It is crucial for determining how the model predicts the class of new observations.\n",
    "\n",
    "- **Separating Hyperplanes**: A separating hyperplane is a generalization of a decision boundary to higher dimensions. \n",
    "In two dimensions, this is a line, while in three dimensions, it becomes a plane. \n",
    "\n",
    "For example, in a two-class classification problem, a linear classifier will use a straight line as the decision boundary to distinguish between the two classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151807b",
   "metadata": {},
   "source": [
    "#### 2. Linear vs. Non-linear Classifiers\n",
    "\n",
    "- **Classifiers**: Classifiers are algorithms that learn from the data to make predictions about class labels. They can be categorized based on the type of decision boundary they create.\n",
    "\n",
    "- **Linear Classifiers**: These assume that the relationship between the features and class labels can be expressed as a linear function. They produce a straight decision boundary. \n",
    "\n",
    "  Examples include:\n",
    "    - **Logistic Regression**: Models the probability of a binary outcome based on one or more predictor variables.\n",
    "\n",
    "    - **Support Vector Machines (SVM)**: Can be used with a linear kernel to classify data by finding the best hyperplane.\n",
    "\n",
    "- **Non-linear Classifiers**: These do not assume a linear relationship and can model complex decision boundaries. They can capture interactions and non-linearities in the data. \n",
    "  \n",
    "  Examples include:\n",
    "    - **Decision Trees**: Model decisions based on feature values and create a tree-like structure for classification.\n",
    "    - **k-Nearest Neighbors (k-NN)**: Classifies a data point based on the classes of its nearest neighbors in the feature space.\n",
    "    - **Support Vector Machines (with a non-linear kernel)**: Can create curved decision boundaries to separate classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093143ad",
   "metadata": {},
   "source": [
    "#### 3. Binary vs. Multi-class Classification\n",
    "\n",
    "- **Classification**: These refer to the types of classification problems based on the number of classes involved in the prediction.\n",
    "- **Binary Classification**: Involves categorizing observations into one of two distinct classes. The model learns to distinguish between the two classes using a decision boundary. \n",
    "  \n",
    "  Examples include:\n",
    "    - **Email Classification**: Classifying emails as \"spam\" or \"not spam.\"\n",
    "    - **Disease Detection**: Predicting whether a patient is \"sick\" or \"healthy.\"\n",
    "\n",
    "- **Multi-class Classification**: Involves categorizing observations into more than two classes. The model must learn to separate multiple categories, often requiring multiple decision boundaries. \n",
    "  \n",
    "  Examples include:\n",
    "    - **Image Classification**: Identifying images of animals into categories such as \"cat,\" \"dog,\" and \"bird.\"\n",
    "    - **Handwritten Digit Recognition**: Classifying digits from 0 to 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb50448",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Simple Classification \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbb8f9",
   "metadata": {},
   "source": [
    "Here's a simple classification task using the Iris dataset with **scikit-learn**. \n",
    "\n",
    "The Iris dataset is a classic dataset for machine learning that consists of 150 samples from three different species of iris flowers: \n",
    "\n",
    "- Setosa, Versicolor, and Virginica \n",
    "\n",
    "- with four features:\n",
    "    - sepal length\n",
    "    - sepal width\n",
    "    - petal length\n",
    "    - petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417996a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Labels (species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the k-NN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11777c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fit the model on the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06be391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make predictions on the testing data\n",
    "y_pred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6802b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate the model\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "class_report_knn = classification_report(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca43e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy_knn:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"k-NN Classification Report:\", class_report_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_knn, cmap='viridis', edgecolor='k', s=100)\n",
    "plt.title(\"Iris Species Classification using k-NN\")\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508e63f",
   "metadata": {},
   "source": [
    "#### Let's check for another algorithm: Support Vector Machine (SVM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f86ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ce656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the SVM classifier\n",
    "svm = SVC(kernel='linear')  # You can change the kernel to 'rbf', 'poly', etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdefb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fit the model on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0327e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make predictions on the testing data\n",
    "y_pred_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fb8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "class_report_svm = classification_report(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy_svm:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e08a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"SVM Classification Report:\", class_report_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize the results for the first two features\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_svm, cmap='viridis', edgecolor='k', s=100)\n",
    "plt.title(\"Iris Species Classification using SVM\")\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f8d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Generate classification reports for both models\n",
    "class_report_knn = classification_report(y_test, y_pred_knn, output_dict=True)\n",
    "class_report_svm = classification_report(y_test, y_pred_svm, output_dict=True)\n",
    "\n",
    "# Convert classification reports to DataFrames\n",
    "report_knn_df = pd.DataFrame(class_report_knn).transpose()\n",
    "report_svm_df = pd.DataFrame(class_report_svm).transpose()\n",
    "\n",
    "\n",
    "# Create a summary DataFrame to compare precision and recall for each class\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': ['Precision', 'Recall'],\n",
    "    'k-NN Class 0': [report_knn_df.loc['0', 'precision'], report_knn_df.loc['0', 'recall']],\n",
    "    'SVM Class 0': [report_svm_df.loc['0', 'precision'], report_svm_df.loc['0', 'recall']],\n",
    "    'k-NN Class 1': [report_knn_df.loc['1', 'precision'], report_knn_df.loc['1', 'recall']],\n",
    "    'SVM Class 1': [report_svm_df.loc['1', 'precision'], report_svm_df.loc['1', 'recall']],\n",
    "    'k-NN Class 2': [report_knn_df.loc['2', 'precision'], report_knn_df.loc['2', 'recall']],\n",
    "    'SVM Class 2': [report_svm_df.loc['2', 'precision'], report_svm_df.loc['2', 'recall']]\n",
    "})\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Classification Algorithms \n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9bde1",
   "metadata": {},
   "source": [
    "Supervised Learning is a type of machine learning that uses labeled data to train algorithms that classify data or predict outcomes accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd725cf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Core Classification Algorithms\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b82ddb",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression [link](https://mlu-explain.github.io/logistic-regression/)\n",
    "**Concept**: Logistic Regression is commonly used for binary classification tasks. Unlike linear regression, which predicts continuous values, logistic regression uses a function called the sigmoid function to model the probability of a binary outcome (like yes/no, 0/1, spam/not spam).\n",
    "\n",
    "**Sigmoid Function**: The sigmoid function takes any input and maps it to a value between 0 and 1, which can be interpreted as a probability. It looks like this:\n",
    "\n",
    "   ### **Sigmoid(z) = 1/(1 + e^{-z})**\n",
    "\n",
    "**Decision Boundary**: Logistic Regression decides between classes based on a threshold (usually 0.5). If the sigmoid function's output is above 0.5, the prediction is class 1; if below, it's class 0.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "```\n",
    "**Pros**:\n",
    "- Simple and easy to implement.\n",
    "- Works well for binary classification.\n",
    "- Provides probabilistic interpretations (output can be interpreted as probability).\n",
    "\n",
    "**Cons**:\n",
    "- Assumes a linear relationship between features and outcome.\n",
    "- Not effective for complex relationships.\n",
    "- Can struggle with high-dimensional datasets.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264caf7a",
   "metadata": {},
   "source": [
    "### 2. k-Nearest Neighbors (k-NN) [demo Link](https://www.philippe-fournier-viger.com/tools/kmeans_demo.php) - [standford demo](http://vision.stanford.edu/teaching/cs231n-demos/knn/)\n",
    "**Concept**: k-NN is an instance-based, non-parametric algorithm, meaning it doesn’t assume a specific form for the underlying data distribution. Instead, it relies on the proximity of data points to classify a new observation.\n",
    "\n",
    "**Distance Metrics**: To measure proximity, k-NN uses a distance metric, typically Euclidean distance, but other distances (like Manhattan) can be used depending on the data.\n",
    "\n",
    "**Value of k**: The number of neighbors (k) is a key hyperparameter. For instance, if \\( k = 3 \\), the model will look at the three closest points to decide the class of the new point. Lower values of \\( k \\) make the model more sensitive to noise, while higher values create a more generalized boundary.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "```\n",
    "**Pros**:\n",
    "- Simple and intuitive; easy to understand.\n",
    "- Non-parametric (makes no assumptions about data distribution).\n",
    "- Effective for small datasets and instances where proximity defines class.\n",
    "\n",
    "**Cons**:\n",
    "- Computationally intensive for large datasets.\n",
    "- Sensitive to the choice of k and distance metric.\n",
    "- Performs poorly on imbalanced or high-dimensional datasets.\n",
    "\n",
    "[Learn more about KNN](https://neptune.ai/blog/k-means-clustering)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f44f0",
   "metadata": {},
   "source": [
    "### 3. Decision Trees [link](https://mlu-explain.github.io/logistic-regression/)\n",
    "**Concept**: A Decision Tree is a tree-like model where data is split into branches based on certain criteria, making it easy to interpret. Each node represents a decision rule based on a feature, and each leaf node represents a class outcome.\n",
    "\n",
    "**Splitting**: The tree splits data by finding the feature and threshold that maximize information gain or minimize Gini impurity. Information gain measures how much a feature improves classification (like reducing randomness), and Gini impurity measures the degree of misclassification in the splits.\n",
    "\n",
    "**Example Structure**: Imagine a dataset where we classify based on income and loan amount. A decision tree might look like this:\n",
    "\n",
    "```yml\n",
    "└── Income > $50K?\n",
    "    ├── Yes: Loan Approved\n",
    "    └── No: Loan Denied\n",
    "```\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "```\n",
    "**Pros**:\n",
    "- Easy to interpret and visualize.\n",
    "- Can handle both numerical and categorical data.\n",
    "- No need for feature scaling.\n",
    "\n",
    "**Cons**:\n",
    "- Prone to overfitting, especially with deep trees.\n",
    "- Can be unstable (small data changes can drastically alter the tree).\n",
    "- High variance (single trees might not generalize well).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec07a0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Advanced Classification Algorithms\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862953ab",
   "metadata": {},
   "source": [
    "### 4. Support Vector Machines (SVM) [Link](https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python)\n",
    "**Concept**: Support Vector Machines (SVM) is a powerful classifier that works by finding a line (or hyperplane in higher dimensions) that best separates the classes. The main idea is to maximize the margin, which is the distance between the dividing hyperplane and the closest data points from each class. These closest points are called *support vectors*.\n",
    "\n",
    "- **Support Vectors**: These are the data points closest to the decision boundary. They are critical in defining the hyperplane, as moving or changing them would alter the position of the boundary.\n",
    "- **Maximum Margin**: SVM aims to find a hyperplane that maximizes the margin, creating the largest possible separation between classes.\n",
    "- **Linear vs. Non-linear Kernels**: If the data is linearly separable, a linear kernel can be used. For non-linear data, SVM can use kernels like the radial basis function (RBF) or polynomial kernel to map data to a higher-dimensional space where it becomes separable.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')  # For linear SVM\n",
    "```\n",
    "**Pros**:\n",
    "- Effective in high-dimensional spaces.\n",
    "- Works well with clear margin of separation.\n",
    "- Supports both linear and non-linear decision boundaries with kernels.\n",
    "\n",
    "**Cons**:\n",
    "- Computationally expensive for large datasets.\n",
    "- Sensitive to the choice of kernel and regularization parameters.\n",
    "- Less interpretable than simpler models like Logistic Regression.\n",
    "\n",
    "\n",
    "- [FreeCodeCamp Tutorial](https://www.freecodecamp.org/news/svm-machine-learning-tutorial-what-is-the-support-vector-machine-algorithm-explained-with-code-examples/)\n",
    "- \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db5557",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "**Concept**: Naive Bayes is a probabilistic classifier based on Bayes' theorem. It assumes that features are independent, which simplifies the calculations and makes the algorithm fast and efficient.\n",
    "\n",
    "**Independence Assumption**: Naive Bayes assumes that all features contribute independently to the outcome, which, while not always realistic, often works well in practice.\n",
    "\n",
    "**Applications**: This algorithm is widely used in text classification tasks, such as spam detection, where the independence assumption holds well across words.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "```\n",
    "**Pros**:\n",
    "- Simple, fast, and efficient, especially for large datasets.\n",
    "- Works well with categorical data.\n",
    "- Performs well with text classification and spam detection.\n",
    "\n",
    "**Cons**:\n",
    "- Assumes independence between features (often unrealistic).\n",
    "- Limited to probabilistic predictions.\n",
    "- Sensitive to feature correlation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f6bee",
   "metadata": {},
   "source": [
    "### Random Forests [link](https://mlu-explain.github.io/random-forest/)\n",
    "**Concept**: Random Forest is an ensemble learning method that combines multiple decision trees to create a robust model. Each tree in the forest is trained on a random subset of the data, which helps reduce overfitting and improves generalization.\n",
    "\n",
    "**Ensemble Learning**: This approach aggregates the predictions of multiple models (in this case, decision trees) to reach a more accurate result.\n",
    "\n",
    "**Improved Accuracy and Generalization**: By averaging the results of multiple trees, Random Forests reduce variance and are less likely to overfit compared to a single decision tree.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100)  # 100 trees in the forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeda93d",
   "metadata": {},
   "source": [
    "**Pros**:\n",
    "- Reduces overfitting compared to single decision trees.\n",
    "- Handles large datasets and high-dimensional spaces well.\n",
    "- Provides feature importance scores.\n",
    "\n",
    "**Cons**:\n",
    "- Can be computationally intensive.\n",
    "- Harder to interpret than a single decision tree.\n",
    "- Not suitable for tasks requiring real-time predictions due to ensemble nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2668002",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Hands-On Activity\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c83069",
   "metadata": {},
   "source": [
    "Let's perform unsupervised learning on a dataset using various algorithms and check their performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2185430-3791-45df-824f-bdec6d7145e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Live Exercise</font> </b>\n",
    "</div>\n",
    "\n",
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d61af-f3e8-4afc-a66b-3814e160aaf3",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task 1: Predicting Loan Approval\n",
    "\n",
    "You work as a data analyst at a bank, and your job is to create a simple classification model to predict loan approval based on applicants' income and loan amounts. This basic model will help the bank decide whether to approve loans for customers. Below is a small sample dataset:\n",
    "\n",
    "```yml\n",
    "| Applicant Income ($1000s) | Loan Amount ($1000s) | Loan Approved (Yes=1, No=0) |\n",
    "|---------------------------|----------------------|-----------------------------|\n",
    "| 25                        | 5                    | 1                           |\n",
    "| 30                        | 8                    | 1                           |\n",
    "| 20                        | 4                    | 0                           |\n",
    "| 35                        | 7                    | 1                           |\n",
    "| 40                        | 10                   | 1                           |\n",
    "| 15                        | 3                    | 0                           |\n",
    "| 28                        | 6                    | 1                           |\n",
    "| 18                        | 5                    | 0                           |\n",
    "```\n",
    "\n",
    "Using this dataset, complete the following tasks:\n",
    "\n",
    "1. **Train a k-Nearest Neighbors (k-NN) Classifier:** Set up a basic k-NN classifier with `k=3` to predict loan approval.\n",
    "   \n",
    "2. **Make a Prediction:** What is the predicted outcome (approval or not) for a new applicant with an income of $22,000 and a requested loan amount of $6,000?\n",
    "\n",
    "3. **Identify Key Trends:** Based on the results, discuss any patterns you see in the data. For instance, does income seem to influence approval more than loan amount, or vice versa?\n",
    "\n",
    "4. **Visualize the Decision Boundary (Optional):** If time permits, plot the decision boundary and the data points to see how the classifier separates approved and not approved loans.\n",
    "\n",
    "By completing this exercise, you'll get hands-on experience using a classification algorithm to predict outcomes based on features, and understand how changes in features impact classification decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61af437",
   "metadata": {},
   "source": [
    "### Solution: Predicting Monthly Electricity Bill Using Linear Regression\n",
    "\n",
    "In this solution, we’ll build a simple linear regression model using Python code to predict the monthly electricity bill based on average daily electricity usage. Comments are provided to explain each line and concept.\n",
    "\n",
    "```python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9ddd9-5558-4b1d-a3e7-04c3dca33b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7ad29",
   "metadata": {},
   "source": [
    "### Interpretation of results\n",
    "\n",
    "The k-NN model, using an applicant’s income and loan amount, predicted whether a loan would be approved or denied. \n",
    "\n",
    "For the new applicant with an income of $22,000 and a loan request of $6,000, the model classified them based on the three nearest data points, which indicated similar applicants in terms of income and loan size. \n",
    "\n",
    "By plotting the decision boundary, we visually confirmed that this applicant fell within the approval region, suggesting that moderate loan requests relative to income often lead to approval. \n",
    "\n",
    "This simple model thus reveals an intuitive relationship: higher income and moderate loan amounts increase approval likelihood, offering insights into the patterns guiding loan decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "1. **What is supervised learning, and how does it differ from unsupervised learning?**\n",
    "   - Explain the concept of supervised learning where models are trained on labeled data to make predictions, contrasting it with unsupervised learning which deals with unlabeled data.\n",
    "\n",
    "2. **Can you describe the main types of classification algorithms used in supervised learning?**\n",
    "   - Discuss various classification algorithms such as logistic regression, decision trees, support vector machines (SVM), and k-Nearest Neighbors (k-NN), highlighting their characteristics and use cases.\n",
    "\n",
    "3. **How do decision trees make decisions, and what are their advantages and disadvantages in classification tasks?**\n",
    "   - Explain how decision trees split data based on feature values to create a model that predicts the target variable, discussing their interpretability and potential issues like overfitting.\n",
    "\n",
    "4. **What is the concept of the support vector machine (SVM), and how does it work for classification?**\n",
    "   - Describe how SVM aims to find the optimal hyperplane that separates different classes in a dataset, emphasizing its effectiveness in high-dimensional spaces.\n",
    "\n",
    "5. **How does the k-Nearest Neighbors (k-NN) algorithm classify new data points?**\n",
    "   - Discuss the mechanism of k-NN, where the class of a new instance is determined by the majority class among its `k` closest neighbors in the feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c7921",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Solutions </h4> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c993a",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

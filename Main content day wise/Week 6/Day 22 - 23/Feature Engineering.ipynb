{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Feature Engineering <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Techniques for feature extraction and selection.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df13f0",
   "metadata": {},
   "source": [
    "<b>Learning Outcomes</b> \n",
    "\n",
    "By the end of this class, students will be able to:\n",
    "\n",
    "1. **Understand the role of feature engineering** in improving machine learning models.\n",
    "2. **Differentiate between feature selection and feature extraction**.\n",
    "3. **Implement feature selection techniques** like filter, wrapper, and embedded methods.\n",
    "4. **Apply feature extraction methods** such as PCA and LDA for dimensionality reduction.\n",
    "5. **Evaluate model performance** before and after applying feature engineering techniques.\n",
    "6. **Code and practice** feature engineering techniques in Python to enhance model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.  Introduction to Feature Engineering<p><p> \n",
    "2.  Feature Selection <p>\n",
    "3.  Feature Extraction <p>\n",
    "4.  Hands - on activity <p>  \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "    If you were a superhero, what would your superpower be?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Introduction to Feature Engineering\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aef031-1416-4707-b2a0-fc60ca4ee7ca",
   "metadata": {},
   "source": [
    "**Feature Engineering** is the process of selecting, modifying, or creating new features from raw data to improve the performance of machine learning (ML) models. It is a crucial step in ML pipelines because the quality and relevance of features directly influence the model’s ability to learn and make predictions.\n",
    "\n",
    "### Definition and Purpose of Feature Engineering:\n",
    "- **Feature Engineering** involves manipulating and preparing data in ways that enhance the learning process for algorithms.\n",
    "- **Purpose**: \n",
    "  - Improve model accuracy by providing the model with more relevant and informative input data.\n",
    "  - Simplify the model by reducing unnecessary or irrelevant features.\n",
    "\n",
    "### Importance of Relevant Features for Improving Model Performance:\n",
    "- **Better Features = Better Models**: The better the features, the better the model’s predictive performance.\n",
    "- **Relevant Features**: Features that directly correlate with the target variable lead to better learning and generalization.\n",
    "- **Irrelevant Features**: Features that are not meaningful for the model can lead to overfitting, where the model performs well on training data but poorly on new data.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d002ebbf",
   "metadata": {},
   "source": [
    "![Feature engineering f4 steps](https://cdn.prod.website-files.com/60cce6512b4ab924a0427124/645b12aaa9eda6169b1f54a5_featureform%20Feature%20Engineering%20Guide-18%20(2).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e919c1af",
   "metadata": {},
   "source": [
    "Get an intro here: https://www.featureform.com/post/feature-engineering-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ed634",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b700b61d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Overview of Feature Selection vs. Feature Extraction\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef889b7",
   "metadata": {},
   "source": [
    "### Feature Selection:\n",
    "Feature selection is the process of **choosing a subset of the most important features** from the original dataset. The goal is to keep only the features that contribute the most to the model’s predictive power.\n",
    "\n",
    "Feature selection is all about picking the right features for your model, just like a chef picks the best ingredients for their dish. The goal is to improve model performance and reduce complexity.\n",
    "\n",
    "Domain knowledge is required for this.\n",
    "\n",
    "- **Methods**:\n",
    "  - **Filter Methods**: Use statistical techniques (e.g., correlation, chi-square) to identify the most relevant features before feeding them to the model.\n",
    "  - **Wrapper Methods**: Search for the best feature subset by evaluating model performance using different combinations of features.\n",
    "  - **Embedded Methods**: Perform feature selection during the model training process, like decision trees and LASSO regression.\n",
    "\n",
    "### Feature Extraction:\n",
    "Feature extraction transforms the original features into **new features** by combining or reducing them. The goal is to reduce the number of features while preserving the important information.\n",
    "\n",
    "- **Example**: Principal Component Analysis (PCA) reduces high-dimensional data to a smaller set of uncorrelated features called principal components.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c232819",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Motivations for Feature Engineering\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b059d",
   "metadata": {},
   "source": [
    "### 1. Reducing Model Complexity:\n",
    "- Feature engineering can reduce the complexity of the model by removing redundant or irrelevant features.\n",
    "- A simpler model can generalize better to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da9932d",
   "metadata": {},
   "source": [
    "### 2. Enhancing Interpretability:\n",
    "- **Interpretability** is crucial, especially in domains where understanding how decisions are made is important (e.g., healthcare, finance).\n",
    "- Feature engineering can make a model more understandable by focusing on meaningful and interpretable features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad9be8c",
   "metadata": {},
   "source": [
    "### 3. Avoiding the Curse of Dimensionality:\n",
    "- The curse of dimensionality occurs when the feature space becomes too large (too many features), leading to overfitting, long computation times, and poor generalization.\n",
    "- By reducing the number of features or transforming them, feature engineering helps in avoiding this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1822e1",
   "metadata": {},
   "source": [
    "### 4. Improving Model Performance:\n",
    "- **Accuracy**: Well-engineered features can directly improve the model’s predictive power.\n",
    "- **Speed**: Reducing the number of features can improve the model’s speed during training and prediction.\n",
    "- **Stability**: Properly selected features can make the model more stable and robust against fluctuations in data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b10a45",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Feature Selection Techniques\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db5b64",
   "metadata": {},
   "source": [
    "Feature selection is the process of identifying and choosing the most important features for use in model building. It plays a critical role in improving the performance of machine learning models.\n",
    "\n",
    "Feature selection is crucial to improving model performance, reducing overfitting, and speeding up training. By using the right combination of feature selection techniques, you can enhance your machine learning model’s ability to make predictions while simplifying the model.\n",
    "\n",
    "### Key Benefits:\n",
    "- **Reduces Overfitting**: By removing irrelevant or redundant features, we reduce the complexity of the model, which helps in avoiding overfitting.\n",
    "- **Improves Accuracy**: A more focused set of features often leads to better model accuracy, as the model can focus on the most relevant patterns.\n",
    "- **Reduces Training Time**: Fewer features result in faster model training and less computational effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24b8f0",
   "metadata": {},
   "source": [
    "![One of the ways can be classified](https://cdn.prod.website-files.com/60cce6512b4ab924a0427124/645b13e01bb20d40dc6eca27_featureform%20Feature%20Engineering%20Guide-19%20(1).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc5e6b",
   "metadata": {},
   "source": [
    "---\n",
    "## Types of Feature Selection Methods\n",
    "\n",
    "There are three main categories of feature selection methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8ed7d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Filter Methods\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c7455",
   "metadata": {},
   "source": [
    "Filter methods evaluate the relevance of each feature independently of the model. These methods use statistical tests or metrics to rank the importance of features before feeding them into a machine learning model.\n",
    "\n",
    "- **Correlation Matrix**:\n",
    "  - Measures the strength of the relationship between features.\n",
    "  - Highly correlated features may be redundant, so one of the pair can be removed to reduce multicollinearity.\n",
    "  - **Example**: If two features have a high correlation (e.g., > 0.9), you might remove one of them.\n",
    "\n",
    "- **Chi-Square Test**:\n",
    "  - Used for categorical variables to determine if a feature is independent of the target variable.\n",
    "  - **Purpose**: Helps to assess the significance of each feature in relation to the target class.\n",
    "\n",
    "- **Variance Threshold**:\n",
    "  - Removes features that have a low variance across the dataset.\n",
    "  - Features with constant values or near-constant values provide little information to the model, so they are often dropped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd437f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Wrapper Methods\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44710683",
   "metadata": {},
   "source": [
    "Wrapper methods use machine learning algorithms to evaluate subsets of features. The model’s performance on the feature subset is used to select the best features.\n",
    "\n",
    "- **Recursive Feature Elimination (RFE)**:\n",
    "  - RFE works by recursively removing the least important features, based on model performance, until the optimal subset of features is achieved.\n",
    "  - **Process**: Starts with all features, fits the model, evaluates importance, and removes the least important feature. This process is repeated until the optimal set is found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce1d073",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Embedded Methods\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834031e",
   "metadata": {},
   "source": [
    "Embedded methods perform feature selection as part of the model training process. They are more efficient than wrapper methods because they integrate feature selection with the model fitting.\n",
    "\n",
    "- **Lasso (L1 Regularization)**:\n",
    "  - Lasso adds a penalty to the regression model, forcing some feature coefficients to become zero, effectively removing them from the model.\n",
    "  - **Benefit**: Automatically performs feature selection while building the model by shrinking the coefficients of less important features to zero.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "4fe188d6",
   "metadata": {},
   "source": [
    "**Regularization** is a technique used in machine learning to prevent overfitting by adding a penalty to the model for being too complex. \n",
    "\n",
    "\n",
    "Regularization is a technique used in machine learning to add a penalty to the model's loss function, discouraging the model from fitting too closely to the training data. \n",
    "\n",
    "It helps prevent overfitting by reducing the complexity of the model, typically by shrinking model parameters or eliminating irrelevant features.\n",
    "\n",
    "In simple terms, it discourages the model from relying too heavily on any one feature and helps it generalize better to new, unseen data.\n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator, and it is a type of regularization technique that uses L1 regularization.\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso (L1): Shrinks some feature coefficients exactly to zero. It's great for feature selection, effectively selecting only the most important features for the model\n",
    "\n",
    "- Ridge (L2): Shrinks the coefficients of features but does not make them exactly zero. It works well when all features are relevant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb44b7b",
   "metadata": {},
   "source": [
    "- **Tree-based Methods**:\n",
    "  - Decision trees and ensemble methods like Random Forest provide a feature importance score based on how much a feature contributes to the model's ability to make accurate predictions.\n",
    "  - **Example**: In Random Forest, features that are more frequently used in splitting nodes are considered more important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0d089",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Feature Extraction Techniques\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea712579",
   "metadata": {},
   "source": [
    "Feature extraction is the process of transforming raw data into a set of features that can be used to train a machine learning model. \n",
    "\n",
    "It involves creating new variables by transforming or combining existing features, making them more meaningful and easier for the model to interpret.\n",
    "\n",
    "### Key Benefits:\n",
    "- **Reduces Dimensionality**: By creating fewer but more meaningful features, feature extraction can reduce the complexity of the model.\n",
    "- **Enhances Model Performance**: Helps in creating features that capture essential patterns in the data, improving the model’s ability to make accurate predictions.\n",
    "- **Enables Better Data Representation**: Allows the transformation of unstructured data (like images, text) into a structured form that can be used by machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c9481",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Common Feature Extraction Techniques\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722456c0",
   "metadata": {},
   "source": [
    "### 1. **Principal Component Analysis (PCA)**\n",
    "\n",
    "PCA is a dimensionality reduction technique that transforms the data into a new set of orthogonal (uncorrelated) features called principal components.\n",
    "- **How it works**: It identifies the directions (principal components) in which the data varies the most, and projects the data along these directions.\n",
    "- **Benefit**: Reduces the number of features while retaining the maximum variance in the dataset.\n",
    "- **When to use**: When you have a large number of features and want to reduce them without losing much information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac0328",
   "metadata": {},
   "source": [
    "**Example Code**:\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Apply PCA to reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# The transformed data (reduced dimensions)\n",
    "print(X_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1940cf67",
   "metadata": {},
   "source": [
    "### 3. **Independent Component Analysis (ICA)**\n",
    "\n",
    "Independent Component Analysis (ICA) is a technique used to separate multivariate signals into independent components. Unlike PCA, which focuses on finding uncorrelated components, ICA seeks statistically independent components.\n",
    "  \n",
    "- **How it works**: ICA decomposes a multivariate signal into additive, independent components. It is widely used in areas like signal processing, especially for separating sources in mixed signals.\n",
    "\n",
    "- **Benefit**: ICA is particularly useful when you need to separate mixed signals where the components are non-Gaussian (e.g., audio signals or EEG data).\n",
    "\n",
    "- **When to use**: ICA is beneficial when you need to extract features that are independent of each other, such as in applications like speech processing or when working with time-series data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e5ee0",
   "metadata": {},
   "source": [
    "**Example Code**:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "# Apply ICA to reduce to 2 components\n",
    "ica = FastICA(n_components=2)\n",
    "X_ica = ica.fit_transform(X_scaled)\n",
    "\n",
    "# The transformed data (independent components)\n",
    "print(X_ica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777aec6",
   "metadata": {},
   "source": [
    "### 4. **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique primarily used for the visualization of high-dimensional data. It maps high-dimensional data into a lower-dimensional space (usually 2D or 3D) while maintaining the pairwise similarities between data points.\n",
    "\n",
    "- **How it works**: t-SNE minimizes the divergence between probability distributions that represent pairwise similarities in both high and low-dimensional spaces. It uses a probability distribution to model the similarity between data points and then attempts to preserve these relationships in the reduced space.\n",
    "\n",
    "- **Benefit**: t-SNE is especially effective at preserving local structure, meaning it can reveal clusters or patterns within the data when visualized in 2D or 3D.\n",
    "\n",
    "- **When to use**: t-SNE is primarily used for visualizing high-dimensional data, especially when dealing with clustering tasks or when you want to explore complex relationships within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985ed76",
   "metadata": {},
   "source": [
    "**Example Code**:\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the transformed data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df['Cluster'], cmap='viridis')\n",
    "plt.title('t-SNE visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0ca79",
   "metadata": {},
   "source": [
    "### 5. **Autoencoders** (optional/Advanced)\n",
    "\n",
    "Autoencoders are a type of artificial neural network used to learn efficient codings (feature representations) of input data, typically for the purpose of dimensionality reduction. The network consists of an encoder that compresses the input into a latent space and a decoder that reconstructs the original input from this compressed form.\n",
    "\n",
    "- **How it works**: The encoder part of the network maps the input data into a lower-dimensional space (latent space). The decoder reconstructs the input data from this representation. The model is trained to minimize the reconstruction error, meaning the output should closely resemble the input data.\n",
    "\n",
    "- **Benefit**: Autoencoders are highly flexible and can be applied to a wide range of data types, including images, text, and time series. They are especially useful for feature extraction, anomaly detection, and denoising.\n",
    "\n",
    "- **When to use**: Autoencoders are used when you have large and complex data (e.g., images or sequential data) and need a compact representation of the data, especially when manual feature engineering is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e07ea4",
   "metadata": {},
   "source": [
    "**Example Code**:\n",
    "```python\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Apply an autoencoder (simplified version using MLPRegressor as an example)\n",
    "autoencoder = MLPRegressor(hidden_layer_sizes=(2,))\n",
    "autoencoder.fit(X_scaled, X_scaled)\n",
    "\n",
    "# Get the encoded features (latent space representation)\n",
    "X_encoded = autoencoder.predict(X_scaled)\n",
    "\n",
    "print(X_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5373419",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>4. Hands-on Activity\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1051b",
   "metadata": {},
   "source": [
    "Additional Resources:\n",
    "\n",
    "- https://medium.com/@chenycy/master-the-art-of-feature-engineering-and-feature-selection-e6e87f76f89b\n",
    "\n",
    "- https://youtu.be/sluoVhT0ehg?si=IMtbUtmzDW8ffNze\n",
    "\n",
    "- https://feature-engine.trainindata.com/en/latest/#feature-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2185430-3791-45df-824f-bdec6d7145e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Live Exercise</font> </b>\n",
    "</div>\n",
    "\n",
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d61af-f3e8-4afc-a66b-3814e160aaf3",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task 1: description of task\n",
    "\n",
    "    - instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9ddd9-5558-4b1d-a3e7-04c3dca33b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "1. topic:\n",
    "    - question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b86c1-64b0-4abd-8ba9-54746bdc9007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
